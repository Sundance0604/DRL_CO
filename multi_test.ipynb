{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带掩码的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为119282.0,58次未求解\n",
      "第1次训练\n",
      "执行时间: 1.8399901390075684 秒,50次未求解，当前强化学习值为262721.0,利润为262721.0\n",
      "第2次训练\n",
      "执行时间: 3.9947502613067627 秒,14次未求解，当前强化学习值为876163.0,利润为876163.0\n",
      "第3次训练\n",
      "执行时间: 5.620569705963135 秒,29次未求解，当前强化学习值为630302.0,利润为630302.0\n",
      "第4次训练\n",
      "执行时间: 8.040527582168579 秒,0次未求解，当前强化学习值为1137983.0,利润为1137983.0\n",
      "第5次训练\n",
      "执行时间: 9.552980184555054 秒,36次未求解，当前强化学习值为493615.0,利润为493615.0\n",
      "第6次训练\n",
      "执行时间: 11.84005880355835 秒,0次未求解，当前强化学习值为1119662.0,利润为1119662.0\n",
      "第7次训练\n",
      "执行时间: 13.321484804153442 秒,35次未求解，当前强化学习值为516251.0,利润为516251.0\n",
      "第8次训练\n",
      "执行时间: 15.030028820037842 秒,23次未求解，当前强化学习值为737416.0,利润为737416.0\n",
      "第9次训练\n",
      "执行时间: 17.870023250579834 秒,49次未求解，当前强化学习值为292569.0,利润为292569.0\n",
      "第10次训练\n",
      "执行时间: 20.277883291244507 秒,0次未求解，当前强化学习值为1122293.0,利润为1122293.0\n",
      "第11次训练\n",
      "执行时间: 21.34235954284668 秒,62次未求解，当前强化学习值为47739.0,利润为47739.0\n",
      "第12次训练\n",
      "执行时间: 23.63964319229126 秒,0次未求解，当前强化学习值为1129111.0,利润为1129111.0\n",
      "第13次训练\n",
      "执行时间: 24.700026035308838 秒,61次未求解，当前强化学习值为64000.0,利润为64000.0\n",
      "第14次训练\n",
      "执行时间: 25.829976558685303 秒,55次未求解，当前强化学习值为166136.0,利润为166136.0\n",
      "第15次训练\n",
      "执行时间: 26.759884119033813 秒,62次未求解，当前强化学习值为49557.0,利润为49557.0\n",
      "第16次训练\n",
      "执行时间: 27.697844743728638 秒,62次未求解，当前强化学习值为49557.0,利润为49557.0\n",
      "第17次训练\n",
      "执行时间: 29.32480239868164 秒,30次未求解，当前强化学习值为619153.0,利润为619153.0\n",
      "第18次训练\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action = agent.take_action_mask(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试版 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为333835.0,48次未求解\n",
      "第1次训练\n",
      "执行时间: 1.7765674591064453 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第2次训练\n",
      "执行时间: 2.8372347354888916 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第3次训练\n",
      "执行时间: 3.9000909328460693 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第4次训练\n",
      "执行时间: 4.9130539894104 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第5次训练\n",
      "执行时间: 5.973233938217163 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第6次训练\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.2500],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'actions' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m     greedy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    189\u001b[0m mask \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_mask(orders_unmatched)\n\u001b[1;32m--> 190\u001b[0m action , logits \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# reward = env.test_step(orders_unmatched,action)\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03mCOUNT = 1000\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03mmax_reward = -999999\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m    reward = env.test_step(orders_unmatched, max_action)\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:174\u001b[0m, in \u001b[0;36mMultiAgentAC.take_action_mask\u001b[1;34m(self, vehicle_states, order_states, mask, explore, greedy)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    172\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask 全 0，导致 softmax 计算无意义！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactions\u001b[49m , logits\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'actions' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 30\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0\n",
    "            explore = False\n",
    "            \n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            \"\"\"\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "            \"\"\"\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action , logits = agent.take_action_mask(vehicle_states, order_states, mask, explore, greedy)\n",
    "            # reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            # grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward +=  objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        # save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        # torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试版2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "未加强化学习利润为200830.0,53次未求解\n",
      "第1次训练\n",
      "v_encoded 出现 NaN，输入状态可能异常！\n",
      "o_encoded 出现 NaN，输入状态可能异常！\n",
      "actor_input 出现 NaN，输入状态可能异常！\n",
      "global vehicle 出现 NaN，输入状态可能异常！\n",
      "repeated global 出现 NaN，输入状态可能异常！\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward0>)\n",
      "logits 出现 NaN，输入状态可能异常！\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'actions' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 187\u001b[0m\n\u001b[0;32m    185\u001b[0m     greedy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    186\u001b[0m mask \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_mask(orders_unmatched)\n\u001b[1;32m--> 187\u001b[0m action, logits \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action_mask_special\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mtest_step(orders_unmatched,action)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mCOUNT = 1000\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03mmax_reward = -999999\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    reward = env.test_step(orders_unmatched, max_action)\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:192\u001b[0m, in \u001b[0;36mMultiAgentAC.take_action_mask_special\u001b[1;34m(self, vehicle_states, order_states, mask, explore, greedy)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask 全 0，导致 softmax 计算无意义！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactions\u001b[49m , logits\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'actions' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update_logtis(vehicle_states, order_states, logits,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action, logits = agent.take_action_mask_special(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例1 burn in后探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例2 burn in后不探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例3 更新条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带跳过的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为351240.0,48次未求解\n",
      "第1次训练\n",
      "执行时间: 1.5951976776123047 秒,66次未求解，当前强化学习值为-316088.0,利润为-1388.0\n",
      "第2次训练\n",
      "执行时间: 2.8220858573913574 秒,40次未求解，当前强化学习值为255049.0,利润为471649.0\n",
      "第3次训练\n",
      "执行时间: 3.92783522605896 秒,48次未求解，当前强化学习值为76507.0,利润为333107.0\n",
      "第4次训练\n",
      "执行时间: 4.801097393035889 秒,60次未求解，当前强化学习值为-187197.0,利润为108603.0\n",
      "第5次训练\n",
      "执行时间: 6.016491651535034 秒,45次未求解，当前强化学习值为168199.0,利润为405999.0\n",
      "第6次训练\n",
      "执行时间: 6.894177436828613 秒,60次未求解，当前强化学习值为-183913.0,利润为112087.0\n",
      "第7次训练\n",
      "执行时间: 7.8495588302612305 秒,60次未求解，当前强化学习值为-186198.0,利润为108602.0\n",
      "第8次训练\n",
      "执行时间: 9.370670318603516 秒,43次未求解，当前强化学习值为195294.0,利润为423494.0\n",
      "第9次训练\n",
      "执行时间: 10.391007423400879 秒,59次未求解，当前强化学习值为-181762.0,利润为112638.0\n",
      "第10次训练\n",
      "执行时间: 11.553189754486084 秒,57次未求解，当前强化学习值为-102270.0,利润为178130.0\n",
      "第11次训练\n",
      "执行时间: 13.124795198440552 秒,47次未求解，当前强化学习值为114564.0,利润为361264.0\n",
      "第12次训练\n",
      "执行时间: 14.306801080703735 秒,60次未求解，当前强化学习值为-180413.0,利润为112087.0\n",
      "第13次训练\n",
      "执行时间: 15.57046389579773 秒,48次未求解，当前强化学习值为68873.0,利润为326673.0\n",
      "第14次训练\n",
      "执行时间: 16.64451813697815 秒,57次未求解，当前强化学习值为-110057.0,利润为170643.0\n",
      "第15次训练\n",
      "执行时间: 17.640617847442627 秒,59次未求解，当前强化学习值为-159380.0,利润为131420.0\n",
      "第16次训练\n",
      "执行时间: 18.647897958755493 秒,59次未求解，当前强化学习值为-168057.0,利润为127243.0\n",
      "第17次训练\n",
      "执行时间: 20.608492374420166 秒,28次未求解，当前强化学习值为534251.0,利润为696851.0\n",
      "第18次训练\n",
      "执行时间: 21.51712131500244 秒,60次未求解，当前强化学习值为-190784.0,利润为105716.0\n",
      "第19次训练\n",
      "执行时间: 22.778335571289062 秒,48次未求解，当前强化学习值为80027.0,利润为333427.0\n",
      "第20次训练\n",
      "执行时间: 23.744807720184326 秒,60次未求解，当前强化学习值为-184613.0,利润为112087.0\n",
      "第21次训练\n",
      "执行时间: 24.99462080001831 秒,51次未求解，当前强化学习值为23549.0,利润为288049.0\n",
      "第22次训练\n",
      "执行时间: 25.945756196975708 秒,60次未求解，当前强化学习值为-187566.0,利润为107234.0\n",
      "第23次训练\n",
      "执行时间: 27.99422001838684 秒,23次未求解，当前强化学习值为649226.0,利润为800126.0\n",
      "第24次训练\n",
      "执行时间: 29.752153396606445 秒,32次未求解，当前强化学习值为436038.0,利润为620738.0\n",
      "第25次训练\n",
      "执行时间: 31.028327465057373 秒,47次未求解，当前强化学习值为129301.0,利润为373001.0\n",
      "第26次训练\n",
      "执行时间: 32.439204454422 秒,45次未求解，当前强化学习值为158061.0,利润为394861.0\n",
      "第27次训练\n",
      "执行时间: 33.439168214797974 秒,57次未求解，当前强化学习值为-118950.0,利润为164950.0\n",
      "第28次训练\n",
      "执行时间: 34.552332639694214 秒,54次未求解，当前强化学习值为-59392.0,利润为217108.0\n",
      "第29次训练\n",
      "执行时间: 35.700438261032104 秒,54次未求解，当前强化学习值为-63310.0,利润为215190.0\n",
      "第30次训练\n",
      "执行时间: 36.633397817611694 秒,60次未求解，当前强化学习值为-187284.0,利润为105716.0\n",
      "第31次训练\n",
      "执行时间: 38.67721676826477 秒,22次未求解，当前强化学习值为680635.0,利润为824735.0\n",
      "第32次训练\n",
      "执行时间: 39.634028673172 秒,60次未求解，当前强化学习值为-187925.0,利润为108375.0\n",
      "第33次训练\n",
      "执行时间: 41.42440843582153 秒,35次未求解，当前强化学习值为369561.0,利润为567161.0\n",
      "第34次训练\n",
      "执行时间: 42.37771916389465 秒,60次未求解，当前强化学习值为-193384.0,利润为105716.0\n",
      "第35次训练\n",
      "执行时间: 43.3869833946228 秒,58次未求解，当前强化学习值为-129827.0,利润为155773.0\n",
      "第36次训练\n",
      "执行时间: 45.71844553947449 秒,11次未求解，当前强化学习值为902113.0,利润为1014313.0\n",
      "第37次训练\n",
      "执行时间: 46.916133403778076 秒,50次未求解，当前强化学习值为43715.0,利润为306115.0\n",
      "第38次训练\n",
      "执行时间: 48.783586502075195 秒,28次未求解，当前强化学习值为548164.0,利润为711764.0\n",
      "第39次训练\n",
      "执行时间: 49.75050187110901 秒,60次未求解，当前强化学习值为-191724.0,利润为108376.0\n",
      "第40次训练\n",
      "执行时间: 51.13849878311157 秒,43次未求解，当前强化学习值为195591.0,利润为421091.0\n",
      "第41次训练\n",
      "执行时间: 52.136146783828735 秒,57次未求解，当前强化学习值为-116542.0,利润为164658.0\n",
      "第42次训练\n",
      "执行时间: 53.34069466590881 秒,51次未求解，当前强化学习值为22358.0,利润为286358.0\n",
      "第43次训练\n",
      "执行时间: 54.45970129966736 秒,53次未求解，当前强化学习值为-46247.0,利润为229853.0\n",
      "第44次训练\n",
      "执行时间: 55.403913497924805 秒,60次未求解，当前强化学习值为-184125.0,利润为108375.0\n",
      "第45次训练\n",
      "执行时间: 56.32914471626282 秒,60次未求解，当前强化学习值为-194084.0,利润为105716.0\n",
      "第46次训练\n",
      "执行时间: 57.52802777290344 秒,50次未求解，当前强化学习值为22843.0,利润为285343.0\n",
      "第47次训练\n",
      "执行时间: 59.19414758682251 秒,34次未求解，当前强化学习值为410426.0,利润为602826.0\n",
      "第48次训练\n",
      "执行时间: 60.40188717842102 秒,50次未求解，当前强化学习值为54377.0,利润为313777.0\n",
      "第49次训练\n",
      "执行时间: 62.1056182384491 秒,36次未求解，当前强化学习值为353559.0,利润为554559.0\n",
      "第50次训练\n",
      "执行时间: 63.049800157547 秒,60次未求解，当前强化学习值为-187625.0,利润为108375.0\n",
      "第51次训练\n",
      "执行时间: 64.5289478302002 秒,41次未求解，当前强化学习值为236401.0,利润为457601.0\n",
      "第52次训练\n",
      "执行时间: 65.51738786697388 秒,58次未求解，当前强化学习值为-135383.0,利润为147517.0\n",
      "第53次训练\n",
      "执行时间: 67.07062864303589 秒,38次未求解，当前强化学习值为285800.0,利润为499800.0\n",
      "第54次训练\n",
      "执行时间: 68.92661738395691 秒,31次未求解，当前强化学习值为477191.0,利润为654391.0\n",
      "第55次训练\n",
      "执行时间: 70.24348044395447 秒,48次未求解，当前强化学习值为86877.0,利润为339677.0\n",
      "第56次训练\n",
      "执行时间: 71.25049686431885 秒,60次未求解，当前强化学习值为-189797.0,利润为108603.0\n",
      "第57次训练\n",
      "执行时间: 72.19648313522339 秒,60次未求解，当前强化学习值为-186590.0,利润为112310.0\n",
      "第58次训练\n",
      "执行时间: 73.2587080001831 秒,55次未求解，当前强化学习值为-77222.0,利润为201478.0\n",
      "第59次训练\n",
      "执行时间: 74.27636647224426 秒,58次未求解，当前强化学习值为-131425.0,利润为155775.0\n",
      "第60次训练\n",
      "执行时间: 75.9955050945282 秒,32次未求解，当前强化学习值为424336.0,利润为609836.0\n",
      "第61次训练\n",
      "执行时间: 77.16693782806396 秒,54次未求解，当前强化学习值为-50641.0,利润为226659.0\n",
      "第62次训练\n",
      "执行时间: 78.11392188072205 秒,60次未求解，当前强化学习值为-188324.0,利润为108376.0\n",
      "第63次训练\n",
      "执行时间: 79.20863389968872 秒,57次未求解，当前强化学习值为-113257.0,利润为170643.0\n",
      "第64次训练\n",
      "执行时间: 80.21722507476807 秒,58次未求解，当前强化学习值为-134936.0,利润为150664.0\n",
      "第65次训练\n",
      "执行时间: 81.21145439147949 秒,58次未求解，当前强化学习值为-133954.0,利润为149646.0\n",
      "第66次训练\n",
      "执行时间: 82.88465523719788 秒,35次未求解，当前强化学习值为388026.0,利润为581726.0\n",
      "第67次训练\n",
      "执行时间: 83.94495129585266 秒,55次未求解，当前强化学习值为-82790.0,利润为194710.0\n",
      "第68次训练\n",
      "执行时间: 84.91909193992615 秒,60次未求解，当前强化学习值为-189397.0,利润为108603.0\n",
      "第69次训练\n",
      "执行时间: 86.38045382499695 秒,41次未求解，当前强化学习值为235517.0,利润为455017.0\n",
      "第70次训练\n",
      "执行时间: 87.39861249923706 秒,60次未求解，当前强化学习值为-183013.0,利润为112087.0\n",
      "第71次训练\n",
      "执行时间: 88.33405995368958 秒,59次未求解，当前强化学习值为-181862.0,利润为112638.0\n",
      "第72次训练\n",
      "执行时间: 89.58133697509766 秒,51次未求解，当前强化学习值为19955.0,利润为288055.0\n",
      "第73次训练\n",
      "执行时间: 91.56137347221375 秒,23次未求解，当前强化学习值为667000.0,利润为816100.0\n",
      "第74次训练\n",
      "执行时间: 92.61986470222473 秒,58次未求解，当前强化学习值为-135307.0,利润为152993.0\n",
      "第75次训练\n",
      "执行时间: 93.57529735565186 秒,60次未求解，当前强化学习值为-189684.0,利润为105716.0\n",
      "第76次训练\n",
      "执行时间: 94.529616355896 秒,60次未求解，当前强化学习值为-188425.0,利润为108375.0\n",
      "第77次训练\n",
      "执行时间: 95.49549078941345 秒,60次未求解，当前强化学习值为-187397.0,利润为108603.0\n",
      "第78次训练\n",
      "执行时间: 96.7206883430481 秒,47次未求解，当前强化学习值为87333.0,利润为338433.0\n",
      "第79次训练\n",
      "执行时间: 97.80291867256165 秒,55次未求解，当前强化学习值为-72993.0,利润为206507.0\n",
      "第80次训练\n",
      "执行时间: 98.76064229011536 秒,60次未求解，当前强化学习值为-185213.0,利润为112087.0\n",
      "第81次训练\n",
      "执行时间: 99.71186685562134 秒,60次未求解，当前强化学习值为-188866.0,利润为107834.0\n",
      "第82次训练\n",
      "执行时间: 101.1367130279541 秒,44次未求解，当前强化学习值为168940.0,利润为403940.0\n",
      "第83次训练\n",
      "执行时间: 102.72883558273315 秒,37次未求解，当前强化学习值为302257.0,利润为513357.0\n",
      "第84次训练\n",
      "执行时间: 103.68984532356262 秒,60次未求解，当前强化学习值为-185397.0,利润为108603.0\n",
      "第85次训练\n",
      "执行时间: 104.7415132522583 秒,60次未求解，当前强化学习值为-187524.0,利润为108376.0\n",
      "第86次训练\n",
      "执行时间: 105.96233081817627 秒,51次未求解，当前强化学习值为35023.0,利润为293323.0\n",
      "第87次训练\n",
      "执行时间: 106.89941883087158 秒,60次未求解，当前强化学习值为-185897.0,利润为108603.0\n",
      "第88次训练\n",
      "执行时间: 107.8716242313385 秒,60次未求解，当前强化学习值为-186790.0,利润为112310.0\n",
      "第89次训练\n",
      "执行时间: 108.86753177642822 秒,60次未求解，当前强化学习值为-189598.0,利润为108602.0\n",
      "第90次训练\n",
      "执行时间: 110.29196691513062 秒,43次未求解，当前强化学习值为176044.0,利润为412544.0\n",
      "第91次训练\n",
      "执行时间: 111.2357029914856 秒,60次未求解，当前强化学习值为-187525.0,利润为108375.0\n",
      "第92次训练\n",
      "执行时间: 112.51746869087219 秒,50次未求解，当前强化学习值为55062.0,利润为308862.0\n",
      "第93次训练\n",
      "执行时间: 113.96824717521667 秒,43次未求解，当前强化学习值为179440.0,利润为413340.0\n",
      "第94次训练\n",
      "执行时间: 115.20708656311035 秒,54次未求解，当前强化学习值为-46443.0,利润为228857.0\n",
      "第95次训练\n",
      "执行时间: 116.19106864929199 秒,60次未求解，当前强化学习值为-191197.0,利润为108603.0\n",
      "第96次训练\n",
      "执行时间: 117.2844717502594 秒,54次未求解，当前强化学习值为-49093.0,利润为226707.0\n",
      "第97次训练\n",
      "执行时间: 118.26419067382812 秒,60次未求解，当前强化学习值为-188684.0,利润为105716.0\n",
      "第98次训练\n",
      "执行时间: 119.45181608200073 秒,51次未求解，当前强化学习值为25755.0,利润为288455.0\n",
      "第99次训练\n",
      "执行时间: 120.3560893535614 秒,60次未求解，当前强化学习值为-184166.0,利润为107834.0\n",
      "第100次训练\n",
      "最优模型为35\n",
      "执行时间: 121.3204836845398 秒,60次未求解，当前强化学习值为-196920.0,利润为104480.0\n",
      "第101次训练\n",
      "最优模型为35\n",
      "执行时间: 122.23788475990295 秒,60次未求解，当前强化学习值为-192184.0,利润为105716.0\n",
      "第102次训练\n",
      "最优模型为35\n",
      "执行时间: 123.19744944572449 秒,60次未求解，当前强化学习值为-185214.0,利润为112086.0\n",
      "第103次训练\n",
      "最优模型为35\n",
      "执行时间: 124.13381385803223 秒,60次未求解，当前强化学习值为-191384.0,利润为105716.0\n",
      "第104次训练\n",
      "最优模型为35\n",
      "执行时间: 125.08623027801514 秒,60次未求解，当前强化学习值为-186925.0,利润为108375.0\n",
      "第105次训练\n",
      "最优模型为35\n",
      "执行时间: 126.30799269676208 秒,48次未求解，当前强化学习值为79958.0,利润为334558.0\n",
      "第106次训练\n",
      "最优模型为35\n",
      "执行时间: 128.32657599449158 秒,23次未求解，当前强化学习值为639598.0,利润为791798.0\n",
      "第107次训练\n",
      "最优模型为35\n",
      "执行时间: 129.66857504844666 秒,47次未求解，当前强化学习值为98970.99993335613,利润为357870.9999333561\n",
      "第108次训练\n",
      "最优模型为35\n",
      "执行时间: 130.82310009002686 秒,58次未求解，当前强化学习值为-143817.0,利润为146083.0\n",
      "第109次训练\n",
      "最优模型为35\n",
      "执行时间: 131.8901650905609 秒,58次未求解，当前强化学习值为-136629.0,利润为152071.0\n",
      "第110次训练\n",
      "最优模型为35\n",
      "执行时间: 134.40908408164978 秒,7次未求解，当前强化学习值为983056.0,利润为1087856.0\n",
      "第111次训练\n",
      "最优模型为35\n",
      "执行时间: 135.56054759025574 秒,58次未求解，当前强化学习值为-137837.0,利润为150663.0\n",
      "第112次训练\n",
      "最优模型为35\n",
      "执行时间: 136.64506363868713 秒,60次未求解，当前强化学习值为-186825.0,利润为108375.0\n",
      "第113次训练\n",
      "最优模型为35\n",
      "执行时间: 137.77993845939636 秒,54次未求解，当前强化学习值为-60230.0,利润为219370.0\n",
      "第114次训练\n",
      "最优模型为35\n",
      "执行时间: 139.00611233711243 秒,59次未求解，当前强化学习值为-165156.0,利润为127244.0\n",
      "第115次训练\n",
      "最优模型为35\n",
      "执行时间: 140.6410892009735 秒,48次未求解，当前强化学习值为86041.0,利润为339341.0\n",
      "第116次训练\n",
      "最优模型为35\n",
      "执行时间: 142.40051746368408 秒,35次未求解，当前强化学习值为376823.0,利润为571523.0\n",
      "第117次训练\n",
      "最优模型为35\n",
      "执行时间: 143.57704281806946 秒,53次未求解，当前强化学习值为-48617.0,利润为224883.0\n",
      "第118次训练\n",
      "最优模型为35\n",
      "执行时间: 145.75464940071106 秒,21次未求解，当前强化学习值为670918.0,利润为821018.0\n",
      "第119次训练\n",
      "最优模型为35\n",
      "执行时间: 147.02079129219055 秒,50次未求解，当前强化学习值为50265.0,利润为305165.0\n",
      "第120次训练\n",
      "最优模型为35\n",
      "执行时间: 148.03165912628174 秒,60次未求解，当前强化学习值为-191624.0,利润为108376.0\n",
      "第121次训练\n",
      "最优模型为35\n",
      "执行时间: 149.89130783081055 秒,27次未求解，当前强化学习值为552953.0,利润为719653.0\n",
      "第122次训练\n",
      "最优模型为35\n",
      "执行时间: 151.05049443244934 秒,45次未求解，当前强化学习值为160090.0,利润为396590.0\n",
      "第123次训练\n",
      "最优模型为35\n",
      "执行时间: 151.98367047309875 秒,60次未求解，当前强化学习值为-191197.0,利润为108603.0\n",
      "第124次训练\n",
      "最优模型为35\n",
      "执行时间: 152.91549134254456 秒,58次未求解，当前强化学习值为-134437.0,利润为150663.0\n",
      "第125次训练\n",
      "最优模型为35\n",
      "执行时间: 154.26281690597534 秒,38次未求解，当前强化学习值为302843.0,利润为512143.0\n",
      "第126次训练\n",
      "最优模型为35\n",
      "执行时间: 155.22281908988953 秒,58次未求解，当前强化学习值为-136437.0,利润为150663.0\n",
      "第127次训练\n",
      "最优模型为35\n",
      "执行时间: 156.25933933258057 秒,52次未求解，当前强化学习值为-13601.0,利润为256899.0\n",
      "第128次训练\n",
      "最优模型为35\n",
      "执行时间: 157.1459345817566 秒,58次未求解，当前强化学习值为-143435.0,利润为143665.0\n",
      "第129次训练\n",
      "最优模型为35\n",
      "执行时间: 158.03922820091248 秒,60次未求解，当前强化学习值为-187925.0,利润为108375.0\n",
      "第130次训练\n",
      "最优模型为35\n",
      "执行时间: 158.92097401618958 秒,60次未求解，当前强化学习值为-193284.0,利润为105716.0\n",
      "第131次训练\n",
      "最优模型为35\n",
      "执行时间: 159.84232568740845 秒,60次未求解，当前强化学习值为-185489.0,利润为112311.0\n",
      "第132次训练\n",
      "最优模型为35\n",
      "执行时间: 160.9287087917328 秒,54次未求解，当前强化学习值为-55947.0,利润为224053.0\n",
      "第133次训练\n",
      "最优模型为35\n",
      "执行时间: 162.09905314445496 秒,51次未求解，当前强化学习值为27708.0,利润为290308.0\n",
      "第134次训练\n",
      "最优模型为35\n",
      "执行时间: 163.93159818649292 秒,18次未求解，当前强化学习值为766780.0,利润为900680.0\n",
      "第135次训练\n",
      "最优模型为35\n",
      "执行时间: 164.9489631652832 秒,60次未求解，当前强化学习值为-187066.0,利润为107834.0\n",
      "第136次训练\n",
      "最优模型为35\n",
      "执行时间: 166.57515454292297 秒,37次未求解，当前强化学习值为310584.0,利润为519484.0\n",
      "第137次训练\n",
      "最优模型为35\n",
      "执行时间: 167.5441632270813 秒,60次未求解，当前强化学习值为-183913.0,利润为112087.0\n",
      "第138次训练\n",
      "最优模型为35\n",
      "执行时间: 168.517085313797 秒,60次未求解，当前强化学习值为-181515.0,利润为112085.0\n",
      "第139次训练\n",
      "最优模型为35\n",
      "执行时间: 169.47752785682678 秒,60次未求解，当前强化学习值为-189025.0,利润为108375.0\n",
      "第140次训练\n",
      "最优模型为35\n",
      "执行时间: 170.4704098701477 秒,58次未求解，当前强化学习值为-145145.0,利润为143955.0\n",
      "第141次训练\n",
      "最优模型为35\n",
      "执行时间: 171.61494302749634 秒,57次未求解，当前强化学习值为-123261.0,利润为163339.0\n",
      "第142次训练\n",
      "最优模型为35\n",
      "执行时间: 172.6714940071106 秒,56次未求解，当前强化学习值为-82394.0,利润为196206.0\n",
      "第143次训练\n",
      "最优模型为35\n",
      "执行时间: 174.24093580245972 秒,40次未求解，当前强化学习值为265885.0,利润为482185.0\n",
      "第144次训练\n",
      "最优模型为35\n",
      "执行时间: 175.2280457019806 秒,59次未求解，当前强化学习值为-164656.0,利润为127244.0\n",
      "第145次训练\n",
      "最优模型为35\n",
      "执行时间: 177.02452993392944 秒,30次未求解，当前强化学习值为494887.0,利润为671987.0\n",
      "第146次训练\n",
      "最优模型为35\n",
      "执行时间: 178.81011962890625 秒,32次未求解，当前强化学习值为454655.0,利润为637055.0\n",
      "第147次训练\n",
      "最优模型为35\n",
      "执行时间: 179.76635479927063 秒,60次未求解，当前强化学习值为-184214.0,利润为112086.0\n",
      "第148次训练\n",
      "最优模型为35\n",
      "执行时间: 180.75130462646484 秒,60次未求解，当前强化学习值为-184889.0,利润为112311.0\n",
      "第149次训练\n",
      "最优模型为35\n",
      "执行时间: 181.89295649528503 秒,51次未求解，当前强化学习值为17677.0,利润为282877.0\n",
      "第150次训练\n",
      "最优模型为35\n",
      "执行时间: 182.9985318183899 秒,54次未求解，当前强化学习值为-56781.0,利润为219019.0\n",
      "第151次训练\n",
      "最优模型为35\n",
      "执行时间: 184.2939157485962 秒,48次未求解，当前强化学习值为107515.0,利润为354815.0\n",
      "第152次训练\n",
      "最优模型为35\n",
      "执行时间: 185.66520404815674 秒,43次未求解，当前强化学习值为196426.0,利润为428626.0\n",
      "第153次训练\n",
      "最优模型为35\n",
      "执行时间: 186.6812460422516 秒,58次未求解，当前强化学习值为-137489.0,利润为147511.0\n",
      "第154次训练\n",
      "最优模型为35\n",
      "执行时间: 187.61258220672607 秒,60次未求解，当前强化学习值为-192720.0,利润为104480.0\n",
      "第155次训练\n",
      "最优模型为35\n",
      "执行时间: 188.56989240646362 秒,60次未求解，当前强化学习值为-195420.0,利润为104480.0\n",
      "第156次训练\n",
      "最优模型为35\n",
      "执行时间: 189.5723340511322 秒,58次未求解，当前强化学习值为-135482.0,利润为147518.0\n",
      "第157次训练\n",
      "最优模型为35\n",
      "执行时间: 190.57703518867493 秒,59次未求解，当前强化学习值为-162056.0,利润为127244.0\n",
      "第158次训练\n",
      "最优模型为35\n",
      "执行时间: 191.57029938697815 秒,58次未求解，当前强化学习值为-131497.0,利润为151303.0\n",
      "第159次训练\n",
      "最优模型为35\n",
      "执行时间: 193.18104481697083 秒,37次未求解，当前强化学习值为314272.0,利润为528572.0\n",
      "第160次训练\n",
      "最优模型为35\n",
      "执行时间: 194.1245415210724 秒,60次未求解，当前强化学习值为-192543.0,利润为104257.0\n",
      "第161次训练\n",
      "最优模型为35\n",
      "执行时间: 196.5143163204193 秒,9次未求解，当前强化学习值为990179.0,利润为1085879.0\n",
      "第162次训练\n",
      "最优模型为35\n",
      "执行时间: 197.64509797096252 秒,58次未求解，当前强化学习值为-129827.0,利润为155773.0\n",
      "第163次训练\n",
      "最优模型为35\n",
      "执行时间: 198.55867409706116 秒,60次未求解，当前强化学习值为-183925.0,利润为108375.0\n",
      "第164次训练\n",
      "最优模型为35\n",
      "执行时间: 199.6782088279724 秒,58次未求解，当前强化学习值为-146245.0,利润为143955.0\n",
      "第165次训练\n",
      "最优模型为35\n",
      "执行时间: 200.66531991958618 秒,59次未求解，当前强化学习值为-166653.0,利润为127247.0\n",
      "第166次训练\n",
      "最优模型为35\n",
      "执行时间: 201.62729954719543 秒,60次未求解，当前强化学习值为-186589.0,利润为112311.0\n",
      "第167次训练\n",
      "最优模型为35\n",
      "执行时间: 202.84313583374023 秒,51次未求解，当前强化学习值为36776.0,利润为297976.0\n",
      "第168次训练\n",
      "最优模型为35\n",
      "执行时间: 203.75165796279907 秒,60次未求解，当前强化学习值为-185115.0,利润为112085.0\n",
      "第169次训练\n",
      "最优模型为35\n",
      "执行时间: 205.69684290885925 秒,6次未求解，当前强化学习值为1047551.0,利润为1136751.0\n",
      "第170次训练\n",
      "最优模型为35\n",
      "执行时间: 206.5575942993164 秒,60次未求解，当前强化学习值为-188198.0,利润为108602.0\n",
      "第171次训练\n",
      "最优模型为35\n",
      "执行时间: 207.49178290367126 秒,58次未求解，当前强化学习值为-148817.0,利润为146083.0\n",
      "第172次训练\n",
      "最优模型为35\n",
      "执行时间: 208.49127411842346 秒,60次未求解，当前强化学习值为-189425.0,利润为108375.0\n",
      "第173次训练\n",
      "最优模型为35\n",
      "执行时间: 209.3841781616211 秒,58次未求解，当前强化学习值为-139403.0,利润为152997.0\n",
      "第174次训练\n",
      "最优模型为35\n",
      "执行时间: 210.64575910568237 秒,43次未求解，当前强化学习值为169834.0,利润为408134.0\n",
      "第175次训练\n",
      "最优模型为35\n",
      "执行时间: 211.51969575881958 秒,60次未求解，当前强化学习值为-193766.0,利润为107234.0\n",
      "第176次训练\n",
      "最优模型为35\n",
      "执行时间: 212.39733839035034 秒,58次未求解，当前强化学习值为-138903.0,利润为145397.0\n",
      "第177次训练\n",
      "最优模型为35\n",
      "执行时间: 213.3943166732788 秒,54次未求解，当前强化学习值为-56505.0,利润为221195.0\n",
      "第178次训练\n",
      "最优模型为35\n",
      "执行时间: 214.3564896583557 秒,54次未求解，当前强化学习值为-58473.0,利润为222027.0\n",
      "第179次训练\n",
      "最优模型为35\n",
      "执行时间: 215.46778202056885 秒,45次未求解，当前强化学习值为152563.0,利润为390763.0\n",
      "第180次训练\n",
      "最优模型为35\n",
      "执行时间: 216.35304832458496 秒,60次未求解，当前强化学习值为-190566.0,利润为107834.0\n",
      "第181次训练\n",
      "最优模型为35\n",
      "执行时间: 217.1901683807373 秒,60次未求解，当前强化学习值为-189566.0,利润为107834.0\n",
      "第182次训练\n",
      "最优模型为35\n",
      "执行时间: 218.28790140151978 秒,47次未求解，当前强化学习值为102841.0,利润为351441.0\n",
      "第183次训练\n",
      "最优模型为35\n",
      "执行时间: 219.1589617729187 秒,60次未求解，当前强化学习值为-186188.0,利润为112312.0\n",
      "第184次训练\n",
      "最优模型为35\n",
      "执行时间: 220.04464960098267 秒,58次未求解，当前强化学习值为-137583.0,利润为149517.0\n",
      "第185次训练\n",
      "最优模型为35\n",
      "执行时间: 220.87177443504333 秒,60次未求解，当前强化学习值为-185797.0,利润为108603.0\n",
      "第186次训练\n",
      "最优模型为35\n",
      "执行时间: 221.76338267326355 秒,58次未求解，当前强化学习值为-136153.0,利润为149647.0\n",
      "第187次训练\n",
      "最优模型为35\n",
      "执行时间: 222.57552814483643 秒,60次未求解，当前强化学习值为-195184.0,利润为105716.0\n",
      "第188次训练\n",
      "最优模型为35\n",
      "执行时间: 223.44258451461792 秒,59次未求解，当前强化学习值为-161479.0,利润为129321.0\n",
      "第189次训练\n",
      "最优模型为35\n",
      "执行时间: 224.59621119499207 秒,43次未求解，当前强化学习值为178706.0,利润为416006.0\n",
      "第190次训练\n",
      "最优模型为35\n",
      "执行时间: 225.45598483085632 秒,60次未求解，当前强化学习值为-192425.0,利润为108375.0\n",
      "第191次训练\n",
      "最优模型为35\n",
      "执行时间: 226.28961968421936 秒,60次未求解，当前强化学习值为-189966.0,利润为107834.0\n",
      "第192次训练\n",
      "最优模型为35\n",
      "执行时间: 228.23953676223755 秒,31次未求解，当前强化学习值为482222.0,利润为660722.0\n",
      "第193次训练\n",
      "最优模型为35\n",
      "执行时间: 229.31199431419373 秒,59次未求解，当前强化学习值为-165180.0,利润为129320.0\n",
      "第194次训练\n",
      "最优模型为35\n",
      "执行时间: 230.2102234363556 秒,60次未求解，当前强化学习值为-182688.0,利润为112312.0\n",
      "第195次训练\n",
      "最优模型为35\n",
      "执行时间: 231.1345226764679 秒,60次未求解，当前强化学习值为-181087.0,利润为112313.0\n",
      "第196次训练\n",
      "最优模型为35\n",
      "执行时间: 232.0563771724701 秒,60次未求解，当前强化学习值为-192484.0,利润为105716.0\n",
      "第197次训练\n",
      "最优模型为35\n",
      "执行时间: 233.13523602485657 秒,51次未求解，当前强化学习值为38378.0,利润为297678.0\n",
      "第198次训练\n",
      "最优模型为35\n",
      "执行时间: 233.98434162139893 秒,60次未求解，当前强化学习值为-191384.0,利润为105716.0\n",
      "第199次训练\n",
      "最优模型为35\n",
      "执行时间: 235.4276783466339 秒,34次未求解，当前强化学习值为398089.0,利润为596089.0\n",
      "第200次训练\n",
      "最优模型为35\n",
      "执行时间: 236.49652671813965 秒,51次未求解，当前强化学习值为42467.0,利润为296067.0\n",
      "第201次训练\n",
      "最优模型为35\n",
      "执行时间: 237.3912591934204 秒,58次未求解，当前强化学习值为-137200.0,利润为151300.0\n",
      "第202次训练\n",
      "最优模型为35\n",
      "执行时间: 238.45400261878967 秒,52次未求解，当前强化学习值为-18412.0,利润为255788.0\n",
      "第203次训练\n",
      "最优模型为35\n",
      "执行时间: 239.55296993255615 秒,47次未求解，当前强化学习值为117494.0,利润为364394.0\n",
      "第204次训练\n",
      "最优模型为35\n",
      "执行时间: 240.5828664302826 秒,51次未求解，当前强化学习值为9457.0,利润为275757.0\n",
      "第205次训练\n",
      "最优模型为35\n",
      "执行时间: 241.49330592155457 秒,60次未求解，当前强化学习值为-190498.0,利润为108602.0\n",
      "第206次训练\n",
      "最优模型为35\n",
      "执行时间: 242.33976364135742 秒,60次未求解，当前强化学习值为-183399.0,利润为108601.0\n",
      "第207次训练\n",
      "最优模型为35\n",
      "执行时间: 243.2067174911499 秒,60次未求解，当前强化学习值为-191443.0,利润为104257.0\n",
      "第208次训练\n",
      "最优模型为35\n",
      "执行时间: 244.13494753837585 秒,59次未求解，当前强化学习值为-161679.0,利润为129321.0\n",
      "第209次训练\n",
      "最优模型为35\n",
      "执行时间: 245.13988399505615 秒,51次未求解，当前强化学习值为23954.0,利润为286754.0\n",
      "第210次训练\n",
      "最优模型为35\n",
      "执行时间: 246.03105187416077 秒,60次未求解，当前强化学习值为-183013.0,利润为112087.0\n",
      "第211次训练\n",
      "最优模型为35\n",
      "执行时间: 247.0749294757843 秒,48次未求解，当前强化学习值为76701.0,利润为330601.0\n",
      "第212次训练\n",
      "最优模型为35\n",
      "执行时间: 248.1468517780304 秒,49次未求解，当前强化学习值为59010.0,利润为318010.0\n",
      "第213次训练\n",
      "最优模型为35\n",
      "执行时间: 249.4684054851532 秒,41次未求解，当前强化学习值为202808.0,利润为437508.0\n",
      "第214次训练\n",
      "最优模型为35\n",
      "执行时间: 250.35275077819824 秒,60次未求解，当前强化学习值为-189198.0,利润为108602.0\n",
      "第215次训练\n",
      "最优模型为35\n",
      "执行时间: 252.0311381816864 秒,24次未求解，当前强化学习值为648412.0,利润为803012.0\n",
      "第216次训练\n",
      "最优模型为35\n",
      "执行时间: 253.1603446006775 秒,51次未求解，当前强化学习值为48759.0,利润为307659.0\n",
      "第217次训练\n",
      "最优模型为35\n",
      "执行时间: 254.5544149875641 秒,40次未求解，当前强化学习值为270620.0,利润为488320.0\n",
      "第218次训练\n",
      "最优模型为35\n",
      "执行时间: 255.55295395851135 秒,59次未求解，当前强化学习值为-163656.0,利润为127244.0\n",
      "第219次训练\n",
      "最优模型为35\n",
      "执行时间: 256.5672302246094 秒,58次未求解，当前强化学习值为-140753.0,利润为149647.0\n",
      "第220次训练\n",
      "最优模型为35\n",
      "执行时间: 257.70841789245605 秒,48次未求解，当前强化学习值为73917.0,利润为328117.0\n",
      "第221次训练\n",
      "最优模型为35\n",
      "执行时间: 258.69633173942566 秒,59次未求解，当前强化学习值为-163358.0,利润为127242.0\n",
      "第222次训练\n",
      "最优模型为35\n",
      "执行时间: 260.21940994262695 秒,43次未求解，当前强化学习值为198826.0,利润为425426.0\n",
      "第223次训练\n",
      "最优模型为35\n",
      "执行时间: 261.903112411499 秒,33次未求解，当前强化学习值为405308.0,利润为600808.0\n",
      "第224次训练\n",
      "最优模型为35\n",
      "执行时间: 262.8095097541809 秒,60次未求解，当前强化学习值为-185625.0,利润为108375.0\n",
      "第225次训练\n",
      "最优模型为35\n",
      "执行时间: 263.83556151390076 秒,57次未求解，当前强化学习值为-123961.0,利润为163339.0\n",
      "第226次训练\n",
      "最优模型为35\n",
      "执行时间: 265.05682849884033 秒,51次未求解，当前强化学习值为40659.0,利润为299259.0\n",
      "第227次训练\n",
      "最优模型为35\n",
      "执行时间: 265.98778200149536 秒,60次未求解，当前强化学习值为-182715.0,利润为112085.0\n",
      "第228次训练\n",
      "最优模型为35\n",
      "执行时间: 267.057181596756 秒,55次未求解，当前强化学习值为-48599.0,利润为218201.0\n",
      "第229次训练\n",
      "最优模型为35\n",
      "执行时间: 267.9928729534149 秒,60次未求解，当前强化学习值为-194184.0,利润为105716.0\n",
      "第230次训练\n",
      "最优模型为35\n",
      "执行时间: 268.964234828949 秒,60次未求解，当前强化学习值为-187866.0,利润为107834.0\n",
      "第231次训练\n",
      "最优模型为35\n",
      "执行时间: 269.9218215942383 秒,60次未求解，当前强化学习值为-193943.0,利润为104257.0\n",
      "第232次训练\n",
      "最优模型为35\n",
      "执行时间: 270.89600133895874 秒,60次未求解，当前强化学习值为-191198.0,利润为108602.0\n",
      "第233次训练\n",
      "最优模型为35\n",
      "执行时间: 272.1765356063843 秒,48次未求解，当前强化学习值为79180.0,利润为333880.0\n",
      "第234次训练\n",
      "最优模型为35\n",
      "执行时间: 273.50864362716675 秒,47次未求解，当前强化学习值为109717.0,利润为353317.0\n",
      "第235次训练\n",
      "最优模型为35\n",
      "执行时间: 274.6288814544678 秒,54次未求解，当前强化学习值为-55096.0,利润为221304.0\n",
      "第236次训练\n",
      "最优模型为35\n",
      "执行时间: 275.55756282806396 秒,60次未求解，当前强化学习值为-181387.0,利润为112313.0\n",
      "第237次训练\n",
      "最优模型为35\n",
      "执行时间: 276.5299873352051 秒,60次未求解，当前强化学习值为-185989.0,利润为112311.0\n",
      "第238次训练\n",
      "最优模型为35\n",
      "执行时间: 277.5316972732544 秒,60次未求解，当前强化学习值为-186115.0,利润为112085.0\n",
      "第239次训练\n",
      "最优模型为35\n",
      "执行时间: 278.9533004760742 秒,42次未求解，当前强化学习值为211392.0,利润为440392.0\n",
      "第240次训练\n",
      "最优模型为35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 151\u001b[0m\n\u001b[0;32m    149\u001b[0m     next_order_states \u001b[38;5;241m=\u001b[39m vectorization_order(orders_unmatched)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# 这里防止梯度爆炸缩小了reward\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgrid_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_vehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_order_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     env\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m=\u001b[39m time\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:167\u001b[0m, in \u001b[0;36mMultiAgentAC.update\u001b[1;34m(self, vehicle_states, order_states, actions, rewards, next_vehicle_states, next_order_states, dones)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 167\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dirty_test\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dirty_test\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid >= invalid_time:\n",
    "                    agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "            # 一个循环代码让我达到最优\n",
    "            # 屁股后面的代码是为了让我达到最优\n",
    "            # 这里是为了让我达到最优\n",
    "            # 现在放弃了重采样\n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        torch.save(agent, f\"model_checkpoint_{episode}.pth\")\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:alns.ALNS:Finished iterating in 0.15s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best routes: [[0, 1, 0], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0]]\n",
      "Best cost: 858.1198159028768\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from alns import ALNS, State\n",
    "from alns.accept import SimulatedAnnealing\n",
    "from alns.stop import MaxIterations\n",
    "from alns.select import RouletteWheel\n",
    "import random\n",
    "\n",
    "# Problem Data\n",
    "np.random.seed(42)\n",
    "num_customers = 10\n",
    "num_vehicles = 3\n",
    "capacity = 100\n",
    "\n",
    "# 随机生成坐标和需求\n",
    "depot = np.array([50, 50])\n",
    "nodes = np.random.randint(0, 100, (num_customers, 2))\n",
    "demands = np.random.randint(5, 20, num_customers)\n",
    "\n",
    "# 计算欧几里得距离\n",
    "def distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "distance_matrix = np.zeros((num_customers + 1, num_customers + 1))\n",
    "nodes_full = np.vstack([depot, nodes])  # 加入仓库\n",
    "for i in range(len(nodes_full)):\n",
    "    for j in range(len(nodes_full)):\n",
    "        distance_matrix[i, j] = distance(nodes_full[i], nodes_full[j])\n",
    "\n",
    "# 初始状态\n",
    "class VRPState(State):\n",
    "    def __init__(self, routes):\n",
    "        self.routes = routes\n",
    "    \n",
    "    def objective(self):\n",
    "        total_cost = sum(\n",
    "            distance_matrix[route[i], route[i+1]]\n",
    "            for route in self.routes for i in range(len(route)-1)\n",
    "        )\n",
    "        return total_cost\n",
    "    \n",
    "    def copy(self):\n",
    "        return VRPState([route[:] for route in self.routes])\n",
    "\n",
    "# 破坏算子\n",
    "def random_removal(state, rng, num_remove=2):\n",
    "    if isinstance(num_remove, np.random.Generator):  \n",
    "        num_remove = rng.integers(1, 4)  \n",
    "\n",
    "    new_state = state.copy()\n",
    "    for _ in range(num_remove):\n",
    "        if any(new_state.routes):\n",
    "            route = random.choice(new_state.routes)\n",
    "            if len(route) > 2:  # 只有在长度 > 2 时才移除\n",
    "                idx = rng.integers(1, max(2, len(route) - 1))  # 确保 idx 合法\n",
    "                route.pop(idx)\n",
    "    return new_state\n",
    "\n",
    "\n",
    "\n",
    "# 修复算子\n",
    "def greedy_insert(state, rng):\n",
    "    new_state = state.copy()\n",
    "    unassigned = [i for i in range(1, num_customers + 1) if not any(i in r for r in new_state.routes)]\n",
    "    for i in unassigned:\n",
    "        best_cost = float('inf')\n",
    "        best_route = None\n",
    "        best_position = None\n",
    "        for route in new_state.routes:\n",
    "            for pos in range(1, len(route)):\n",
    "                temp_route = route[:pos] + [i] + route[pos:]\n",
    "                cost = sum(distance_matrix[temp_route[j], temp_route[j+1]] for j in range(len(temp_route)-1))\n",
    "                if cost < best_cost:\n",
    "                    best_cost, best_route, best_position = cost, route, pos\n",
    "        if best_route is not None:\n",
    "            best_route.insert(best_position, i)\n",
    "    return new_state\n",
    "\n",
    "# ALNS 运行\n",
    "initial_routes = [[0, i, 0] for i in range(1, num_customers + 1)]  # 每个客户单独一辆车\n",
    "initial_state = VRPState(initial_routes)\n",
    "alns = ALNS()\n",
    "alns.add_destroy_operator(random_removal)\n",
    "alns.add_repair_operator(greedy_insert)\n",
    "\n",
    "# 设定接受准则（模拟退火）\n",
    "accept = SimulatedAnnealing(1000, 1, 500, method=\"linear\")\n",
    "select = RouletteWheel([1] * 4, 0.8, 1, 1)\n",
    "stop = MaxIterations(1000)\n",
    "\n",
    "result = alns.iterate(initial_state, select, accept, stop)\n",
    "\n",
    "# 输出最优解\n",
    "best_state = result.best_state\n",
    "print(\"Best routes:\", best_state.routes)\n",
    "print(\"Best cost:\", best_state.objective())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据固定的逐步调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# 假设有如下初始化函数\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 生成固定样本\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G, speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 保存样本到本地\n",
    "with open('sample_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'Vehicles': Vehicles,\n",
    "        'Total_order': Total_order,\n",
    "        'G':G\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'MARL_BASE.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为569776.0,101次未求解\n",
      "第1次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 0, 7, 0, 7, 7, 0]\n",
      "[21, 7, 0, 2, 7, 7, 7, 0]\n",
      "[15, 7, 0, 0, 7, 1, 0, 0]\n",
      "[7, 7, 0, 0, 7, 0, 0, 0]\n",
      "[1, 0, 0, 0, 4, 7, 0, 0]\n",
      "[0, 0, 0, 0, 0, 3, 0, 0]\n",
      "[0, 0, 0, 1, 1, 0, 0, 0]\n",
      "[0, 0, 3, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 2, 0]\n",
      "[0, 0, 3, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 3, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 2, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 2.333763837814331 秒,107次未求解，当前强化学习值为456085.0,利润为456085.0\n",
      "第2次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 1, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 7, 7, 0, 0, 7, 0]\n",
      "[21, 7, 7, 9, 7, 0, 0, 0]\n",
      "[21, 2, 0, 7, 0, 0, 0, 0]\n",
      "[21, 0, 0, 0, 0, 0, 0, 0]\n",
      "[9, 0, 0, 0, 0, 3, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[1, 0, 1, 0, 1, 0, 0, 0]\n",
      "[1, 2, 0, 3, 0, 0, 0, 0]\n",
      "[0, 0, 4, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 0, 2, 0, 0, 0, 2, 0]\n",
      "[0, 0, 3, 0, 0, 0, 0, 0]\n",
      "[0, 0, 2, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 2, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 2.965988874435425 秒,120次未求解，当前强化学习值为220610.0,利润为220610.0\n",
      "第3次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 7, 14, 0, 0, 7, 0]\n",
      "[21, 7, 7, 7, 2, 0, 7, 0]\n",
      "[21, 2, 0, 0, 0, 0, 7, 0]\n",
      "[14, 0, 7, 0, 0, 0, 0, 0]\n",
      "[2, 0, 1, 0, 0, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 3.381150007247925 秒,131次未求解，当前强化学习值为-14445.0,利润为-14445.0\n",
      "第4次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 0, 7, 0, 7, 7, 0]\n",
      "[21, 7, 0, 2, 7, 7, 7, 0]\n",
      "[15, 7, 0, 0, 7, 1, 0, 0]\n",
      "[7, 7, 0, 0, 7, 0, 0, 0]\n",
      "[1, 0, 0, 0, 4, 7, 0, 0]\n",
      "[0, 0, 0, 0, 0, 3, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 3, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 4.082542896270752 秒,118次未求解，当前强化学习值为262033.0,利润为262033.0\n",
      "第5次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 0, 7, 0, 7, 7, 0]\n",
      "[21, 7, 0, 2, 7, 7, 7, 0]\n",
      "[15, 7, 0, 0, 7, 1, 0, 0]\n",
      "[7, 7, 0, 0, 7, 0, 0, 0]\n",
      "[1, 0, 0, 0, 4, 7, 0, 0]\n",
      "[0, 0, 0, 0, 0, 3, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 2, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 4.800032377243042 秒,117次未求解，当前强化学习值为269880.0,利润为269880.0\n",
      "第6次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 0, 7, 0, 7, 7, 0]\n",
      "[21, 7, 0, 2, 7, 7, 7, 0]\n",
      "[15, 7, 0, 0, 7, 1, 0, 0]\n",
      "[7, 7, 0, 0, 7, 0, 0, 0]\n",
      "[1, 0, 0, 0, 4, 7, 0, 0]\n",
      "[0, 0, 0, 0, 0, 3, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 4, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 2, 0, 1, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 5, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[1, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 6.490978240966797 秒,77次未求解，当前强化学习值为1047112.0,利润为1047112.0\n",
      "第7次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 7, 14, 0, 0, 7, 0]\n",
      "[21, 7, 7, 7, 2, 0, 7, 0]\n",
      "[21, 2, 0, 0, 0, 0, 7, 0]\n",
      "[14, 0, 7, 0, 0, 0, 0, 0]\n",
      "[2, 0, 1, 0, 0, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 6.912146806716919 秒,131次未求解，当前强化学习值为-14444.0,利润为-14444.0\n",
      "第8次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 7, 14, 0, 0, 7, 0]\n",
      "[21, 7, 7, 7, 2, 0, 7, 0]\n",
      "[21, 2, 0, 0, 0, 0, 7, 0]\n",
      "[14, 0, 7, 0, 0, 0, 0, 0]\n",
      "[2, 0, 1, 0, 0, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 2, 0, 0, 0, 0, 0]\n",
      "[7, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 3, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 2, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 2, 0, 0, 3, 0]\n",
      "[0, 1, 0, 0, 0, 0, 3, 0]\n",
      "[0, 1, 0, 0, 0, 0, 3, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 2, 0, 0, 0]\n",
      "[1, 1, 0, 0, 2, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 3, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 2, 0, 1, 0, 0]\n",
      "[0, 0, 1, 2, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 4, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 3, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 2, 2, 0, 0, 0, 0, 0]\n",
      "[2, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 9.205718517303467 秒,63次未求解，当前强化学习值为1291685.0,利润为1291685.0\n",
      "第9次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 0, 7, 0, 7, 7, 0]\n",
      "[21, 7, 0, 2, 7, 7, 7, 0]\n",
      "[15, 7, 0, 0, 7, 1, 0, 0]\n",
      "[7, 7, 0, 0, 7, 0, 0, 0]\n",
      "[1, 0, 0, 0, 4, 7, 0, 0]\n",
      "[0, 0, 0, 0, 0, 3, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 3, 1, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 9.82104754447937 秒,122次未求解，当前强化学习值为160291.0,利润为160291.0\n",
      "第10次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 7, 14, 0, 0, 7, 0]\n",
      "[21, 7, 7, 7, 2, 0, 7, 0]\n",
      "[21, 2, 0, 0, 0, 0, 7, 0]\n",
      "[14, 0, 7, 0, 0, 0, 0, 0]\n",
      "[2, 0, 1, 0, 0, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 7, 0, 0, 0]\n",
      "[0, 0, 0, 0, 4, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 3, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 2, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 2, 0, 0, 0, 0, 3, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 1, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 3, 3, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[1, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 11.118277072906494 秒,93次未求解，当前强化学习值为722553.0,利润为722553.0\n",
      "第11次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 7, 14, 0, 0, 7, 0]\n",
      "[21, 7, 7, 7, 2, 0, 7, 0]\n",
      "[21, 2, 0, 0, 0, 0, 7, 0]\n",
      "[14, 0, 7, 0, 0, 0, 0, 0]\n",
      "[2, 0, 1, 0, 0, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 3, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 2, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 11.887090682983398 秒,117次未求解，当前强化学习值为270917.0,利润为270917.0\n",
      "第12次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 7, 14, 0, 0, 7, 0]\n",
      "[21, 7, 7, 7, 2, 0, 7, 0]\n",
      "[21, 2, 0, 0, 0, 0, 7, 0]\n",
      "[14, 0, 7, 0, 0, 0, 0, 0]\n",
      "[2, 0, 1, 0, 0, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 7, 0, 0, 0]\n",
      "[0, 0, 0, 0, 4, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 2, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 3, 0, 2, 0, 0]\n",
      "[0, 1, 1, 0, 0, 4, 1, 0]\n",
      "[0, 1, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 3, 2, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 2, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 2, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 3, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 2, 0, 0]\n",
      "[2, 0, 0, 0, 0, 0, 2, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 3, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 3, 2, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "执行时间: 13.891269207000732 秒,66次未求解，当前强化学习值为1234534.0,利润为1234534.0\n",
      "第13次训练\n",
      "[28, 21, 21, 7, 3, 14, 28, 1]\n",
      "[28, 10, 21, 7, 0, 14, 21, 0]\n",
      "[22, 7, 15, 7, 0, 14, 14, 0]\n",
      "[21, 8, 14, 7, 0, 7, 7, 0]\n",
      "[21, 7, 7, 14, 0, 0, 7, 0]\n",
      "[21, 7, 7, 7, 2, 0, 7, 0]\n",
      "[21, 2, 0, 0, 0, 0, 7, 0]\n",
      "[14, 0, 7, 0, 0, 0, 0, 0]\n",
      "[2, 0, 1, 0, 0, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 3, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 2, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[3, 0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 2, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 3, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import MARL_BASE as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM_VEHICLE+STATE_DIM_ORDER\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        \n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            if last_vehicle:\n",
    "                \n",
    "                agent.update_third(vehicle_states, order_states, action, selected_log_probs, log_probs, probs,\n",
    "                            grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            if len(group[0]) != 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                mask = env.get_mask(orders_unmatched)\n",
    "                # 注意这里有多种take_action形式\n",
    "                action, selected_log_probs, log_probs, probs = agent.take_action_vehicle(vehicle_states, order_states, mask, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "                ACTIONS.append(action) \n",
    "\n",
    "        \n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else :\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # grid_reward =   (objval-base_revenue[time])/1000\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "           \n",
    "       \n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ppo.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为569776.0,101次未求解\n",
      "第1次训练\n",
      "执行时间: 2.099555492401123 秒,125次未求解，当前强化学习值为109770.0,利润为109770.0\n",
      "第2次训练\n",
      "执行时间: 3.600386381149292 秒,84次未求解，当前强化学习值为909874.0,利润为909874.0\n",
      "第3次训练\n",
      "执行时间: 4.82237696647644 秒,95次未求解，当前强化学习值为697123.0,利润为697123.0\n",
      "第4次训练\n",
      "执行时间: 5.321692228317261 秒,128次未求解，当前强化学习值为50047.0,利润为50047.0\n",
      "第5次训练\n",
      "执行时间: 6.3974316120147705 秒,101次未求解，当前强化学习值为581304.0,利润为581304.0\n",
      "第6次训练\n",
      "执行时间: 6.854481935501099 秒,128次未求解，当前强化学习值为48468.0,利润为48468.0\n",
      "第7次训练\n",
      "执行时间: 7.520401954650879 秒,118次未求解，当前强化学习值为258541.0,利润为258541.0\n",
      "第8次训练\n",
      "执行时间: 8.176277875900269 秒,118次未求解，当前强化学习值为255749.0,利润为255749.0\n",
      "第9次训练\n",
      "执行时间: 8.810486793518066 秒,122次未求解，当前强化学习值为162252.0,利润为162252.0\n",
      "第10次训练\n",
      "执行时间: 9.574955701828003 秒,113次未求解，当前强化学习值为340865.0,利润为340865.0\n",
      "第11次训练\n",
      "执行时间: 13.014147758483887 秒,21次未求解，当前强化学习值为2125014.0,利润为2125014.0\n",
      "第12次训练\n",
      "执行时间: 13.391227960586548 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第13次训练\n",
      "执行时间: 13.979040145874023 秒,124次未求解，当前强化学习值为128873.0,利润为128873.0\n",
      "第14次训练\n",
      "执行时间: 15.122689008712769 秒,98次未求解，当前强化学习值为640858.0,利润为640858.0\n",
      "第15次训练\n",
      "执行时间: 17.511192083358765 秒,54次未求解，当前强化学习值为1473594.0,利润为1473594.0\n",
      "第16次训练\n",
      "执行时间: 17.895281314849854 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第17次训练\n",
      "执行时间: 18.289573192596436 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第18次训练\n",
      "执行时间: 20.123640298843384 秒,75次未求解，当前强化学习值为1092282.0,利润为1092282.0\n",
      "第19次训练\n",
      "执行时间: 20.928426265716553 秒,117次未求解，当前强化学习值为256324.0,利润为256324.0\n",
      "第20次训练\n",
      "执行时间: 21.562841653823853 秒,120次未求解，当前强化学习值为211272.0,利润为211272.0\n",
      "第21次训练\n",
      "执行时间: 21.93136167526245 秒,135次未求解，当前强化学习值为-96507.0,利润为-96507.0\n",
      "第22次训练\n",
      "执行时间: 22.359026193618774 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第23次训练\n",
      "执行时间: 26.151572942733765 秒,50次未求解，当前强化学习值为1570413.0,利润为1570413.0\n",
      "第24次训练\n",
      "执行时间: 28.079548597335815 秒,93次未求解，当前强化学习值为722323.0,利润为722323.0\n",
      "第25次训练\n",
      "执行时间: 29.298118352890015 秒,111次未求解，当前强化学习值为381340.0,利润为381340.0\n",
      "第26次训练\n",
      "执行时间: 30.404069185256958 秒,116次未求解，当前强化学习值为288291.0,利润为288291.0\n",
      "第27次训练\n",
      "执行时间: 31.293179512023926 秒,122次未求解，当前强化学习值为162447.0,利润为162447.0\n",
      "第28次训练\n",
      "执行时间: 32.74196910858154 秒,107次未求解，当前强化学习值为464472.0,利润为464472.0\n",
      "第29次训练\n",
      "执行时间: 33.82258701324463 秒,120次未求解，当前强化学习值为211387.0,利润为211387.0\n",
      "第30次训练\n",
      "执行时间: 34.79345107078552 秒,120次未求解，当前强化学习值为220407.0,利润为220407.0\n",
      "第31次训练\n",
      "执行时间: 36.52657651901245 秒,100次未求解，当前强化学习值为582739.0,利润为582739.0\n",
      "第32次训练\n",
      "执行时间: 37.10871076583862 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第33次训练\n",
      "执行时间: 37.74041962623596 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第34次训练\n",
      "执行时间: 38.56515383720398 秒,127次未求解，当前强化学习值为67690.0,利润为67690.0\n",
      "第35次训练\n",
      "执行时间: 41.38965034484863 秒,84次未求解，当前强化学习值为917713.0,利润为917713.0\n",
      "第36次训练\n",
      "执行时间: 42.843769550323486 秒,107次未求解，当前强化学习值为475200.0,利润为475200.0\n",
      "第37次训练\n",
      "执行时间: 44.40604853630066 秒,84次未求解，当前强化学习值为907656.0,利润为907656.0\n",
      "第38次训练\n",
      "执行时间: 46.649606704711914 秒,61次未求解，当前强化学习值为1331806.0,利润为1331806.0\n",
      "第39次训练\n",
      "执行时间: 47.49415373802185 秒,111次未求解，当前强化学习值为357941.0,利润为357941.0\n",
      "第40次训练\n",
      "执行时间: 50.59581446647644 秒,32次未求解，当前强化学习值为1885738.0,利润为1885738.0\n",
      "第41次训练\n",
      "执行时间: 51.68519592285156 秒,100次未求解，当前强化学习值为594109.0,利润为594109.0\n",
      "第42次训练\n",
      "执行时间: 52.10348105430603 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第43次训练\n",
      "执行时间: 52.847288370132446 秒,116次未求解，当前强化学习值为288405.0,利润为288405.0\n",
      "第44次训练\n",
      "执行时间: 53.445828676223755 秒,124次未求解，当前强化学习值为131519.0,利润为131519.0\n",
      "第45次训练\n",
      "执行时间: 54.09752058982849 秒,123次未求解，当前强化学习值为147756.0,利润为147756.0\n",
      "第46次训练\n",
      "执行时间: 55.644293785095215 秒,86次未求解，当前强化学习值为891921.0,利润为891921.0\n",
      "第47次训练\n",
      "执行时间: 56.046414613723755 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第48次训练\n",
      "执行时间: 56.699758768081665 秒,124次未求解，当前强化学习值为137233.0,利润为137233.0\n",
      "第49次训练\n",
      "执行时间: 57.55503439903259 秒,113次未求解，当前强化学习值为337155.0,利润为337155.0\n",
      "第50次训练\n",
      "执行时间: 59.94221758842468 秒,61次未求解，当前强化学习值为1335246.0,利润为1335246.0\n",
      "第51次训练\n",
      "执行时间: 60.32195854187012 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第52次训练\n",
      "执行时间: 61.027605056762695 秒,116次未求解，当前强化学习值为288502.0,利润为288502.0\n",
      "第53次训练\n",
      "执行时间: 61.82113528251648 秒,118次未求解，当前强化学习值为260246.0,利润为260246.0\n",
      "第54次训练\n",
      "执行时间: 62.206764936447144 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第55次训练\n",
      "执行时间: 62.57320976257324 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第56次训练\n",
      "执行时间: 63.27038049697876 秒,120次未求解，当前强化学习值为204749.0,利润为204749.0\n",
      "第57次训练\n",
      "执行时间: 65.26348876953125 秒,74次未求解，当前强化学习值为1135368.0,利润为1135368.0\n",
      "第58次训练\n",
      "执行时间: 66.38290572166443 秒,105次未求解，当前强化学习值为510813.0,利润为510813.0\n",
      "第59次训练\n",
      "执行时间: 66.77114605903625 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第60次训练\n",
      "执行时间: 67.15326857566833 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第61次训练\n",
      "执行时间: 67.90891289710999 秒,115次未求解，当前强化学习值为299484.0,利润为299484.0\n",
      "第62次训练\n",
      "执行时间: 68.70112156867981 秒,115次未求解，当前强化学习值为307215.0,利润为307215.0\n",
      "第63次训练\n",
      "执行时间: 69.09306263923645 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第64次训练\n",
      "执行时间: 70.2794337272644 秒,99次未求解，当前强化学习值为597216.0,利润为597216.0\n",
      "第65次训练\n",
      "执行时间: 71.63106060028076 秒,95次未求解，当前强化学习值为683533.0,利润为683533.0\n",
      "第66次训练\n",
      "执行时间: 72.01217365264893 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第67次训练\n",
      "执行时间: 72.38819599151611 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第68次训练\n",
      "执行时间: 73.21895861625671 秒,113次未求解，当前强化学习值为319773.0,利润为319773.0\n",
      "第69次训练\n",
      "执行时间: 73.77508211135864 秒,124次未求解，当前强化学习值为141316.0,利润为141316.0\n",
      "第70次训练\n",
      "执行时间: 74.1755940914154 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第71次训练\n",
      "执行时间: 75.77723240852356 秒,81次未求解，当前强化学习值为965839.0,利润为965839.0\n",
      "第72次训练\n",
      "执行时间: 76.8055112361908 秒,107次未求解，当前强化学习值为471171.0007095492,利润为471171.0007095492\n",
      "第73次训练\n",
      "执行时间: 78.6650013923645 秒,77次未求解，当前强化学习值为1038494.0,利润为1038494.0\n",
      "第74次训练\n",
      "执行时间: 79.18583559989929 秒,126次未求解，当前强化学习值为68973.0,利润为68973.0\n",
      "第75次训练\n",
      "执行时间: 79.70122528076172 秒,128次未求解，当前强化学习值为50664.0,利润为50664.0\n",
      "第76次训练\n",
      "执行时间: 80.40199089050293 秒,122次未求解，当前强化学习值为167144.0,利润为167144.0\n",
      "第77次训练\n",
      "执行时间: 81.20448589324951 秒,118次未求解，当前强化学习值为254803.0,利润为254803.0\n",
      "第78次训练\n",
      "执行时间: 81.7427818775177 秒,127次未求解，当前强化学习值为60895.0,利润为60895.0\n",
      "第79次训练\n",
      "执行时间: 82.68213820457458 秒,115次未求解，当前强化学习值为299865.0,利润为299865.0\n",
      "第80次训练\n",
      "执行时间: 83.30768465995789 秒,124次未求解，当前强化学习值为134688.0,利润为134688.0\n",
      "第81次训练\n",
      "执行时间: 84.014488697052 秒,115次未求解，当前强化学习值为298515.0,利润为298515.0\n",
      "第82次训练\n",
      "执行时间: 84.60570740699768 秒,124次未求解，当前强化学习值为136760.0,利润为136760.0\n",
      "第83次训练\n",
      "执行时间: 85.62912106513977 秒,108次未求解，当前强化学习值为436329.0,利润为436329.0\n",
      "第84次训练\n",
      "执行时间: 86.18744397163391 秒,127次未求解，当前强化学习值为60895.0,利润为60895.0\n",
      "第85次训练\n",
      "执行时间: 87.13325428962708 秒,109次未求解，当前强化学习值为418509.0,利润为418509.0\n",
      "第86次训练\n",
      "执行时间: 88.41487574577332 秒,110次未求解，当前强化学习值为402371.0,利润为402371.0\n",
      "第87次训练\n",
      "执行时间: 90.62568807601929 秒,91次未求解，当前强化学习值为766352.0,利润为766352.0\n",
      "第88次训练\n",
      "执行时间: 91.42163324356079 秒,127次未求解，当前强化学习值为72184.0,利润为72184.0\n",
      "第89次训练\n",
      "执行时间: 92.25588941574097 秒,124次未求解，当前强化学习值为136758.0,利润为136758.0\n",
      "第90次训练\n",
      "执行时间: 93.00472283363342 秒,128次未求解，当前强化学习值为52248.0,利润为52248.0\n",
      "第91次训练\n",
      "执行时间: 94.56831860542297 秒,108次未求解，当前强化学习值为439328.0,利润为439328.0\n",
      "第92次训练\n",
      "执行时间: 95.91832113265991 秒,111次未求解，当前强化学习值为373183.0,利润为373183.0\n",
      "第93次训练\n",
      "执行时间: 96.66555833816528 秒,127次未求解，当前强化学习值为66195.0,利润为66195.0\n",
      "第94次训练\n",
      "执行时间: 97.2381272315979 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第95次训练\n",
      "执行时间: 98.13977122306824 秒,122次未求解，当前强化学习值为169792.0,利润为169792.0\n",
      "第96次训练\n",
      "执行时间: 99.0630738735199 秒,120次未求解，当前强化学习值为219299.0,利润为219299.0\n",
      "第97次训练\n",
      "执行时间: 100.43940830230713 秒,108次未求解，当前强化学习值为443360.0,利润为443360.0\n",
      "第98次训练\n",
      "执行时间: 102.01557946205139 秒,107次未求解，当前强化学习值为487996.0,利润为487996.0\n",
      "第99次训练\n",
      "执行时间: 103.07116985321045 秒,120次未求解，当前强化学习值为208972.0,利润为208972.0\n",
      "第100次训练\n",
      "执行时间: 104.58206629753113 秒,107次未求解，当前强化学习值为461652.0,利润为461652.0\n",
      "第101次训练\n",
      "执行时间: 105.15307712554932 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第102次训练\n",
      "执行时间: 106.25974607467651 秒,118次未求解，当前强化学习值为253573.0,利润为253573.0\n",
      "第103次训练\n",
      "执行时间: 107.20878648757935 秒,122次未求解，当前强化学习值为171529.0,利润为171529.0\n",
      "第104次训练\n",
      "执行时间: 108.2742109298706 秒,116次未求解，当前强化学习值为279246.0,利润为279246.0\n",
      "第105次训练\n",
      "执行时间: 109.15968179702759 秒,122次未求解，当前强化学习值为173595.0,利润为173595.0\n",
      "第106次训练\n",
      "执行时间: 109.96851348876953 秒,126次未求解，当前强化学习值为74135.0,利润为74135.0\n",
      "第107次训练\n",
      "执行时间: 110.79961013793945 秒,127次未求解，当前强化学习值为58722.0,利润为58722.0\n",
      "第108次训练\n",
      "执行时间: 111.70149683952332 秒,124次未求解，当前强化学习值为137432.0,利润为137432.0\n",
      "第109次训练\n",
      "执行时间: 112.51511549949646 秒,127次未求解，当前强化学习值为64346.0,利润为64346.0\n",
      "第110次训练\n",
      "执行时间: 113.28750443458557 秒,127次未求解，当前强化学习值为69916.0,利润为69916.0\n",
      "第111次训练\n",
      "执行时间: 114.36343574523926 秒,119次未求解，当前强化学习值为217114.0,利润为217114.0\n",
      "第112次训练\n",
      "执行时间: 115.90560698509216 秒,105次未求解，当前强化学习值为507151.0,利润为507151.0\n",
      "第113次训练\n",
      "执行时间: 116.78497648239136 秒,123次未求解，当前强化学习值为153101.0,利润为153101.0\n",
      "第114次训练\n",
      "执行时间: 119.57200598716736 秒,77次未求解，当前强化学习值为1058771.0,利润为1058771.0\n",
      "第115次训练\n",
      "执行时间: 120.16067266464233 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第116次训练\n",
      "执行时间: 120.75075507164001 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第117次训练\n",
      "执行时间: 121.33441138267517 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第118次训练\n",
      "执行时间: 121.924391746521 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第119次训练\n",
      "执行时间: 125.10371541976929 秒,69次未求解，当前强化学习值为1202143.0,利润为1202143.0\n",
      "第120次训练\n",
      "执行时间: 126.20084714889526 秒,118次未求解，当前强化学习值为254337.0,利润为254337.0\n",
      "第121次训练\n",
      "执行时间: 126.78955554962158 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第122次训练\n",
      "执行时间: 127.38505697250366 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第123次训练\n",
      "执行时间: 128.35244345664978 秒,124次未求解，当前强化学习值为139230.0,利润为139230.0\n",
      "第124次训练\n",
      "执行时间: 129.59269404411316 秒,113次未求解，当前强化学习值为326100.0,利润为326100.0\n",
      "第125次训练\n",
      "执行时间: 130.1271858215332 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第126次训练\n",
      "执行时间: 130.70691180229187 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第127次训练\n",
      "执行时间: 131.79564952850342 秒,116次未求解，当前强化学习值为290872.0,利润为290872.0\n",
      "第128次训练\n",
      "执行时间: 132.6821644306183 秒,124次未求解，当前强化学习值为117123.0,利润为117123.0\n",
      "第129次训练\n",
      "执行时间: 133.83393049240112 秒,116次未求解，当前强化学习值为285851.0,利润为285851.0\n",
      "第130次训练\n",
      "执行时间: 135.0160322189331 秒,116次未求解，当前强化学习值为280789.0,利润为280789.0\n",
      "第131次训练\n",
      "执行时间: 135.8746635913849 秒,124次未求解，当前强化学习值为136953.0,利润为136953.0\n",
      "第132次训练\n",
      "执行时间: 138.14397287368774 秒,82次未求解，当前强化学习值为942266.0,利润为942266.0\n",
      "第133次训练\n",
      "执行时间: 141.37658047676086 秒,65次未求解，当前强化学习值为1282364.0,利润为1282364.0\n",
      "第134次训练\n",
      "执行时间: 142.24833250045776 秒,122次未求解，当前强化学习值为169369.0,利润为169369.0\n",
      "第135次训练\n",
      "执行时间: 144.0191044807434 秒,100次未求解，当前强化学习值为592409.0,利润为592409.0\n",
      "第136次训练\n",
      "执行时间: 145.02290081977844 秒,120次未求解，当前强化学习值为210966.0,利润为210966.0\n",
      "第137次训练\n",
      "执行时间: 146.38286519050598 秒,111次未求解，当前强化学习值为376454.0,利润为376454.0\n",
      "第138次训练\n",
      "执行时间: 147.2437138557434 秒,124次未求解，当前强化学习值为136924.0,利润为136924.0\n",
      "第139次训练\n",
      "执行时间: 149.38916611671448 秒,98次未求解，当前强化学习值为640640.0,利润为640640.0\n",
      "第140次训练\n",
      "执行时间: 150.47820901870728 秒,116次未求解，当前强化学习值为276963.0,利润为276963.0\n",
      "第141次训练\n",
      "执行时间: 151.5076720714569 秒,117次未求解，当前强化学习值为276327.0,利润为276327.0\n",
      "第142次训练\n",
      "执行时间: 153.08337688446045 秒,103次未求解，当前强化学习值为524064.0,利润为524064.0\n",
      "第143次训练\n",
      "执行时间: 153.71248769760132 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第144次训练\n",
      "执行时间: 154.69621872901917 秒,124次未求解，当前强化学习值为123324.0,利润为123324.0\n",
      "第145次训练\n",
      "执行时间: 156.4721281528473 秒,101次未求解，当前强化学习值为560838.0,利润为560838.0\n",
      "第146次训练\n",
      "执行时间: 157.64261078834534 秒,117次未求解，当前强化学习值为262352.0,利润为262352.0\n",
      "第147次训练\n",
      "执行时间: 158.22846603393555 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第148次训练\n",
      "执行时间: 159.20412063598633 秒,122次未求解，当前强化学习值为165488.0,利润为165488.0\n",
      "第149次训练\n",
      "执行时间: 160.5211205482483 秒,113次未求解，当前强化学习值为313750.0,利润为313750.0\n",
      "第150次训练\n",
      "执行时间: 161.11204171180725 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第151次训练\n",
      "执行时间: 162.15729212760925 秒,119次未求解，当前强化学习值为237478.0,利润为237478.0\n",
      "第152次训练\n",
      "执行时间: 164.4719169139862 秒,88次未求解，当前强化学习值为855090.0,利润为855090.0\n",
      "第153次训练\n",
      "执行时间: 165.39761114120483 秒,124次未求解，当前强化学习值为136750.0,利润为136750.0\n",
      "第154次训练\n",
      "执行时间: 165.97862458229065 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第155次训练\n",
      "执行时间: 167.1859769821167 秒,115次未求解，当前强化学习值为305681.0,利润为305681.0\n",
      "第156次训练\n",
      "执行时间: 168.30723905563354 秒,116次未求解，当前强化学习值为290307.0,利润为290307.0\n",
      "第157次训练\n",
      "执行时间: 169.39155173301697 秒,116次未求解，当前强化学习值为287852.0,利润为287852.0\n",
      "第158次训练\n",
      "执行时间: 170.54340410232544 秒,115次未求解，当前强化学习值为302854.0,利润为302854.0\n",
      "第159次训练\n",
      "执行时间: 171.32196497917175 秒,128次未求解，当前强化学习值为52257.0,利润为52257.0\n",
      "第160次训练\n",
      "执行时间: 171.9096221923828 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第161次训练\n",
      "执行时间: 172.5184450149536 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第162次训练\n",
      "执行时间: 173.10913920402527 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第163次训练\n",
      "执行时间: 176.0835108757019 秒,75次未求解，当前强化学习值为1085015.0,利润为1085015.0\n",
      "第164次训练\n",
      "执行时间: 176.68606567382812 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第165次训练\n",
      "执行时间: 178.16663265228271 秒,106次未求解，当前强化学习值为486107.0,利润为486107.0\n",
      "第166次训练\n",
      "执行时间: 180.17468738555908 秒,98次未求解，当前强化学习值为624756.0,利润为624756.0\n",
      "第167次训练\n",
      "执行时间: 183.0884690284729 秒,76次未求解，当前强化学习值为1075488.0,利润为1075488.0\n",
      "第168次训练\n",
      "执行时间: 183.69649028778076 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第169次训练\n",
      "执行时间: 184.6565318107605 秒,122次未求解，当前强化学习值为167544.0,利润为167544.0\n",
      "第170次训练\n",
      "执行时间: 185.8689169883728 秒,119次未求解，当前强化学习值为245799.0,利润为245799.0\n",
      "第171次训练\n",
      "执行时间: 187.03905987739563 秒,115次未求解，当前强化学习值为297606.0,利润为297606.0\n",
      "第172次训练\n",
      "执行时间: 187.6478726863861 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第173次训练\n",
      "执行时间: 188.6157248020172 秒,124次未求解，当前强化学习值为132578.0,利润为132578.0\n",
      "第174次训练\n",
      "执行时间: 189.72766947746277 秒,117次未求解，当前强化学习值为280930.0,利润为280930.0\n",
      "第175次训练\n",
      "执行时间: 193.01240968704224 秒,67次未求解，当前强化学习值为1240644.0,利润为1240644.0\n",
      "第176次训练\n",
      "执行时间: 195.5177412033081 秒,89次未求解，当前强化学习值为816459.0,利润为816459.0\n",
      "第177次训练\n",
      "执行时间: 196.0926320552826 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第178次训练\n",
      "执行时间: 197.08972477912903 秒,124次未求解，当前强化学习值为125118.0,利润为125118.0\n",
      "第179次训练\n",
      "执行时间: 198.20329904556274 秒,117次未求解，当前强化学习值为277559.0,利润为277559.0\n",
      "第180次训练\n",
      "执行时间: 198.80175399780273 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第181次训练\n",
      "执行时间: 201.07814073562622 秒,89次未求解，当前强化学习值为824852.0,利润为824852.0\n",
      "第182次训练\n",
      "执行时间: 201.9535369873047 秒,124次未求解，当前强化学习值为136950.0,利润为136950.0\n",
      "第183次训练\n",
      "执行时间: 204.78879070281982 秒,78次未求解，当前强化学习值为1019979.0,利润为1019979.0\n",
      "第184次训练\n",
      "执行时间: 206.93170881271362 秒,89次未求解，当前强化学习值为807387.0,利润为807387.0\n",
      "第185次训练\n",
      "执行时间: 207.85067677497864 秒,124次未求解，当前强化学习值为131959.0,利润为131959.0\n",
      "第186次训练\n",
      "执行时间: 210.24776482582092 秒,91次未求解，当前强化学习值为780446.0,利润为780446.0\n",
      "第187次训练\n",
      "执行时间: 211.345712184906 秒,119次未求解，当前强化学习值为242787.0,利润为242787.0\n",
      "第188次训练\n",
      "执行时间: 212.15283298492432 秒,127次未求解，当前强化学习值为66175.0,利润为66175.0\n",
      "第189次训练\n",
      "执行时间: 213.71897554397583 秒,107次未求解，当前强化学习值为482382.0,利润为482382.0\n",
      "第190次训练\n",
      "执行时间: 214.31889820098877 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第191次训练\n",
      "执行时间: 215.92952132225037 秒,108次未求解，当前强化学习值为436435.0,利润为436435.0\n",
      "第192次训练\n",
      "执行时间: 217.1174671649933 秒,116次未求解，当前强化学习值为294963.0,利润为294963.0\n",
      "第193次训练\n",
      "执行时间: 217.98636031150818 秒,124次未求解，当前强化学习值为124872.0,利润为124872.0\n",
      "第194次训练\n",
      "执行时间: 218.58020853996277 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第195次训练\n",
      "执行时间: 222.85597372055054 秒,46次未求解，当前强化学习值为1620500.0,利润为1620500.0\n",
      "第196次训练\n",
      "执行时间: 226.2138237953186 秒,69次未求解，当前强化学习值为1187710.0,利润为1187710.0\n",
      "第197次训练\n",
      "执行时间: 227.29872584342957 秒,119次未求解，当前强化学习值为239828.0,利润为239828.0\n",
      "第198次训练\n",
      "执行时间: 227.88328528404236 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第199次训练\n",
      "执行时间: 228.97123336791992 秒,118次未求解，当前强化学习值为253094.0,利润为253094.0\n",
      "第200次训练\n",
      "执行时间: 229.5738627910614 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第201次训练\n",
      "执行时间: 230.82342314720154 秒,115次未求解，当前强化学习值为301035.0,利润为301035.0\n",
      "第202次训练\n",
      "执行时间: 231.41112756729126 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第203次训练\n",
      "执行时间: 232.17177963256836 秒,127次未求解，当前强化学习值为69906.0,利润为69906.0\n",
      "第204次训练\n",
      "执行时间: 233.01907968521118 秒,124次未求解，当前强化学习值为136489.0,利润为136489.0\n",
      "第205次训练\n",
      "执行时间: 234.56930804252625 秒,116次未求解，当前强化学习值为284222.0,利润为284222.0\n",
      "第206次训练\n",
      "执行时间: 235.27513599395752 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第207次训练\n",
      "执行时间: 235.90015530586243 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第208次训练\n",
      "执行时间: 238.32325410842896 秒,88次未求解，当前强化学习值为837776.0,利润为837776.0\n",
      "第209次训练\n",
      "执行时间: 239.1887686252594 秒,124次未求解，当前强化学习值为136940.0,利润为136940.0\n",
      "第210次训练\n",
      "执行时间: 240.0939495563507 秒,127次未求解，当前强化学习值为60872.0,利润为60872.0\n",
      "第211次训练\n",
      "执行时间: 240.67589807510376 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第212次训练\n",
      "执行时间: 241.70222425460815 秒,120次未求解，当前强化学习值为213698.0,利润为213698.0\n",
      "第213次训练\n",
      "执行时间: 242.5331289768219 秒,126次未求解，当前强化学习值为88657.0,利润为88657.0\n",
      "第214次训练\n",
      "执行时间: 243.1130063533783 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第215次训练\n",
      "执行时间: 245.38732624053955 秒,88次未求解，当前强化学习值为838528.0,利润为838528.0\n",
      "第216次训练\n",
      "执行时间: 247.93610882759094 秒,86次未求解，当前强化学习值为883301.0,利润为883301.0\n",
      "第217次训练\n",
      "执行时间: 248.70353031158447 秒,127次未求解，当前强化学习值为58722.0,利润为58722.0\n",
      "第218次训练\n",
      "执行时间: 250.2584309577942 秒,106次未求解，当前强化学习值为476333.0,利润为476333.0\n",
      "第219次训练\n",
      "执行时间: 251.76357173919678 秒,107次未求解，当前强化学习值为454353.0,利润为454353.0\n",
      "第220次训练\n",
      "执行时间: 252.36292910575867 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第221次训练\n",
      "执行时间: 253.51047945022583 秒,116次未求解，当前强化学习值为281243.0,利润为281243.0\n",
      "第222次训练\n",
      "执行时间: 256.0288140773773 秒,87次未求解，当前强化学习值为861323.0,利润为861323.0\n",
      "第223次训练\n",
      "执行时间: 257.96440505981445 秒,95次未求解，当前强化学习值为699688.0,利润为699688.0\n",
      "第224次训练\n",
      "执行时间: 258.9723250865936 秒,125次未求解，当前强化学习值为105078.0,利润为105078.0\n",
      "第225次训练\n",
      "执行时间: 262.6833076477051 秒,58次未求解，当前强化学习值为1377622.0,利润为1377622.0\n",
      "第226次训练\n",
      "执行时间: 263.51837849617004 秒,124次未求解，当前强化学习值为139204.0,利润为139204.0\n",
      "第227次训练\n",
      "执行时间: 265.1527307033539 秒,103次未求解，当前强化学习值为539555.0,利润为539555.0\n",
      "第228次训练\n",
      "执行时间: 265.7400653362274 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第229次训练\n",
      "执行时间: 266.97127628326416 秒,112次未求解，当前强化学习值为338414.0,利润为338414.0\n",
      "第230次训练\n",
      "执行时间: 268.19846510887146 秒,115次未求解，当前强化学习值为312861.0,利润为312861.0\n",
      "第231次训练\n",
      "执行时间: 271.268435716629 秒,70次未求解，当前强化学习值为1215039.0,利润为1215039.0\n",
      "第232次训练\n",
      "执行时间: 271.8250195980072 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第233次训练\n",
      "执行时间: 272.3901889324188 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第234次训练\n",
      "执行时间: 278.13312101364136 秒,0次未求解，当前强化学习值为2535729.0,利润为2535729.0\n",
      "第235次训练\n",
      "执行时间: 279.0401747226715 秒,124次未求解，当前强化学习值为126620.0,利润为126620.0\n",
      "第236次训练\n",
      "执行时间: 280.2163724899292 秒,117次未求解，当前强化学习值为265531.0,利润为265531.0\n",
      "第237次训练\n",
      "执行时间: 281.6801998615265 秒,113次未求解，当前强化学习值为324746.0,利润为324746.0\n",
      "第238次训练\n",
      "执行时间: 282.786239862442 秒,120次未求解，当前强化学习值为217349.0,利润为217349.0\n",
      "第239次训练\n",
      "执行时间: 283.84307837486267 秒,118次未求解，当前强化学习值为255615.0,利润为255615.0\n",
      "第240次训练\n",
      "执行时间: 284.4258906841278 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第241次训练\n",
      "执行时间: 285.3362956047058 秒,122次未求解，当前强化学习值为161459.0,利润为161459.0\n",
      "第242次训练\n",
      "执行时间: 286.366179227829 秒,124次未求解，当前强化学习值为132159.0,利润为132159.0\n",
      "第243次训练\n",
      "执行时间: 290.52877950668335 秒,53次未求解，当前强化学习值为1502338.0,利润为1502338.0\n",
      "第244次训练\n",
      "执行时间: 292.23558592796326 秒,103次未求解，当前强化学习值为548689.0,利润为548689.0\n",
      "第245次训练\n",
      "执行时间: 292.8309848308563 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第246次训练\n",
      "执行时间: 294.1229636669159 秒,115次未求解，当前强化学习值为293750.0,利润为293750.0\n",
      "第247次训练\n",
      "执行时间: 295.01354122161865 秒,124次未求解，当前强化学习值为139205.0,利润为139205.0\n",
      "第248次训练\n",
      "执行时间: 296.3528256416321 秒,112次未求解，当前强化学习值为362221.0,利润为362221.0\n",
      "第249次训练\n",
      "执行时间: 297.4509770870209 秒,115次未求解，当前强化学习值为307667.0,利润为307667.0\n",
      "第250次训练\n",
      "执行时间: 298.0360515117645 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第251次训练\n",
      "执行时间: 303.9300363063812 秒,0次未求解，当前强化学习值为2537826.0,利润为2537826.0\n",
      "第252次训练\n",
      "执行时间: 304.51025342941284 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第253次训练\n",
      "执行时间: 305.0781672000885 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第254次训练\n",
      "执行时间: 306.1974284648895 秒,118次未求解，当前强化学习值为248899.0,利润为248899.0\n",
      "第255次训练\n",
      "执行时间: 307.94365549087524 秒,103次未求解，当前强化学习值为539981.0,利润为539981.0\n",
      "第256次训练\n",
      "执行时间: 308.53445053100586 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第257次训练\n",
      "执行时间: 309.5813179016113 秒,121次未求解，当前强化学习值为177037.0,利润为177037.0\n",
      "第258次训练\n",
      "执行时间: 310.1851191520691 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第259次训练\n",
      "执行时间: 311.30008840560913 秒,119次未求解，当前强化学习值为238615.0,利润为238615.0\n",
      "第260次训练\n",
      "执行时间: 312.6602671146393 秒,109次未求解，当前强化学习值为418179.0,利润为418179.0\n",
      "第261次训练\n",
      "执行时间: 313.8185474872589 秒,115次未求解，当前强化学习值为303607.0,利润为303607.0\n",
      "第262次训练\n",
      "执行时间: 315.02254605293274 秒,116次未求解，当前强化学习值为279085.0,利润为279085.0\n",
      "第263次训练\n",
      "执行时间: 316.89747190475464 秒,100次未求解，当前强化学习值为590488.0,利润为590488.0\n",
      "第264次训练\n",
      "执行时间: 317.40570878982544 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第265次训练\n",
      "执行时间: 318.40453720092773 秒,122次未求解，当前强化学习值为160967.0,利润为160967.0\n",
      "第266次训练\n",
      "执行时间: 318.94093441963196 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第267次训练\n",
      "执行时间: 321.8790075778961 秒,71次未求解，当前强化学习值为1155377.0,利润为1155377.0\n",
      "第268次训练\n",
      "执行时间: 322.8581199645996 秒,120次未求解，当前强化学习值为207672.0,利润为207672.0\n",
      "第269次训练\n",
      "执行时间: 323.897527217865 秒,120次未求解，当前强化学习值为204776.0,利润为204776.0\n",
      "第270次训练\n",
      "执行时间: 324.8079283237457 秒,123次未求解，当前强化学习值为149560.0,利润为149560.0\n",
      "第271次训练\n",
      "执行时间: 325.38646125793457 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第272次训练\n",
      "执行时间: 325.96817994117737 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第273次训练\n",
      "执行时间: 326.74762415885925 秒,126次未求解，当前强化学习值为84224.0,利润为84224.0\n",
      "第274次训练\n",
      "执行时间: 327.5815854072571 秒,124次未求解，当前强化学习值为132159.0,利润为132159.0\n",
      "第275次训练\n",
      "执行时间: 328.4932942390442 秒,123次未求解，当前强化学习值为149504.0,利润为149504.0\n",
      "第276次训练\n",
      "执行时间: 329.0330674648285 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第277次训练\n",
      "执行时间: 334.03586769104004 秒,22次未求解，当前强化学习值为2094298.0,利润为2094298.0\n",
      "第278次训练\n",
      "执行时间: 335.183944940567 秒,117次未求解，当前强化学习值为270473.0,利润为270473.0\n",
      "第279次训练\n",
      "执行时间: 335.7819502353668 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第280次训练\n",
      "执行时间: 336.65716648101807 秒,127次未求解，当前强化学习值为66195.0,利润为66195.0\n",
      "第281次训练\n",
      "执行时间: 339.68809819221497 秒,73次未求解，当前强化学习值为1113628.0,利润为1113628.0\n",
      "第282次训练\n",
      "执行时间: 340.2406470775604 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第283次训练\n",
      "执行时间: 340.8270742893219 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第284次训练\n",
      "执行时间: 341.4127674102783 秒,135次未求解，当前强化学习值为-96543.0,利润为-96543.0\n",
      "第285次训练\n",
      "执行时间: 342.52801394462585 秒,115次未求解，当前强化学习值为295595.0,利润为295595.0\n",
      "第286次训练\n",
      "执行时间: 343.09676218032837 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第287次训练\n",
      "执行时间: 345.5538957118988 秒,82次未求解，当前强化学习值为940568.0,利润为940568.0\n",
      "第288次训练\n",
      "执行时间: 346.40922951698303 秒,124次未求解，当前强化学习值为136983.0,利润为136983.0\n",
      "第289次训练\n",
      "执行时间: 347.347247838974 秒,123次未求解，当前强化学习值为147464.0,利润为147464.0\n",
      "第290次训练\n",
      "执行时间: 348.3829891681671 秒,118次未求解，当前强化学习值为253504.0,利润为253504.0\n",
      "第291次训练\n",
      "执行时间: 349.34359431266785 秒,122次未求解，当前强化学习值为159209.0,利润为159209.0\n",
      "第292次训练\n",
      "执行时间: 349.9262936115265 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第293次训练\n",
      "执行时间: 350.5103826522827 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第294次训练\n",
      "执行时间: 351.7386083602905 秒,115次未求解，当前强化学习值为306835.0,利润为306835.0\n",
      "第295次训练\n",
      "执行时间: 353.22682762145996 秒,105次未求解，当前强化学习值为496240.0,利润为496240.0\n",
      "第296次训练\n",
      "执行时间: 356.27046155929565 秒,74次未求解，当前强化学习值为1093703.0,利润为1093703.0\n",
      "第297次训练\n",
      "执行时间: 357.01004004478455 秒,128次未求解，当前强化学习值为50669.0,利润为50669.0\n",
      "第298次训练\n",
      "执行时间: 361.06163024902344 秒,46次未求解，当前强化学习值为1654651.0,利润为1654651.0\n",
      "第299次训练\n",
      "执行时间: 362.0278639793396 秒,122次未求解，当前强化学习值为154422.0,利润为154422.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import ppo as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 32         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM_VEHICLE+STATE_DIM_ORDER\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        \n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            if last_vehicle:\n",
    "                \n",
    "                agent.update_third(vehicle_states, order_states, action, selected_log_probs, log_probs, probs,\n",
    "                            grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            if len(group[0]) != 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                mask = env.get_mask(orders_unmatched)\n",
    "                # 注意这里有多种take_action形式\n",
    "                action, selected_log_probs, log_probs, probs = agent.take_action_vehicle(vehicle_states, order_states, mask, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "                ACTIONS.append(action) \n",
    "\n",
    "        \n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else :\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            grid_reward =   (objval-base_revenue[time])/1000\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "           \n",
    "       \n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# buffer版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'aloha_buffer.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为569776.0,101次未求解\n",
      "第1次训练\n",
      "执行时间: 1.8205502033233643 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第2次训练\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got PackedSequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 135\u001b[0m\n\u001b[0;32m    132\u001b[0m         grid_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(grid_reward, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    133\u001b[0m         agent\u001b[38;5;241m.\u001b[39mstore_experience(vehicle_states, order_states, selected_log_probs, \n\u001b[0;32m    134\u001b[0m                log_probs, probs, grid_reward, next_vehicle_states, next_order_states)\n\u001b[1;32m--> 135\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_third_buffer_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     env\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m=\u001b[39m time\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\aloha_buffer.py:185\u001b[0m, in \u001b[0;36mMultiAgentAC.update_third_buffer_rnn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    182\u001b[0m next_v_encoded \u001b[38;5;241m=\u001b[39m encode_states(next_v_states, next_v_lengths)\n\u001b[0;32m    183\u001b[0m next_o_encoded \u001b[38;5;241m=\u001b[39m encode_states(next_o_states, next_o_lengths)\n\u001b[1;32m--> 185\u001b[0m global_current \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo_encoded\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m global_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([next_v_encoded, next_o_encoded], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    188\u001b[0m current_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(v_encoded\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got PackedSequence"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import aloha_buffer as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 64         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM =  num_city, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = num_city *2\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        \n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = seat_count(CAPACITY, city_node)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order_mask(orders_unmatched, G, num_city)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            if last_vehicle:\n",
    "                grid_reward = np.clip(grid_reward, 0, 2)\n",
    "                agent.store_experience(vehicle_states, order_states, selected_log_probs, \n",
    "                       log_probs, probs, grid_reward, next_vehicle_states, next_order_states)\n",
    "                agent.update_third_buffer_rnn()\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = seat_count(CAPACITY, city_node)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order_mask(orders_unmatched, G, num_city)\n",
    "            if len(group[0]) != 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                mask = env.get_mask(orders_unmatched)\n",
    "                action, selected_log_probs, log_probs, probs = agent.take_action_third(vehicle_states, order_states, mask, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "                ACTIONS.append(action) \n",
    "\n",
    "        \n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else :\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            grid_reward =   objval-base_revenue[time]\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "           \n",
    "       \n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 订单输入是掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 64         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM =  num_city, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        \n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order_mask(orders_unmatched, G, num_city)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            if last_vehicle:\n",
    "                grid_reward = np.clip(grid_reward, 0, 2)\n",
    "                agent.store_experience(vehicle_states, order_states, selected_log_probs, \n",
    "                       log_probs, probs, grid_reward, next_vehicle_states, next_order_states)\n",
    "                agent.update_third_buffer_rnn()\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order_mask(orders_unmatched, G, num_city)\n",
    "            if len(group[0]) != 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                mask = env.get_mask(orders_unmatched)\n",
    "                action, selected_log_probs, log_probs, probs = agent.take_action_third(vehicle_states, order_states, mask, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "                ACTIONS.append(action) \n",
    "\n",
    "        \n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else :\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            grid_reward =   objval/1000\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "           \n",
    "       \n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验记录\n",
    "\n",
    "1. 基模型：不收敛\n",
    "2. greedy_epsilon二次收敛，模型永远更新参数\n",
    "3. 改变了参数的传递，使得take_action_third()返回当前掩码下的概率分布，同时将这些概率传入update_third()，因此在对应使用中不再是之前的重新计算。\n",
    "4. 优化模型不求解则强化学习亦不更新参数\n",
    "5. update采用RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAMQFFUEAAAAAgCIAQQAAAAAANwRBAAAAAMiHHUEAAAAAQJD3wAAAAADgsRRBAAAAANCQ98AAAAAAcNr6QAAAAADWIiVBAAAAANCQ98AAAAAAmCwAQQAAAABAkPfAAAAAAECQ98AAAAAAuAASQQAAAADQkPfAAAAAAECQ98AAAAAA0JD3wAAAAADYnRxBAAAAABgFEEEAAAAAsLEAQQAAAABgD/JAAAAAAFBcHEEAAAAA0JbwQAAAAADdVzVBAAAAANCQ98AAAAAA0Nr3QAAAAADGCiVBAAAAAECQ98AAAAAAqJsSQQAAAADQkPfAAAAAAOCUKUEAAAAArPEbQQAAAABAkPfAAAAAANAB9EAAAAAA0JD3wAAAAABAkPfAAAAAADCh8UAAAAAAQJD3wAAAAADMixFBAAAAAGQyFEEAAAAA0JD3wAAAAABQ8gJBAAAAAECQ98AAAAAAzLwgQQAAAACUrylBAAAAAGzXOEEAAAAAFNgVQQAAAAAg1/pAAAAAANBBEEEAAAAANQM0QQAAAADsKhBBAAAAAAztEkEAAAAAiMYTQQAAAADgOvhAAAAAAFB1+kAAAAAA0JD3wAAAAAD4uxJBAAAAALiSNkEAAAAAyNsvQQAAAAAU3BJBAAAAADDpAEEAAAAAQJD3wAAAAAAgwf1AAAAAAEBC70AAAAAA1O4xQQAAAAAczx9BAAAAAECQ98AAAAAA0MX+QAAAAACwjSRBAAAAAKBVGEEAAAAAKMMEQQAAAADQkPfAAAAAABCFEUEAAAAAfAcRQQAAAADQkPfAAAAAABxKF0EAAAAAoO4LQQAAAADWzCpBAAAAAPDfAkEAAAAAILUUQQAAAABAkPfAAAAAABBkCkEAAAAA0JD3wAAAAADcFRNBAAAAAFhpAEEAAAAAUrkvQQAAAADQkPfAAAAAAECQ98AAAAAAJuUjQQAAAABsfTJBAAAAAKiLFUEAAAAAQJD3wAAAAAAUDBJBAAAAAECQ98AAAAAAuAwXQQAAAAAQiAJBAAAAAPDNOkEAAAAAkMb+QAAAAAC0fRlBAAAAAHAQ/kAAAAAAeHMTQQAAAABgif5AAAAAAICa/0AAAAAAHB4jQQAAAAC8NiZBAAAAAEgdD0EAAAAA0JD3wAAAAABAkPfAAAAAAHhUE0EAAAAAMF8cQQAAAAAApxJBAAAAACy9FUEAAAAAQDQAQQAAAADQkPfAAAAAAECQ98AAAAAAENEPQQAAAACoURBBAAAAAJCNHUEAAAAAQJD3wAAAAADQkPfAAAAAANCQ98AAAAAAqNQIQQAAAADgnhVBAAAAABAAC0EAAAAA6JUdQQAAAADIVRBBAAAAAIQqMUEAAAAA1OQvQQAAAADQkPfAAAAAAGgtEkEAAAAAZHMfQQAAAAAmiCFBAAAAAByqIUEAAAAAQJD3wAAAAADQkPfAAAAAAMBn/kAAAAAAsEn/QAAAAACA2vpAAAAAAOAuAEEAAAAAYA/yQAAAAABAkPfAAAAAAETxEUEAAAAA8Mr7QAAAAADsuhxBAAAAAKoLJkEAAAAAQJD3wAAAAAA88jVBAAAAAIAK+UAAAAAAjDUiQQAAAABw9QVBAAAAACCp+UAAAAAAZFQUQQAAAABAkPfAAAAAAAD1GkEAAAAA6PgQQQAAAAA4xBxBAAAAANCQ98AAAAAAELbxQAAAAADMsBpBAAAAABD3GkEAAAAACGgSQQAAAABY+Q1BAAAAADDWGkEAAAAAvAEcQQAAAAAM1zZBAAAAABzMGkEAAAAAQJD3wAAAAAAwwftAAAAAAC7bMEEAAAAA/EYYQQAAAABAkPfAAAAAANCQ98AAAAAAwPUBQQAAAAB4sBNBAAAAAECQ98AAAAAAQIwOQQAAAADwg/lAAAAAAOxnIEEAAAAAJmkpQQAAAAAA0B5BAAAAAKC//UAAAAAAXHsdQQAAAADQkPfAAAAAAL76JkEAAAAAgOb/QAAAAABA1QtBAAAAALg7GEEAAAAAcMADQQAAAAAQv/9AAAAAAHxBGkEAAAAArDoQQQAAAABAkPfAAAAAAFr1LEEAAAAAq7o2QQAAAAAEVBxBAAAAACQ1HEEAAAAAQJD3wAAAAABeaiVBAAAAANxAGEEAAAAASJ4oQQAAAAAeWilBAAAAACjqAkEAAAAAOPMOQQAAAABwmQlBAAAAAAgHEkEAAAAAQuYsQQAAAADA4wpBAAAAAPCJ/0AAAAAA8Ef6QAAAAABO4iJBAAAAACKJKUEAAAAA0JD3wAAAAADQkPfAAAAAAGCH8EAAAAAAEHwAQQAAAAAIjgRBAAAAANCQ98AAAAAAQLr+QAAAAAAE7h1BAAAAAECQ98AAAAAAoGj+QAAAAACYmARBAAAAANCQ98AAAAAA0MX+QAAAAADQIflAAAAAAJPTNkEAAAAA8KgrQQAAAABgW/tAAAAAAGBF/0AAAAAAtOYVQQAAAADQUSBBAAAAAACzIUEAAAAASAgkQQAAAABuBCFBAAAAAKB78UAAAAAAQJD3wAAAAABAkPfAAAAAAM6QJUEAAAAAA+QzQQAAAABAkPfAAAAAANAuIkEAAAAAQJD3wAAAAABAkPfAAAAAAOhrBkEAAAAAQJD3wAAAAABgpiRBAAAAAJAyG0EAAAAA0JD3wAAAAADQkPfAAAAAACIIJkEAAAAA4EkVQQAAAACqTyNBAAAAANiYDEEAAAAAyFceQQAAAABwdxlBAAAAANCQ98AAAAAAAGAPQQAAAADUhRVBAAAAAIQeHUEAAAAA6NoCQQAAAAAU9BFBAAAAALgvAEEAAAAAEE32QAAAAAD42ihBAAAAABsoMEEAAAAAoFIEQQAAAADQkPfAAAAAAEiQHUEAAAAAKMMcQQAAAABAkPfAAAAAAECQ98AAAAAA0JD3wAAAAABYBC1BAAAAANCQ98AAAAAASoEsQQAAAABAkPfAAAAAANCQ98AAAAAA3KIjQQAAAAB4cw5BAAAAAHaQMUEAAAAA0JD3wAAAAAAowiFBAAAAAHDc8kAAAAAAQJD3wAAAAAC8WhxBAAAAALAY8UAAAAAAtdUxQQAAAAAoVxxBAAAAALi+DEEAAAAALI0hQQAAAADQkPfAAAAAAECQ98AAAAAAQJD3wAAAAAC87xRBAAAAAKa0M0EAAAAAKLIMQQAAAAAQvwFBAAAAAKDNMkEAAAAAQJD3wA==",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新智能体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**新智能体的设计逻辑**\n",
    "1. 将订单按照最终可行的城市集分为多类\n",
    "2. 为每一个类设计一个智能体\n",
    "3. 智能体只在有对应订单时才进行强化学习\n",
    "4. 智能体的奖励来自于订单执行后的利润"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\dirty_test\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次训练\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiAgentAC' object has no attribute 'current_order'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 132\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# 判断订单当前是否激活，并且赋值当前激活的订单给current_order\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m AGENT\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 132\u001b[0m     \u001b[43mactive_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morders_unmatched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     get_multi_reward(agent)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m episode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\tool_func.py:315\u001b[0m, in \u001b[0;36mactive_test\u001b[1;34m(action_type, agent, orders_unmatched)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactive_test\u001b[39m(action_type, agent, orders_unmatched):\n\u001b[0;32m    314\u001b[0m     agent\u001b[38;5;241m.\u001b[39mactive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     agent\u001b[38;5;241m.\u001b[39mlast_order \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_order\u001b[49m)\n\u001b[0;32m    316\u001b[0m     agent\u001b[38;5;241m.\u001b[39mcurrent_order \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m order \u001b[38;5;129;01min\u001b[39;00m orders_unmatched\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dirty_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultiAgentAC' object has no attribute 'current_order'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 10     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "action_types = order_same_action(Total_order, num_city, G)\n",
    "AGENT = {}\n",
    "for action_type, same_orders in action_types.items():\n",
    "    agent = magent.MultiAgentAC(\n",
    "        device = DEVICE,\n",
    "        VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "        ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "        NUM_CITIES = len(action_type), \n",
    "        HIDDEN_DIM = HIDDEN_DIM, \n",
    "        STATE_DIM = STATE_DIM\n",
    "    )\n",
    "    agent.action_key = action_type\n",
    "    AGENT[action_type] = agent\n",
    "\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        # 判断订单当前是否激活，并且赋值当前激活的订单给current_order\n",
    "        for agent in AGENT.values():\n",
    "            active_test(agent.action_key, agent, orders_unmatched)\n",
    "            get_multi_reward(agent)\n",
    "\n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            \n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            if last_vehicle:\n",
    "                for agent in AGENT.values():\n",
    "                    if agent.active:\n",
    "                        next_order_states = vectorization_order(agent.current_order)\n",
    "                        if agent.last_order:\n",
    "                            order_states = vectorization_order(agent.last_order)\n",
    "                        else:\n",
    "                            continue\n",
    "                            # 过去time=0确保了order_states存在，现在需要第一次训练后才有\n",
    "                        # 改一下grid_reward\n",
    "                        # print(agent.action_key,time, agent.reward)\n",
    "                        agent.update(agent.v_states, order_states, agent.action, \n",
    "                                    agent.reward + objval, next_vehicle_states, next_order_states , if_end)\n",
    "                    \n",
    "            env.time = time\n",
    "\n",
    "        \n",
    "\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            if len(group[0]) != 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                # if greedy > greedy_epsilon:\n",
    "                greedy = False\n",
    "                vehicle_states = vectorization_vehicle(Vehicles)\n",
    "                # 这里也改了\n",
    "                \n",
    "                for agent in AGENT.values():\n",
    "                    if agent.active:\n",
    "                        agent.v_states = vehicle_states\n",
    "                        order_states = vectorization_order(agent.current_order)\n",
    "                        agent.action= agent.take_action_skyrim(agent.v_states, order_states, explore, greedy)\n",
    "                        # ACTIONS.append(agent.action) \n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time        \n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 赢麻了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'aloha.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为569776.0,101次未求解\n",
      "第1次训练\n",
      "tensor([28.0000, 21.0000, 21.0000,  7.0000,  7.0000, 14.0000, 28.0000, 14.0000,\n",
      "         0.3333,  0.3333,  0.3333,  0.3333,  0.1667,  0.6667,  0.0000,  0.3333])\n",
      "tensor([28.0000, 21.0000, 21.0000,  7.0000,  0.0000, 14.0000, 28.0000,  0.0000,\n",
      "         0.1667,  0.3333,  0.1667,  0.5000,  0.0000,  0.1667,  0.1667,  0.3333])\n",
      "tensor([28.0000,  7.0000, 21.0000,  7.0000,  0.0000, 14.0000, 21.0000,  0.0000,\n",
      "         0.0000,  0.1667,  0.5000,  0.1667,  0.0000,  0.3333,  0.1667,  0.1667])\n",
      "tensor([21.0000, 14.0000, 14.0000,  7.0000,  0.0000, 14.0000, 14.0000,  0.0000,\n",
      "         0.1667,  0.3333,  0.5000,  0.1667,  0.1667,  0.5000,  0.1667,  0.3333])\n",
      "tensor([21.0000,  7.0000, 14.0000,  7.0000,  0.0000,  7.0000,  7.0000,  0.0000,\n",
      "         0.3333,  0.1667,  0.6667,  0.1667,  0.1667,  0.1667,  0.0000,  0.3333])\n",
      "tensor([21.0000,  7.0000,  0.0000, 14.0000,  7.0000,  7.0000,  7.0000,  0.0000,\n",
      "         0.3333,  0.5000,  0.3333,  0.1667,  0.0000,  0.5000,  0.1667,  0.5000])\n",
      "tensor([28.0000,  7.0000,  0.0000,  0.0000,  7.0000,  7.0000,  7.0000,  0.0000,\n",
      "         0.1667,  0.0000,  0.5000,  0.3333,  0.1667,  0.5000,  0.0000,  0.6667])\n",
      "tensor([21.0000,  7.0000,  7.0000,  0.0000,  7.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.3333,  0.0000,  0.6667,  0.1667,  0.5000,  0.5000,  0.0000,  0.0000])\n",
      "tensor([7.0000, 7.0000, 0.0000, 0.0000, 7.0000, 7.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.5000, 0.1667, 0.1667, 0.3333, 0.8333, 0.1667, 0.3333])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.6667, 0.3333, 0.6667, 0.0000, 0.0000, 0.0000, 0.8333])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.5000, 0.4167, 0.4167, 0.0000, 0.1667, 0.0000, 0.6667])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.3889, 0.2778, 0.3889, 0.1111, 0.3333, 0.0556, 0.5556])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.4583, 0.3333, 0.3333, 0.0833, 0.4167, 0.0833, 0.5000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3000,\n",
      "        0.4000, 0.3333, 0.3333, 0.1000, 0.4333, 0.0667, 0.4333])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3056,\n",
      "        0.3889, 0.3889, 0.3333, 0.1389, 0.4444, 0.0556, 0.4167])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2619,\n",
      "        0.3571, 0.3810, 0.3571, 0.1190, 0.4286, 0.0714, 0.3810])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2292,\n",
      "        0.3333, 0.3750, 0.3750, 0.1042, 0.4375, 0.0625, 0.3750])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2222,\n",
      "        0.3519, 0.3519, 0.3889, 0.0926, 0.4259, 0.0741, 0.3889])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2167,\n",
      "        0.3333, 0.3500, 0.4000, 0.1000, 0.4333, 0.0667, 0.3667])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2727,\n",
      "        0.3030, 0.3333, 0.3636, 0.1212, 0.4545, 0.0606, 0.3485])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2778,\n",
      "        0.3056, 0.3333, 0.3611, 0.1111, 0.4444, 0.0694, 0.3333])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2987,\n",
      "        0.2987, 0.3506, 0.3636, 0.1169, 0.4416, 0.0649, 0.3377])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2805,\n",
      "        0.3049, 0.3293, 0.3659, 0.1098, 0.4390, 0.0732, 0.3415])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2738,\n",
      "        0.2857, 0.3333, 0.3571, 0.1190, 0.4524, 0.0833, 0.2976])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2697,\n",
      "        0.3034, 0.3146, 0.3596, 0.1124, 0.4494, 0.0899, 0.3034])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2609,\n",
      "        0.3152, 0.3043, 0.3696, 0.0870, 0.4348, 0.0978, 0.3261])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2474,\n",
      "        0.3299, 0.3093, 0.3918, 0.0825, 0.4330, 0.1031, 0.3299])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2475,\n",
      "        0.3366, 0.2871, 0.3861, 0.0792, 0.4455, 0.0990, 0.3465])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2524,\n",
      "        0.3301, 0.2913, 0.3689, 0.0680, 0.4660, 0.1165, 0.3398])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2692,\n",
      "        0.3365, 0.3269, 0.3846, 0.0577, 0.4615, 0.1154, 0.3365])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2600,\n",
      "        0.3400, 0.3300, 0.4000, 0.0700, 0.4700, 0.1300, 0.3300])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500,\n",
      "        0.3462, 0.3269, 0.4327, 0.0673, 0.4712, 0.1250, 0.3173])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2647,\n",
      "        0.3235, 0.3529, 0.4118, 0.0490, 0.4510, 0.1176, 0.3137])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2692,\n",
      "        0.2981, 0.3462, 0.3942, 0.0481, 0.4327, 0.1058, 0.3269])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2718,\n",
      "        0.3107, 0.3398, 0.3786, 0.0485, 0.4466, 0.1165, 0.3301])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2673,\n",
      "        0.3465, 0.3663, 0.3663, 0.0396, 0.4455, 0.1287, 0.3564])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2700,\n",
      "        0.3600, 0.3700, 0.3700, 0.0400, 0.4500, 0.1500, 0.3300])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2600,\n",
      "        0.3700, 0.3600, 0.4100, 0.0500, 0.4500, 0.1600, 0.3200])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2816,\n",
      "        0.3786, 0.3592, 0.4078, 0.0485, 0.4563, 0.1553, 0.3204])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2772,\n",
      "        0.3861, 0.3762, 0.3960, 0.0396, 0.4653, 0.1584, 0.3267])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2885,\n",
      "        0.3846, 0.3846, 0.4231, 0.0385, 0.4519, 0.1538, 0.3077])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2981,\n",
      "        0.3750, 0.4135, 0.3942, 0.0481, 0.4615, 0.1635, 0.2981])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3010,\n",
      "        0.3689, 0.4272, 0.3981, 0.0583, 0.4757, 0.1650, 0.2913])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2990,\n",
      "        0.3814, 0.4227, 0.3918, 0.0619, 0.4845, 0.1546, 0.3093])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2947,\n",
      "        0.3684, 0.4316, 0.4000, 0.0737, 0.4526, 0.1263, 0.3263])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2812,\n",
      "        0.3646, 0.4375, 0.3750, 0.0833, 0.4583, 0.1250, 0.2917])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2842,\n",
      "        0.3895, 0.4316, 0.3684, 0.0947, 0.4632, 0.1263, 0.2842])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2959,\n",
      "        0.3878, 0.4286, 0.3673, 0.1020, 0.4388, 0.1224, 0.2959])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2990,\n",
      "        0.3918, 0.4021, 0.3918, 0.0928, 0.4536, 0.1237, 0.2784])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3093,\n",
      "        0.3711, 0.4227, 0.3814, 0.1031, 0.4536, 0.1237, 0.2784])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3163,\n",
      "        0.3571, 0.4184, 0.3571, 0.1224, 0.4694, 0.1122, 0.2857])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3069,\n",
      "        0.3366, 0.4059, 0.3465, 0.1089, 0.4653, 0.0990, 0.2871])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941,\n",
      "        0.3431, 0.4118, 0.3333, 0.1078, 0.4706, 0.0980, 0.3039])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2816,\n",
      "        0.3301, 0.3883, 0.3495, 0.1068, 0.4466, 0.0971, 0.3010])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2772,\n",
      "        0.3069, 0.3564, 0.3465, 0.1287, 0.4554, 0.1089, 0.2772])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2718,\n",
      "        0.3010, 0.3689, 0.3204, 0.1262, 0.4757, 0.1262, 0.2621])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500,\n",
      "        0.2885, 0.3462, 0.3173, 0.1346, 0.4808, 0.1346, 0.2500])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2788,\n",
      "        0.2981, 0.3269, 0.3173, 0.1442, 0.4808, 0.1346, 0.2500])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3000,\n",
      "        0.2700, 0.3600, 0.3300, 0.1500, 0.4800, 0.1100, 0.2400])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2917,\n",
      "        0.2396, 0.3542, 0.3125, 0.1562, 0.4479, 0.1146, 0.2500])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2755,\n",
      "        0.2245, 0.3571, 0.3163, 0.1633, 0.4388, 0.1122, 0.2551])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2917,\n",
      "        0.2292, 0.3854, 0.3229, 0.1667, 0.4062, 0.1250, 0.2708])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2800,\n",
      "        0.2500, 0.4000, 0.3200, 0.1600, 0.4200, 0.1300, 0.2700])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3069,\n",
      "        0.2475, 0.4059, 0.3069, 0.1683, 0.4158, 0.1287, 0.2772])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3235,\n",
      "        0.2549, 0.4216, 0.3137, 0.1863, 0.3922, 0.1176, 0.2745])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3300,\n",
      "        0.2500, 0.4200, 0.3000, 0.2000, 0.4200, 0.1100, 0.2700])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3400,\n",
      "        0.2800, 0.4100, 0.3300, 0.1900, 0.3800, 0.1100, 0.2900])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3400,\n",
      "        0.2800, 0.4000, 0.3300, 0.1900, 0.4100, 0.1000, 0.3200])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3235,\n",
      "        0.2843, 0.3824, 0.3137, 0.1961, 0.4216, 0.1176, 0.3235])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3469,\n",
      "        0.2959, 0.4082, 0.3367, 0.1837, 0.4082, 0.1429, 0.3061])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3404,\n",
      "        0.3085, 0.3723, 0.3617, 0.1702, 0.3936, 0.1489, 0.3191])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3404,\n",
      "        0.2979, 0.3617, 0.3404, 0.1702, 0.4255, 0.1489, 0.3191])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3298,\n",
      "        0.2979, 0.3723, 0.3298, 0.1702, 0.4574, 0.1596, 0.3404])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3297,\n",
      "        0.3297, 0.3736, 0.3736, 0.1538, 0.4505, 0.1538, 0.3516])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3226,\n",
      "        0.3548, 0.3548, 0.3763, 0.1398, 0.4516, 0.1505, 0.3548])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3298,\n",
      "        0.3617, 0.3617, 0.3723, 0.1489, 0.4681, 0.1489, 0.3511])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3438,\n",
      "        0.3438, 0.3958, 0.3854, 0.1667, 0.4583, 0.1250, 0.3542])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3370,\n",
      "        0.3478, 0.4130, 0.3913, 0.1630, 0.4674, 0.1196, 0.3587])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3085,\n",
      "        0.3298, 0.3936, 0.3830, 0.1489, 0.4894, 0.1170, 0.3511])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3265,\n",
      "        0.3367, 0.3878, 0.3878, 0.1531, 0.5102, 0.1224, 0.3469])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3100,\n",
      "        0.3300, 0.3800, 0.4000, 0.1700, 0.5400, 0.1100, 0.3400])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3300,\n",
      "        0.3500, 0.3900, 0.3700, 0.1600, 0.5300, 0.1100, 0.3600])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3400,\n",
      "        0.3700, 0.3600, 0.3800, 0.1700, 0.5600, 0.1200, 0.3600])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3398,\n",
      "        0.3786, 0.3495, 0.3786, 0.1650, 0.5534, 0.1262, 0.3689])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3168,\n",
      "        0.3861, 0.3267, 0.3663, 0.1584, 0.5941, 0.1386, 0.3663])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3000,\n",
      "        0.3700, 0.3200, 0.4000, 0.1500, 0.5900, 0.1600, 0.3500])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3061,\n",
      "        0.3673, 0.3367, 0.3878, 0.1633, 0.5714, 0.1224, 0.3571])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3505,\n",
      "        0.3814, 0.3299, 0.3918, 0.1443, 0.5464, 0.1031, 0.3608])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3600,\n",
      "        0.3900, 0.3500, 0.3900, 0.1600, 0.5500, 0.1000, 0.3500])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3505,\n",
      "        0.3814, 0.3505, 0.3608, 0.1649, 0.5979, 0.1031, 0.3608])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3500,\n",
      "        0.3900, 0.3800, 0.3500, 0.1700, 0.6000, 0.1000, 0.3400])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3711,\n",
      "        0.4021, 0.3711, 0.3814, 0.2062, 0.5567, 0.0928, 0.3299])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3711,\n",
      "        0.3814, 0.3505, 0.4021, 0.1959, 0.5773, 0.1134, 0.3196])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3814,\n",
      "        0.3711, 0.3505, 0.3918, 0.2062, 0.5979, 0.1134, 0.3093])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3600,\n",
      "        0.3800, 0.3500, 0.4100, 0.2000, 0.5700, 0.1000, 0.3000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3495,\n",
      "        0.3786, 0.3786, 0.3981, 0.1845, 0.5728, 0.0971, 0.3107])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3737,\n",
      "        0.3636, 0.4040, 0.3737, 0.1717, 0.6061, 0.0909, 0.2929])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3600,\n",
      "        0.3700, 0.4000, 0.3700, 0.1600, 0.6000, 0.1000, 0.2800])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3673,\n",
      "        0.3878, 0.4184, 0.3878, 0.1531, 0.5714, 0.1122, 0.2959])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3861,\n",
      "        0.4158, 0.4257, 0.4158, 0.1386, 0.5545, 0.0990, 0.2970])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3535,\n",
      "        0.4343, 0.4141, 0.3939, 0.1414, 0.5758, 0.1010, 0.2929])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3469,\n",
      "        0.3980, 0.4082, 0.3776, 0.1531, 0.6020, 0.1020, 0.2653])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.4062, 0.4062, 0.3958, 0.1458, 0.5833, 0.1354, 0.2500])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3370,\n",
      "        0.4130, 0.3804, 0.3696, 0.1413, 0.5761, 0.1304, 0.2609])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3191,\n",
      "        0.4149, 0.3830, 0.3617, 0.1383, 0.5745, 0.1383, 0.2660])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3226,\n",
      "        0.4409, 0.4194, 0.3763, 0.1505, 0.5591, 0.1290, 0.2473])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2990,\n",
      "        0.4330, 0.4330, 0.3608, 0.1443, 0.5567, 0.1237, 0.2680])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2828,\n",
      "        0.4545, 0.4444, 0.3535, 0.1414, 0.5354, 0.1212, 0.2727])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2871,\n",
      "        0.4653, 0.4554, 0.3465, 0.1188, 0.5446, 0.1188, 0.2772])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2525,\n",
      "        0.4545, 0.4141, 0.3434, 0.1010, 0.5253, 0.1111, 0.3232])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2647,\n",
      "        0.4412, 0.4510, 0.3235, 0.1078, 0.5392, 0.1078, 0.3039])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2718,\n",
      "        0.4563, 0.4369, 0.3301, 0.0971, 0.5340, 0.1068, 0.3107])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2549,\n",
      "        0.4510, 0.4314, 0.3039, 0.0980, 0.5392, 0.1176, 0.3333])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2692,\n",
      "        0.4423, 0.4327, 0.2788, 0.1154, 0.5385, 0.1250, 0.3173])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500,\n",
      "        0.4712, 0.4038, 0.2788, 0.0962, 0.5481, 0.1442, 0.3077])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2476,\n",
      "        0.4476, 0.3810, 0.2857, 0.0952, 0.5333, 0.1429, 0.3238])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2079,\n",
      "        0.4455, 0.3861, 0.3069, 0.0792, 0.5347, 0.1386, 0.3366])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2059,\n",
      "        0.4608, 0.3824, 0.3235, 0.0686, 0.5294, 0.1373, 0.3627])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1919,\n",
      "        0.4545, 0.3737, 0.3232, 0.0606, 0.5051, 0.1414, 0.3636])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1771,\n",
      "        0.4375, 0.3646, 0.3229, 0.0729, 0.5417, 0.1458, 0.3229])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000,\n",
      "        0.4000, 0.3579, 0.3158, 0.0842, 0.5263, 0.1368, 0.3053])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2371,\n",
      "        0.4021, 0.3711, 0.2990, 0.0825, 0.5464, 0.1443, 0.3093])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2525,\n",
      "        0.4040, 0.3737, 0.3030, 0.0909, 0.5455, 0.1515, 0.2929])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2525,\n",
      "        0.4040, 0.3737, 0.3131, 0.0909, 0.5556, 0.1515, 0.2929])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2788,\n",
      "        0.4135, 0.3846, 0.3462, 0.0865, 0.5288, 0.1538, 0.2788])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2642,\n",
      "        0.3962, 0.4151, 0.3585, 0.0849, 0.5283, 0.1321, 0.2642])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2752,\n",
      "        0.4037, 0.4312, 0.3578, 0.0826, 0.5138, 0.1284, 0.2752])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2804,\n",
      "        0.3925, 0.4299, 0.3271, 0.0748, 0.5140, 0.1308, 0.2897])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2647,\n",
      "        0.3824, 0.4020, 0.3431, 0.0784, 0.5000, 0.1275, 0.2941])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2626,\n",
      "        0.4040, 0.4040, 0.3737, 0.0606, 0.5051, 0.1313, 0.2626])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2755,\n",
      "        0.3980, 0.3980, 0.3776, 0.0714, 0.5306, 0.1224, 0.2347])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2887,\n",
      "        0.4021, 0.4330, 0.3814, 0.0515, 0.5052, 0.1237, 0.2371])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3053,\n",
      "        0.3895, 0.4632, 0.3895, 0.0526, 0.4526, 0.1053, 0.2421])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3163,\n",
      "        0.3776, 0.4490, 0.3980, 0.0612, 0.4592, 0.1224, 0.2347])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3163,\n",
      "        0.3980, 0.4388, 0.4286, 0.0612, 0.4694, 0.1224, 0.2449])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.3922, 0.4412, 0.4314, 0.0686, 0.4608, 0.1078, 0.2451])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3267,\n",
      "        0.3861, 0.4455, 0.4257, 0.0792, 0.4554, 0.1188, 0.2277])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3168,\n",
      "        0.3663, 0.4257, 0.4356, 0.0792, 0.4455, 0.1287, 0.2376])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3163,\n",
      "        0.3571, 0.3776, 0.4490, 0.0816, 0.4388, 0.1122, 0.2551])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3366,\n",
      "        0.3564, 0.3663, 0.4653, 0.0891, 0.4158, 0.1089, 0.2673])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3200,\n",
      "        0.3800, 0.3500, 0.4900, 0.0800, 0.4300, 0.1200, 0.2700])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.3939, 0.3333, 0.4848, 0.0707, 0.4242, 0.1111, 0.2828])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3579,\n",
      "        0.4000, 0.3368, 0.4632, 0.0842, 0.4316, 0.1158, 0.2737])\n",
      "执行时间: 2.609196424484253 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第2次训练\n",
      "tensor([28.0000, 21.0000, 21.0000,  7.0000,  7.0000, 14.0000, 28.0000, 14.0000,\n",
      "         0.3333,  0.3333,  0.3333,  0.3333,  0.1667,  0.6667,  0.0000,  0.3333])\n",
      "tensor([28.0000, 21.0000, 21.0000,  7.0000,  0.0000, 14.0000, 28.0000,  0.0000,\n",
      "         0.1667,  0.3333,  0.1667,  0.5000,  0.0000,  0.1667,  0.1667,  0.3333])\n",
      "tensor([28.0000,  7.0000, 21.0000,  7.0000,  0.0000, 14.0000, 21.0000,  0.0000,\n",
      "         0.0000,  0.1667,  0.5000,  0.1667,  0.0000,  0.3333,  0.1667,  0.1667])\n",
      "tensor([21.0000,  7.0000, 14.0000,  7.0000,  0.0000, 14.0000, 14.0000,  0.0000,\n",
      "         0.1667,  0.3333,  0.5000,  0.1667,  0.1667,  0.5000,  0.1667,  0.3333])\n",
      "tensor([21.0000,  7.0000, 14.0000,  7.0000,  0.0000,  7.0000,  7.0000,  0.0000,\n",
      "         0.3333,  0.1667,  0.6667,  0.1667,  0.1667,  0.1667,  0.0000,  0.3333])\n",
      "tensor([21.0000,  7.0000,  7.0000, 14.0000,  7.0000,  0.0000,  7.0000,  0.0000,\n",
      "         0.3333,  0.5000,  0.3333,  0.1667,  0.0000,  0.5000,  0.1667,  0.5000])\n",
      "tensor([28.0000,  7.0000,  7.0000,  7.0000,  7.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.1667,  0.0000,  0.5000,  0.3333,  0.1667,  0.5000,  0.0000,  0.6667])\n",
      "tensor([28.0000,  0.0000,  7.0000,  7.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.3333,  0.0000,  0.6667,  0.1667,  0.5000,  0.5000,  0.0000,  0.0000])\n",
      "tensor([21.0000,  0.0000,  0.0000,  0.0000,  0.0000, 14.0000,  0.0000,  0.0000,\n",
      "         0.3333,  0.5000,  0.1667,  0.1667,  0.3333,  0.8333,  0.1667,  0.3333])\n",
      "tensor([7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.6667, 0.3333, 0.6667, 0.0000, 0.0000, 0.0000, 0.8333])\n",
      "tensor([14.0000,  0.0000,  0.0000, 14.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.2222,  0.4444,  0.4444,  0.3333,  0.0000,  0.2222,  0.0000,  0.6667])\n",
      "tensor([ 0.0000,  0.0000, 14.0000,  0.0000, 21.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.2857,  0.2857,  0.1429,  0.4286,  0.2857,  0.5714,  0.1429,  0.4286])\n",
      "tensor([ 0.0000,  7.0000,  0.0000, 14.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.3333,  0.6667,  0.5000,  0.1667,  0.0000,  0.6667,  0.1667,  0.3333])\n",
      "tensor([0.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1667,\n",
      "        0.1667, 0.3333, 0.3333, 0.1667, 0.5000, 0.0000, 0.1667])\n",
      "tensor([ 7.0000,  0.0000, 14.0000,  0.0000,  0.0000,  0.0000,  7.0000,  0.0000,\n",
      "         0.2222,  0.3333,  0.5556,  0.3333,  0.3333,  0.5556,  0.0000,  0.3333])\n",
      "tensor([ 0.0000, 14.0000, 21.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.1250,  0.2500,  0.3750,  0.5000,  0.1250,  0.3750,  0.1250,  0.2500])\n",
      "tensor([ 7.0000,  7.0000, 21.0000,  0.0000,  0.0000,  7.0000,  7.0000,  0.0000,\n",
      "         0.0000,  0.1667,  0.3333,  0.5000,  0.0000,  0.5000,  0.0000,  0.3333])\n",
      "tensor([7.0000, 7.0000, 7.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.1667,\n",
      "        0.5000, 0.1667, 0.5000, 0.0000, 0.3333, 0.1667, 0.5000])\n",
      "tensor([0.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1429,\n",
      "        0.1429, 0.2857, 0.4286, 0.1429, 0.4286, 0.0000, 0.2857])\n",
      "tensor([0.0000, 0.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5455,\n",
      "        0.0909, 0.2727, 0.1818, 0.2727, 0.6364, 0.0000, 0.1818])\n",
      "tensor([0.0000, 7.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3571,\n",
      "        0.2143, 0.2857, 0.2857, 0.2143, 0.5000, 0.0714, 0.2143])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000, 21.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.4000,  0.3333,  0.4667,  0.4000,  0.2000,  0.4667,  0.0667,  0.3333])\n",
      "tensor([ 7.0000, 14.0000,  0.0000, 14.0000, 14.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.2941,  0.2941,  0.2941,  0.4118,  0.1765,  0.4706,  0.1765,  0.3529])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 14.0000,  0.0000,\n",
      "         0.3636,  0.1818,  0.3636,  0.3636,  0.1818,  0.3636,  0.1818,  0.1818])\n",
      "tensor([7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2308,\n",
      "        0.3077, 0.3846, 0.3077, 0.0769, 0.3846, 0.1538, 0.3077])\n",
      "tensor([7.0000, 0.0000, 0.0000, 7.0000, 0.0000, 7.0000, 7.0000, 0.0000, 0.2500,\n",
      "        0.3125, 0.3750, 0.2500, 0.0625, 0.3750, 0.1875, 0.4375])\n",
      "tensor([0.0000, 0.0000, 0.0000, 7.0000, 7.0000, 0.0000, 7.0000, 0.0000, 0.2857,\n",
      "        0.2857, 0.4286, 0.5000, 0.0714, 0.3571, 0.1429, 0.3571])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 7.0000, 0.0000, 0.2667,\n",
      "        0.2667, 0.2000, 0.2667, 0.0667, 0.4000, 0.1333, 0.4667])\n",
      "tensor([0.0000, 0.0000, 7.0000, 0.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.2778, 0.2778, 0.2222, 0.0556, 0.5556, 0.2222, 0.3889])\n",
      "tensor([0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 7.0000, 0.0000, 0.4500,\n",
      "        0.3000, 0.4500, 0.3000, 0.0500, 0.5000, 0.1500, 0.3500])\n",
      "tensor([ 7.0000,  0.0000, 14.0000,  0.0000,  0.0000,  0.0000,  7.0000,  0.0000,\n",
      "         0.4762,  0.3810,  0.4762,  0.3333,  0.0952,  0.5238,  0.1905,  0.3333])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.4500,\n",
      "        0.4000, 0.4000, 0.5000, 0.0500, 0.4000, 0.1000, 0.4000])\n",
      "tensor([ 0.0000,  0.0000,  7.0000,  0.0000, 14.0000,  0.0000,  7.0000,  0.0000,\n",
      "         0.5000,  0.2727,  0.3636,  0.4091,  0.0455,  0.3636,  0.0909,  0.3636])\n",
      "tensor([14.0000,  7.0000,  7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.4545,  0.2273,  0.3636,  0.3182,  0.0909,  0.4091,  0.0909,  0.4091])\n",
      "tensor([ 0.0000, 14.0000,  0.0000,  0.0000,  7.0000,  0.0000,  7.0000,  0.0000,\n",
      "         0.4545,  0.3182,  0.3636,  0.3636,  0.0455,  0.4545,  0.1364,  0.4091])\n",
      "tensor([0.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 7.0000, 0.0000, 0.4762,\n",
      "        0.3810, 0.4286, 0.3333, 0.0000, 0.3810, 0.0952, 0.4762])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.4286,\n",
      "        0.3333, 0.3333, 0.3333, 0.0000, 0.4762, 0.1905, 0.3333])\n",
      "tensor([0.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3750,\n",
      "        0.3333, 0.2500, 0.5000, 0.0417, 0.5000, 0.2083, 0.3333])\n",
      "tensor([ 0.0000,  0.0000, 14.0000,  0.0000,  7.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.4444,  0.4074,  0.3704,  0.4074,  0.0370,  0.4815,  0.1481,  0.3333])\n",
      "tensor([0.0000, 7.0000, 0.0000, 7.0000, 7.0000, 0.0000, 7.0000, 0.0000, 0.4615,\n",
      "        0.4231, 0.4231, 0.3846, 0.0769, 0.5385, 0.1538, 0.3462])\n",
      "tensor([7.0000, 0.0000, 7.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.4444,\n",
      "        0.4444, 0.4074, 0.4444, 0.0741, 0.4815, 0.1852, 0.3333])\n",
      "tensor([0.0000, 0.0000, 0.0000, 7.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4444,\n",
      "        0.3333, 0.5185, 0.2963, 0.1111, 0.5556, 0.1481, 0.3333])\n",
      "tensor([14.0000,  7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  7.0000,  0.0000,\n",
      "         0.4000,  0.3333,  0.4333,  0.3333,  0.1333,  0.6000,  0.1667,  0.3333])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000, 14.0000,  7.0000,  0.0000,  0.0000,\n",
      "         0.3200,  0.4800,  0.4000,  0.4000,  0.0800,  0.5200,  0.1200,  0.4000])\n",
      "tensor([ 0.0000, 14.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.2692,  0.5385,  0.3846,  0.4231,  0.1154,  0.5000,  0.1154,  0.4615])\n",
      "tensor([0.0000, 7.0000, 0.0000, 0.0000, 7.0000, 0.0000, 7.0000, 0.0000, 0.2963,\n",
      "        0.4444, 0.4444, 0.4074, 0.1111, 0.4815, 0.1481, 0.2963])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 14.0000,  0.0000,\n",
      "         0.3214,  0.5714,  0.3571,  0.4643,  0.1071,  0.5000,  0.1786,  0.3571])\n",
      "tensor([ 0.0000,  0.0000,  7.0000,  7.0000, 14.0000,  0.0000,  7.0000,  0.0000,\n",
      "         0.3333,  0.5000,  0.4000,  0.4000,  0.1333,  0.4000,  0.1333,  0.4333])\n",
      "tensor([7.0000, 0.0000, 0.0000, 0.0000, 7.0000, 0.0000, 7.0000, 0.0000, 0.2963,\n",
      "        0.4815, 0.3333, 0.4074, 0.0741, 0.5185, 0.1111, 0.4074])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3200,\n",
      "        0.4000, 0.3600, 0.3200, 0.1200, 0.6000, 0.1200, 0.4000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2903,\n",
      "        0.3226, 0.3548, 0.2903, 0.1613, 0.5806, 0.0968, 0.3871])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2778,\n",
      "        0.2778, 0.3056, 0.2500, 0.1389, 0.5833, 0.0833, 0.3611])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2439,\n",
      "        0.2927, 0.3171, 0.2439, 0.1220, 0.5854, 0.0732, 0.3659])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2340,\n",
      "        0.2766, 0.2979, 0.3191, 0.1064, 0.5106, 0.0638, 0.3404])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500,\n",
      "        0.2692, 0.2885, 0.3077, 0.1346, 0.5000, 0.0769, 0.3077])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2632,\n",
      "        0.2632, 0.3158, 0.2632, 0.1228, 0.5263, 0.1053, 0.2807])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2419,\n",
      "        0.2581, 0.3226, 0.2581, 0.1290, 0.5161, 0.1129, 0.2742])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2647,\n",
      "        0.2647, 0.3382, 0.2647, 0.1618, 0.5294, 0.1176, 0.2647])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3000,\n",
      "        0.2286, 0.3857, 0.2857, 0.1571, 0.5000, 0.0857, 0.2571])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3000,\n",
      "        0.2143, 0.3714, 0.2857, 0.1571, 0.4429, 0.0857, 0.2857])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2667,\n",
      "        0.2133, 0.3600, 0.2933, 0.1600, 0.4400, 0.1067, 0.2800])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2949,\n",
      "        0.2179, 0.3974, 0.2821, 0.1667, 0.4359, 0.1154, 0.2949])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2892,\n",
      "        0.2410, 0.4096, 0.2771, 0.1566, 0.4578, 0.1205, 0.2892])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3103,\n",
      "        0.2414, 0.4138, 0.2759, 0.1609, 0.4483, 0.1149, 0.2874])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3222,\n",
      "        0.2556, 0.4222, 0.3000, 0.1778, 0.4222, 0.1111, 0.2889])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.2581, 0.4086, 0.3011, 0.1935, 0.4301, 0.1075, 0.2903])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3474,\n",
      "        0.2842, 0.4000, 0.3263, 0.1895, 0.4000, 0.1053, 0.3053])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3469,\n",
      "        0.2857, 0.3980, 0.3265, 0.1939, 0.4184, 0.1020, 0.3265])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3267,\n",
      "        0.2871, 0.3861, 0.3069, 0.1980, 0.4257, 0.1188, 0.3267])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3469,\n",
      "        0.2959, 0.4082, 0.3367, 0.1837, 0.4082, 0.1429, 0.3061])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3404,\n",
      "        0.3085, 0.3723, 0.3617, 0.1702, 0.3936, 0.1489, 0.3191])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3404,\n",
      "        0.2979, 0.3617, 0.3404, 0.1702, 0.4255, 0.1489, 0.3191])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3298,\n",
      "        0.2979, 0.3723, 0.3298, 0.1702, 0.4574, 0.1596, 0.3404])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3297,\n",
      "        0.3297, 0.3736, 0.3736, 0.1538, 0.4505, 0.1538, 0.3516])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3226,\n",
      "        0.3548, 0.3548, 0.3763, 0.1398, 0.4516, 0.1505, 0.3548])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3298,\n",
      "        0.3617, 0.3617, 0.3723, 0.1489, 0.4681, 0.1489, 0.3511])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3438,\n",
      "        0.3438, 0.3958, 0.3854, 0.1667, 0.4583, 0.1250, 0.3542])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3370,\n",
      "        0.3478, 0.4130, 0.3913, 0.1630, 0.4674, 0.1196, 0.3587])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3085,\n",
      "        0.3298, 0.3936, 0.3830, 0.1489, 0.4894, 0.1170, 0.3511])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3265,\n",
      "        0.3367, 0.3878, 0.3878, 0.1531, 0.5102, 0.1224, 0.3469])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3100,\n",
      "        0.3300, 0.3800, 0.4000, 0.1700, 0.5400, 0.1100, 0.3400])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3300,\n",
      "        0.3500, 0.3900, 0.3700, 0.1600, 0.5300, 0.1100, 0.3600])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3400,\n",
      "        0.3700, 0.3600, 0.3800, 0.1700, 0.5600, 0.1200, 0.3600])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3398,\n",
      "        0.3786, 0.3495, 0.3786, 0.1650, 0.5534, 0.1262, 0.3689])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3168,\n",
      "        0.3861, 0.3267, 0.3663, 0.1584, 0.5941, 0.1386, 0.3663])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3000,\n",
      "        0.3700, 0.3200, 0.4000, 0.1500, 0.5900, 0.1600, 0.3500])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3061,\n",
      "        0.3673, 0.3367, 0.3878, 0.1633, 0.5714, 0.1224, 0.3571])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3505,\n",
      "        0.3814, 0.3299, 0.3918, 0.1443, 0.5464, 0.1031, 0.3608])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3600,\n",
      "        0.3900, 0.3500, 0.3900, 0.1600, 0.5500, 0.1000, 0.3500])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3505,\n",
      "        0.3814, 0.3505, 0.3608, 0.1649, 0.5979, 0.1031, 0.3608])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3500,\n",
      "        0.3900, 0.3800, 0.3500, 0.1700, 0.6000, 0.1000, 0.3400])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3711,\n",
      "        0.4021, 0.3711, 0.3814, 0.2062, 0.5567, 0.0928, 0.3299])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3711,\n",
      "        0.3814, 0.3505, 0.4021, 0.1959, 0.5773, 0.1134, 0.3196])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3814,\n",
      "        0.3711, 0.3505, 0.3918, 0.2062, 0.5979, 0.1134, 0.3093])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3600,\n",
      "        0.3800, 0.3500, 0.4100, 0.2000, 0.5700, 0.1000, 0.3000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3495,\n",
      "        0.3786, 0.3786, 0.3981, 0.1845, 0.5728, 0.0971, 0.3107])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3737,\n",
      "        0.3636, 0.4040, 0.3737, 0.1717, 0.6061, 0.0909, 0.2929])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3600,\n",
      "        0.3700, 0.4000, 0.3700, 0.1600, 0.6000, 0.1000, 0.2800])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3673,\n",
      "        0.3878, 0.4184, 0.3878, 0.1531, 0.5714, 0.1122, 0.2959])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3861,\n",
      "        0.4158, 0.4257, 0.4158, 0.1386, 0.5545, 0.0990, 0.2970])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3535,\n",
      "        0.4343, 0.4141, 0.3939, 0.1414, 0.5758, 0.1010, 0.2929])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3469,\n",
      "        0.3980, 0.4082, 0.3776, 0.1531, 0.6020, 0.1020, 0.2653])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.4062, 0.4062, 0.3958, 0.1458, 0.5833, 0.1354, 0.2500])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3370,\n",
      "        0.4130, 0.3804, 0.3696, 0.1413, 0.5761, 0.1304, 0.2609])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3191,\n",
      "        0.4149, 0.3830, 0.3617, 0.1383, 0.5745, 0.1383, 0.2660])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3226,\n",
      "        0.4409, 0.4194, 0.3763, 0.1505, 0.5591, 0.1290, 0.2473])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2990,\n",
      "        0.4330, 0.4330, 0.3608, 0.1443, 0.5567, 0.1237, 0.2680])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2828,\n",
      "        0.4545, 0.4444, 0.3535, 0.1414, 0.5354, 0.1212, 0.2727])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2871,\n",
      "        0.4653, 0.4554, 0.3465, 0.1188, 0.5446, 0.1188, 0.2772])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2525,\n",
      "        0.4545, 0.4141, 0.3434, 0.1010, 0.5253, 0.1111, 0.3232])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2647,\n",
      "        0.4412, 0.4510, 0.3235, 0.1078, 0.5392, 0.1078, 0.3039])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2718,\n",
      "        0.4563, 0.4369, 0.3301, 0.0971, 0.5340, 0.1068, 0.3107])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2549,\n",
      "        0.4510, 0.4314, 0.3039, 0.0980, 0.5392, 0.1176, 0.3333])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2692,\n",
      "        0.4423, 0.4327, 0.2788, 0.1154, 0.5385, 0.1250, 0.3173])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500,\n",
      "        0.4712, 0.4038, 0.2788, 0.0962, 0.5481, 0.1442, 0.3077])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2476,\n",
      "        0.4476, 0.3810, 0.2857, 0.0952, 0.5333, 0.1429, 0.3238])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2079,\n",
      "        0.4455, 0.3861, 0.3069, 0.0792, 0.5347, 0.1386, 0.3366])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2059,\n",
      "        0.4608, 0.3824, 0.3235, 0.0686, 0.5294, 0.1373, 0.3627])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1919,\n",
      "        0.4545, 0.3737, 0.3232, 0.0606, 0.5051, 0.1414, 0.3636])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1771,\n",
      "        0.4375, 0.3646, 0.3229, 0.0729, 0.5417, 0.1458, 0.3229])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000,\n",
      "        0.4000, 0.3579, 0.3158, 0.0842, 0.5263, 0.1368, 0.3053])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2371,\n",
      "        0.4021, 0.3711, 0.2990, 0.0825, 0.5464, 0.1443, 0.3093])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2525,\n",
      "        0.4040, 0.3737, 0.3030, 0.0909, 0.5455, 0.1515, 0.2929])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2525,\n",
      "        0.4040, 0.3737, 0.3131, 0.0909, 0.5556, 0.1515, 0.2929])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2788,\n",
      "        0.4135, 0.3846, 0.3462, 0.0865, 0.5288, 0.1538, 0.2788])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2642,\n",
      "        0.3962, 0.4151, 0.3585, 0.0849, 0.5283, 0.1321, 0.2642])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2752,\n",
      "        0.4037, 0.4312, 0.3578, 0.0826, 0.5138, 0.1284, 0.2752])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2804,\n",
      "        0.3925, 0.4299, 0.3271, 0.0748, 0.5140, 0.1308, 0.2897])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2647,\n",
      "        0.3824, 0.4020, 0.3431, 0.0784, 0.5000, 0.1275, 0.2941])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2626,\n",
      "        0.4040, 0.4040, 0.3737, 0.0606, 0.5051, 0.1313, 0.2626])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2755,\n",
      "        0.3980, 0.3980, 0.3776, 0.0714, 0.5306, 0.1224, 0.2347])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2887,\n",
      "        0.4021, 0.4330, 0.3814, 0.0515, 0.5052, 0.1237, 0.2371])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3053,\n",
      "        0.3895, 0.4632, 0.3895, 0.0526, 0.4526, 0.1053, 0.2421])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3163,\n",
      "        0.3776, 0.4490, 0.3980, 0.0612, 0.4592, 0.1224, 0.2347])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3163,\n",
      "        0.3980, 0.4388, 0.4286, 0.0612, 0.4694, 0.1224, 0.2449])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "        0.3922, 0.4412, 0.4314, 0.0686, 0.4608, 0.1078, 0.2451])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3267,\n",
      "        0.3861, 0.4455, 0.4257, 0.0792, 0.4554, 0.1188, 0.2277])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3168,\n",
      "        0.3663, 0.4257, 0.4356, 0.0792, 0.4455, 0.1287, 0.2376])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3163,\n",
      "        0.3571, 0.3776, 0.4490, 0.0816, 0.4388, 0.1122, 0.2551])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import aloha as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = num_city * 2\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        \n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = seat_count(CAPACITY,city_node)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order_mask(orders_unmatched, G, num_city)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update_third(vehicle_states, order_states, action, selected_log_probs, log_probs, probs,\n",
    "                        grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = seat_count(CAPACITY,city_node)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order_mask(orders_unmatched, G, num_city)\n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            # 注意这里有多种take_action形式\n",
    "            action, selected_log_probs, log_probs, probs = agent.take_action_vehicle(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "        \n",
    "            ACTIONS.append(action) \n",
    "\n",
    "        \n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else :\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # grid_reward =   (objval-base_revenue[time])/1000\n",
    "            grid_reward =  objval /1000\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "           \n",
    "       \n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aloha buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'aloha_buffer.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为569776.0,101次未求解\n",
      "第1次训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mycodelife\\workshop\\DRL_CO\\aloha_buffer.py:138: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  o_states = torch.tensor([exp.order_states for exp in batch], dtype=torch.float).to(self.device)\n",
      "d:\\mycodelife\\workshop\\DRL_CO\\aloha_buffer.py:165: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(current_v, td_target.detach())\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 131\u001b[0m\n\u001b[0;32m    129\u001b[0m next_order_states \u001b[38;5;241m=\u001b[39m vectorization_order_mask(orders_unmatched, G, num_city)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# 这里防止梯度爆炸缩小了reward\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_experience(vehicle_states, order_states, selected_log_probs, action,\n\u001b[0;32m    133\u001b[0m            log_probs, probs, grid_reward, next_vehicle_states, next_order_states, if_end)\n\u001b[0;32m    134\u001b[0m env\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m=\u001b[39m time \n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\aloha_buffer.py:168\u001b[0m, in \u001b[0;36mMultiAgentAC.update\u001b[1;34m(self, time)\u001b[0m\n\u001b[0;32m    165\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(current_v, td_target\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# 计算 Actor 损失\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    169\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([exp\u001b[38;5;241m.\u001b[39mlog_probs \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m batch], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    170\u001b[0m selected_log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([exp\u001b[38;5;241m.\u001b[39mselected_log_probs \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m batch], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import aloha_buffer as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 32         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = num_city*2 \n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        \n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = seat_count(CAPACITY,city_node)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order_mask(orders_unmatched, G, num_city)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(env.time)\n",
    "            agent.store_experience(vehicle_states, order_states, selected_log_probs, action,\n",
    "                       log_probs, probs, grid_reward, next_vehicle_states, next_order_states, if_end)\n",
    "            env.time = time \n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = seat_count(CAPACITY,city_node)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order_mask(orders_unmatched, G, num_city)\n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            # 注意这里有多种take_action形式\n",
    "            action, selected_log_probs, log_probs, probs = agent.take_action_vehicle(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "        \n",
    "            ACTIONS.append(action) \n",
    "\n",
    "        \n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else :\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # grid_reward =   (objval-base_revenue[time])/1000\n",
    "            grid_reward =  objval /1000\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "           \n",
    "       \n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dirty_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
