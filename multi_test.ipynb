{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带掩码的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为119282.0,58次未求解\n",
      "第1次训练\n",
      "执行时间: 1.8399901390075684 秒,50次未求解，当前强化学习值为262721.0,利润为262721.0\n",
      "第2次训练\n",
      "执行时间: 3.9947502613067627 秒,14次未求解，当前强化学习值为876163.0,利润为876163.0\n",
      "第3次训练\n",
      "执行时间: 5.620569705963135 秒,29次未求解，当前强化学习值为630302.0,利润为630302.0\n",
      "第4次训练\n",
      "执行时间: 8.040527582168579 秒,0次未求解，当前强化学习值为1137983.0,利润为1137983.0\n",
      "第5次训练\n",
      "执行时间: 9.552980184555054 秒,36次未求解，当前强化学习值为493615.0,利润为493615.0\n",
      "第6次训练\n",
      "执行时间: 11.84005880355835 秒,0次未求解，当前强化学习值为1119662.0,利润为1119662.0\n",
      "第7次训练\n",
      "执行时间: 13.321484804153442 秒,35次未求解，当前强化学习值为516251.0,利润为516251.0\n",
      "第8次训练\n",
      "执行时间: 15.030028820037842 秒,23次未求解，当前强化学习值为737416.0,利润为737416.0\n",
      "第9次训练\n",
      "执行时间: 17.870023250579834 秒,49次未求解，当前强化学习值为292569.0,利润为292569.0\n",
      "第10次训练\n",
      "执行时间: 20.277883291244507 秒,0次未求解，当前强化学习值为1122293.0,利润为1122293.0\n",
      "第11次训练\n",
      "执行时间: 21.34235954284668 秒,62次未求解，当前强化学习值为47739.0,利润为47739.0\n",
      "第12次训练\n",
      "执行时间: 23.63964319229126 秒,0次未求解，当前强化学习值为1129111.0,利润为1129111.0\n",
      "第13次训练\n",
      "执行时间: 24.700026035308838 秒,61次未求解，当前强化学习值为64000.0,利润为64000.0\n",
      "第14次训练\n",
      "执行时间: 25.829976558685303 秒,55次未求解，当前强化学习值为166136.0,利润为166136.0\n",
      "第15次训练\n",
      "执行时间: 26.759884119033813 秒,62次未求解，当前强化学习值为49557.0,利润为49557.0\n",
      "第16次训练\n",
      "执行时间: 27.697844743728638 秒,62次未求解，当前强化学习值为49557.0,利润为49557.0\n",
      "第17次训练\n",
      "执行时间: 29.32480239868164 秒,30次未求解，当前强化学习值为619153.0,利润为619153.0\n",
      "第18次训练\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action = agent.take_action_mask(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试版 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为333835.0,48次未求解\n",
      "第1次训练\n",
      "执行时间: 1.7765674591064453 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第2次训练\n",
      "执行时间: 2.8372347354888916 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第3次训练\n",
      "执行时间: 3.9000909328460693 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第4次训练\n",
      "执行时间: 4.9130539894104 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第5次训练\n",
      "执行时间: 5.973233938217163 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第6次训练\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.2500],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'actions' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m     greedy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    189\u001b[0m mask \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_mask(orders_unmatched)\n\u001b[1;32m--> 190\u001b[0m action , logits \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# reward = env.test_step(orders_unmatched,action)\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03mCOUNT = 1000\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03mmax_reward = -999999\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m    reward = env.test_step(orders_unmatched, max_action)\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:174\u001b[0m, in \u001b[0;36mMultiAgentAC.take_action_mask\u001b[1;34m(self, vehicle_states, order_states, mask, explore, greedy)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    172\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask 全 0，导致 softmax 计算无意义！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactions\u001b[49m , logits\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'actions' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 30\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0\n",
    "            explore = False\n",
    "            \n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            \"\"\"\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "            \"\"\"\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action , logits = agent.take_action_mask(vehicle_states, order_states, mask, explore, greedy)\n",
    "            # reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            # grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward +=  objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        # save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        # torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试版2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "未加强化学习利润为200830.0,53次未求解\n",
      "第1次训练\n",
      "v_encoded 出现 NaN，输入状态可能异常！\n",
      "o_encoded 出现 NaN，输入状态可能异常！\n",
      "actor_input 出现 NaN，输入状态可能异常！\n",
      "global vehicle 出现 NaN，输入状态可能异常！\n",
      "repeated global 出现 NaN，输入状态可能异常！\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward0>)\n",
      "logits 出现 NaN，输入状态可能异常！\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'actions' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 187\u001b[0m\n\u001b[0;32m    185\u001b[0m     greedy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    186\u001b[0m mask \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_mask(orders_unmatched)\n\u001b[1;32m--> 187\u001b[0m action, logits \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action_mask_special\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mtest_step(orders_unmatched,action)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mCOUNT = 1000\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03mmax_reward = -999999\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    reward = env.test_step(orders_unmatched, max_action)\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:192\u001b[0m, in \u001b[0;36mMultiAgentAC.take_action_mask_special\u001b[1;34m(self, vehicle_states, order_states, mask, explore, greedy)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask 全 0，导致 softmax 计算无意义！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactions\u001b[49m , logits\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'actions' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update_logtis(vehicle_states, order_states, logits,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action, logits = agent.take_action_mask_special(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例1 burn in后探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAIAHyMAAAAAAhFYoQQAAAADYZA9BAAAAAACcwsAAAAAAwT4yQQAAAAAgHPVAAAAAAIAHyMAAAAAAUkMtQQAAAAAAnMLAAAAAAACcwsAAAAAAAnAvQQAAAADcnitBAAAAAIAHyMAAAAAAQNfzQAAAAAAEriRBAAAAAFxuL0EAAAAAeiwsQQAAAAAgXvRAAAAAAACcwsAAAAAAj3UyQQAAAAAAnMLAAAAAAACcwsAAAAAAAJzCwAAAAABkUiRBAAAAAKiGKkEAAAAAgAfIwAAAAACAB8jAAAAAAAdzMkEAAAAAAJzCwAAAAAB8LBFBAAAAAJydGUEAAAAAgAfIwAAAAACAB8jAAAAAAIAHyMAAAAAARVIyQQAAAAAAnMLAAAAAAGBf9EAAAAAAoMceQQAAAACAB8jAAAAAAACcwsAAAAAAgAfIwAAAAAAAnMLAAAAAAHQ7HEEAAAAAI0wyQQAAAAAAnMLAAAAAAIAHyMAAAAAAgAfIwAAAAACAB8jAAAAAAP76LEEAAAAAOvooQQAAAACAB8jAAAAAAAQjMUEAAAAAgAfIwAAAAACAB8jAAAAAALCg80AAAAAAOCsoQQAAAACAB8jAAAAAAACcwsAAAAAAAJzCwAAAAAAY7ydBAAAAAACcwsAAAAAAgigyQQAAAAAAnMLAAAAAAACcwsAAAAAAAJzCwAAAAACkex9BAAAAAOy3EkEAAAAAAJzCwAAAAAAAnMLAAAAAAIAHyMAAAAAAAJzCwAAAAACAB8jAAAAAAIAHyMAAAAAAAJzCwAAAAAA0DBpBAAAAAIylLUEAAAAAgAfIwAAAAAAQPhpBAAAAABKAIUEAAAAA9NElQQAAAACAB8jAAAAAAIAHyMAAAAAAgAfIwAAAAADQ5SdBAAAAACBe9EAAAAAAAJzCwAAAAACAB8jAAAAAAIAHyMAAAAAAdEwcQQAAAADc4x9BAAAAALCTEkEAAAAAapAhQQAAAADQS/VAAAAAAIxfMkEAAAAAyDQyQQAAAACAB8jAAAAAACA/MkEAAAAAAJT0QAAAAADm+y1BAAAAALxCMkEAAAAAGDUSQQAAAAA1RDJBAAAAAIAHyMAAAAAAKMAuQQAAAADQABNBAAAAAMRFH0EAAAAAtsggQQAAAACCCSxBAAAAAIAHyMAAAAAAgAfIwAAAAACAB8jAAAAAAIAHyMAAAAAA/GcyQQAAAACAB8jAAAAAAMhLDUEAAAAAxGoUQQAAAABSTTFBAAAAALKoMUEAAAAAXKMkQQAAAAB2GjJBAAAAAMAB/UAAAAAAgAfIwAAAAADBHzJBAAAAAN6TLkEAAAAAfLMpQQAAAACYigxBAAAAACTpE0EAAAAA8DcPQQAAAACAB8jAAAAAAOw3HkEAAAAAvGElQQAAAAD4YTJBAAAAAN7DLkEAAAAADPwRQQAAAACAB8jAAAAAAECN/EAAAAAAwKokQQAAAAC2IClBAAAAAIAHyMAAAAAA0GEfQQAAAADokBtBAAAAAF9cMkEAAAAA2MkSQQAAAAA9tjBBAAAAAFpLIkEAAAAAgAfIwAAAAACAB8jAAAAAABxlHEEAAAAAAEMyQQAAAABIAAxBAAAAAMyzMUEAAAAAgAfIwAAAAABefypBAAAAAIAHyMAAAAAAsKkjQQAAAADIQS9BAAAAAIAHyMAAAAAAyEsHQQAAAACAB8jAAAAAAIqRJEEAAAAAgAfIwAAAAABAjfxAAAAAAOJUKEEAAAAAxO4SQQAAAADwDDJBAAAAABNXMkEAAAAABPkhQQAAAACDQTJBAAAAAPaDIUEAAAAABgoyQQAAAACwsRJBAAAAACBAMkEAAAAAKK0RQQAAAAA4CBpBAAAAAB87MkEAAAAAFD8jQQAAAADgYhBBAAAAAJAOG0EAAAAAgAfIwAAAAACsdBJBAAAAAMJhMkEAAAAA+CQIQQAAAACAB8jAAAAAAIAHyMAAAAAASEgrQQAAAABs+i1BAAAAAB82MkEAAAAAgAfIwAAAAADu5SFBAAAAAOwvJ0EAAAAAkoshQQAAAABPwDFBAAAAAMjMJUEAAAAAeJcTQQAAAACAB8jAAAAAAIAHyMAAAAAAJMoxQQAAAAAQqhlBAAAAANYNMkEAAAAAILf9QAAAAACAB8jAAAAAABptKEEAAAAAoAoUQQAAAAAXEzJBAAAAAIAHyMAAAAAA0OMtQQAAAACk1ClBAAAAAIpcIEEAAAAAgAfIwAAAAACI1xpBAAAAAPTsIUEAAAAAYF8lQQAAAACAB8jAAAAAAGgrDkEAAAAAyU8yQQAAAACIoA9BAAAAAIAHyMAAAAAA9PAcQQAAAACAB8jAAAAAAHglL0EAAAAAgAfIwAAAAAAgJyBBAAAAADq/LkEAAAAAgAfIwAAAAAA40SFBAAAAAFgxKkEAAAAAxEYyQQAAAAAwbjJBAAAAAIAHyMAAAAAAOoEqQQAAAAAAPxxBAAAAAKidB0EAAAAAC2sxQQAAAADoJwpBAAAAAMBFLkEAAAAAgAfIwAAAAACAB8jAAAAAAIAHyMAAAAAA/JkWQQAAAAB8nBlBAAAAAIgmMUEAAAAA8IsVQQAAAADqLyBBAAAAAIAHyMAAAAAAOpIqQQAAAACobhxBAAAAAIAHyMAAAAAAO1syQQAAAABAuBJBAAAAAC5PK0EAAAAAdAUgQQAAAAAYoBhBAAAAAIAHyMAAAAAAwwMyQQAAAACAB8jAAAAAACiwDEEAAAAAiKIbQQAAAAAcYClBAAAAAHk+MkEAAAAAhKkRQQAAAADIaRFBAAAAAKAnIkEAAAAAUI38QAAAAACAB8jAAAAAANaoL0EAAAAAaGEyQQAAAACsBBlBAAAAAE7XIEEAAAAAgAfIwAAAAABP8DFBAAAAANNMMkEAAAAAimYuQQAAAADAZzJBAAAAAH0WMEEAAAAAFMkhQQAAAAB2TCFBAAAAAKxtHUEAAAAAgAfIwAAAAACnHDBBAAAAAO5cJUEAAAAAw2syQQAAAACAB8jAAAAAABBMDUEAAAAAkEAMQQAAAAAUEDFBAAAAAIAHyMAAAAAAgAfIwAAAAACm/yhBAAAAAHjHGkEAAAAAsF8SQQAAAADmgSRBAAAAAIAHyMAAAAAAgAfIwAAAAACwMhRBAAAAAFRWMkEAAAAAgAfIwAAAAABcbRJBAAAAANIOIEEAAAAAcF0gQQ==",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例2 burn in后不探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例3 更新条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带跳过的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为351240.0,48次未求解\n",
      "第1次训练\n",
      "执行时间: 1.5951976776123047 秒,66次未求解，当前强化学习值为-316088.0,利润为-1388.0\n",
      "第2次训练\n",
      "执行时间: 2.8220858573913574 秒,40次未求解，当前强化学习值为255049.0,利润为471649.0\n",
      "第3次训练\n",
      "执行时间: 3.92783522605896 秒,48次未求解，当前强化学习值为76507.0,利润为333107.0\n",
      "第4次训练\n",
      "执行时间: 4.801097393035889 秒,60次未求解，当前强化学习值为-187197.0,利润为108603.0\n",
      "第5次训练\n",
      "执行时间: 6.016491651535034 秒,45次未求解，当前强化学习值为168199.0,利润为405999.0\n",
      "第6次训练\n",
      "执行时间: 6.894177436828613 秒,60次未求解，当前强化学习值为-183913.0,利润为112087.0\n",
      "第7次训练\n",
      "执行时间: 7.8495588302612305 秒,60次未求解，当前强化学习值为-186198.0,利润为108602.0\n",
      "第8次训练\n",
      "执行时间: 9.370670318603516 秒,43次未求解，当前强化学习值为195294.0,利润为423494.0\n",
      "第9次训练\n",
      "执行时间: 10.391007423400879 秒,59次未求解，当前强化学习值为-181762.0,利润为112638.0\n",
      "第10次训练\n",
      "执行时间: 11.553189754486084 秒,57次未求解，当前强化学习值为-102270.0,利润为178130.0\n",
      "第11次训练\n",
      "执行时间: 13.124795198440552 秒,47次未求解，当前强化学习值为114564.0,利润为361264.0\n",
      "第12次训练\n",
      "执行时间: 14.306801080703735 秒,60次未求解，当前强化学习值为-180413.0,利润为112087.0\n",
      "第13次训练\n",
      "执行时间: 15.57046389579773 秒,48次未求解，当前强化学习值为68873.0,利润为326673.0\n",
      "第14次训练\n",
      "执行时间: 16.64451813697815 秒,57次未求解，当前强化学习值为-110057.0,利润为170643.0\n",
      "第15次训练\n",
      "执行时间: 17.640617847442627 秒,59次未求解，当前强化学习值为-159380.0,利润为131420.0\n",
      "第16次训练\n",
      "执行时间: 18.647897958755493 秒,59次未求解，当前强化学习值为-168057.0,利润为127243.0\n",
      "第17次训练\n",
      "执行时间: 20.608492374420166 秒,28次未求解，当前强化学习值为534251.0,利润为696851.0\n",
      "第18次训练\n",
      "执行时间: 21.51712131500244 秒,60次未求解，当前强化学习值为-190784.0,利润为105716.0\n",
      "第19次训练\n",
      "执行时间: 22.778335571289062 秒,48次未求解，当前强化学习值为80027.0,利润为333427.0\n",
      "第20次训练\n",
      "执行时间: 23.744807720184326 秒,60次未求解，当前强化学习值为-184613.0,利润为112087.0\n",
      "第21次训练\n",
      "执行时间: 24.99462080001831 秒,51次未求解，当前强化学习值为23549.0,利润为288049.0\n",
      "第22次训练\n",
      "执行时间: 25.945756196975708 秒,60次未求解，当前强化学习值为-187566.0,利润为107234.0\n",
      "第23次训练\n",
      "执行时间: 27.99422001838684 秒,23次未求解，当前强化学习值为649226.0,利润为800126.0\n",
      "第24次训练\n",
      "执行时间: 29.752153396606445 秒,32次未求解，当前强化学习值为436038.0,利润为620738.0\n",
      "第25次训练\n",
      "执行时间: 31.028327465057373 秒,47次未求解，当前强化学习值为129301.0,利润为373001.0\n",
      "第26次训练\n",
      "执行时间: 32.439204454422 秒,45次未求解，当前强化学习值为158061.0,利润为394861.0\n",
      "第27次训练\n",
      "执行时间: 33.439168214797974 秒,57次未求解，当前强化学习值为-118950.0,利润为164950.0\n",
      "第28次训练\n",
      "执行时间: 34.552332639694214 秒,54次未求解，当前强化学习值为-59392.0,利润为217108.0\n",
      "第29次训练\n",
      "执行时间: 35.700438261032104 秒,54次未求解，当前强化学习值为-63310.0,利润为215190.0\n",
      "第30次训练\n",
      "执行时间: 36.633397817611694 秒,60次未求解，当前强化学习值为-187284.0,利润为105716.0\n",
      "第31次训练\n",
      "执行时间: 38.67721676826477 秒,22次未求解，当前强化学习值为680635.0,利润为824735.0\n",
      "第32次训练\n",
      "执行时间: 39.634028673172 秒,60次未求解，当前强化学习值为-187925.0,利润为108375.0\n",
      "第33次训练\n",
      "执行时间: 41.42440843582153 秒,35次未求解，当前强化学习值为369561.0,利润为567161.0\n",
      "第34次训练\n",
      "执行时间: 42.37771916389465 秒,60次未求解，当前强化学习值为-193384.0,利润为105716.0\n",
      "第35次训练\n",
      "执行时间: 43.3869833946228 秒,58次未求解，当前强化学习值为-129827.0,利润为155773.0\n",
      "第36次训练\n",
      "执行时间: 45.71844553947449 秒,11次未求解，当前强化学习值为902113.0,利润为1014313.0\n",
      "第37次训练\n",
      "执行时间: 46.916133403778076 秒,50次未求解，当前强化学习值为43715.0,利润为306115.0\n",
      "第38次训练\n",
      "执行时间: 48.783586502075195 秒,28次未求解，当前强化学习值为548164.0,利润为711764.0\n",
      "第39次训练\n",
      "执行时间: 49.75050187110901 秒,60次未求解，当前强化学习值为-191724.0,利润为108376.0\n",
      "第40次训练\n",
      "执行时间: 51.13849878311157 秒,43次未求解，当前强化学习值为195591.0,利润为421091.0\n",
      "第41次训练\n",
      "执行时间: 52.136146783828735 秒,57次未求解，当前强化学习值为-116542.0,利润为164658.0\n",
      "第42次训练\n",
      "执行时间: 53.34069466590881 秒,51次未求解，当前强化学习值为22358.0,利润为286358.0\n",
      "第43次训练\n",
      "执行时间: 54.45970129966736 秒,53次未求解，当前强化学习值为-46247.0,利润为229853.0\n",
      "第44次训练\n",
      "执行时间: 55.403913497924805 秒,60次未求解，当前强化学习值为-184125.0,利润为108375.0\n",
      "第45次训练\n",
      "执行时间: 56.32914471626282 秒,60次未求解，当前强化学习值为-194084.0,利润为105716.0\n",
      "第46次训练\n",
      "执行时间: 57.52802777290344 秒,50次未求解，当前强化学习值为22843.0,利润为285343.0\n",
      "第47次训练\n",
      "执行时间: 59.19414758682251 秒,34次未求解，当前强化学习值为410426.0,利润为602826.0\n",
      "第48次训练\n",
      "执行时间: 60.40188717842102 秒,50次未求解，当前强化学习值为54377.0,利润为313777.0\n",
      "第49次训练\n",
      "执行时间: 62.1056182384491 秒,36次未求解，当前强化学习值为353559.0,利润为554559.0\n",
      "第50次训练\n",
      "执行时间: 63.049800157547 秒,60次未求解，当前强化学习值为-187625.0,利润为108375.0\n",
      "第51次训练\n",
      "执行时间: 64.5289478302002 秒,41次未求解，当前强化学习值为236401.0,利润为457601.0\n",
      "第52次训练\n",
      "执行时间: 65.51738786697388 秒,58次未求解，当前强化学习值为-135383.0,利润为147517.0\n",
      "第53次训练\n",
      "执行时间: 67.07062864303589 秒,38次未求解，当前强化学习值为285800.0,利润为499800.0\n",
      "第54次训练\n",
      "执行时间: 68.92661738395691 秒,31次未求解，当前强化学习值为477191.0,利润为654391.0\n",
      "第55次训练\n",
      "执行时间: 70.24348044395447 秒,48次未求解，当前强化学习值为86877.0,利润为339677.0\n",
      "第56次训练\n",
      "执行时间: 71.25049686431885 秒,60次未求解，当前强化学习值为-189797.0,利润为108603.0\n",
      "第57次训练\n",
      "执行时间: 72.19648313522339 秒,60次未求解，当前强化学习值为-186590.0,利润为112310.0\n",
      "第58次训练\n",
      "执行时间: 73.2587080001831 秒,55次未求解，当前强化学习值为-77222.0,利润为201478.0\n",
      "第59次训练\n",
      "执行时间: 74.27636647224426 秒,58次未求解，当前强化学习值为-131425.0,利润为155775.0\n",
      "第60次训练\n",
      "执行时间: 75.9955050945282 秒,32次未求解，当前强化学习值为424336.0,利润为609836.0\n",
      "第61次训练\n",
      "执行时间: 77.16693782806396 秒,54次未求解，当前强化学习值为-50641.0,利润为226659.0\n",
      "第62次训练\n",
      "执行时间: 78.11392188072205 秒,60次未求解，当前强化学习值为-188324.0,利润为108376.0\n",
      "第63次训练\n",
      "执行时间: 79.20863389968872 秒,57次未求解，当前强化学习值为-113257.0,利润为170643.0\n",
      "第64次训练\n",
      "执行时间: 80.21722507476807 秒,58次未求解，当前强化学习值为-134936.0,利润为150664.0\n",
      "第65次训练\n",
      "执行时间: 81.21145439147949 秒,58次未求解，当前强化学习值为-133954.0,利润为149646.0\n",
      "第66次训练\n",
      "执行时间: 82.88465523719788 秒,35次未求解，当前强化学习值为388026.0,利润为581726.0\n",
      "第67次训练\n",
      "执行时间: 83.94495129585266 秒,55次未求解，当前强化学习值为-82790.0,利润为194710.0\n",
      "第68次训练\n",
      "执行时间: 84.91909193992615 秒,60次未求解，当前强化学习值为-189397.0,利润为108603.0\n",
      "第69次训练\n",
      "执行时间: 86.38045382499695 秒,41次未求解，当前强化学习值为235517.0,利润为455017.0\n",
      "第70次训练\n",
      "执行时间: 87.39861249923706 秒,60次未求解，当前强化学习值为-183013.0,利润为112087.0\n",
      "第71次训练\n",
      "执行时间: 88.33405995368958 秒,59次未求解，当前强化学习值为-181862.0,利润为112638.0\n",
      "第72次训练\n",
      "执行时间: 89.58133697509766 秒,51次未求解，当前强化学习值为19955.0,利润为288055.0\n",
      "第73次训练\n",
      "执行时间: 91.56137347221375 秒,23次未求解，当前强化学习值为667000.0,利润为816100.0\n",
      "第74次训练\n",
      "执行时间: 92.61986470222473 秒,58次未求解，当前强化学习值为-135307.0,利润为152993.0\n",
      "第75次训练\n",
      "执行时间: 93.57529735565186 秒,60次未求解，当前强化学习值为-189684.0,利润为105716.0\n",
      "第76次训练\n",
      "执行时间: 94.529616355896 秒,60次未求解，当前强化学习值为-188425.0,利润为108375.0\n",
      "第77次训练\n",
      "执行时间: 95.49549078941345 秒,60次未求解，当前强化学习值为-187397.0,利润为108603.0\n",
      "第78次训练\n",
      "执行时间: 96.7206883430481 秒,47次未求解，当前强化学习值为87333.0,利润为338433.0\n",
      "第79次训练\n",
      "执行时间: 97.80291867256165 秒,55次未求解，当前强化学习值为-72993.0,利润为206507.0\n",
      "第80次训练\n",
      "执行时间: 98.76064229011536 秒,60次未求解，当前强化学习值为-185213.0,利润为112087.0\n",
      "第81次训练\n",
      "执行时间: 99.71186685562134 秒,60次未求解，当前强化学习值为-188866.0,利润为107834.0\n",
      "第82次训练\n",
      "执行时间: 101.1367130279541 秒,44次未求解，当前强化学习值为168940.0,利润为403940.0\n",
      "第83次训练\n",
      "执行时间: 102.72883558273315 秒,37次未求解，当前强化学习值为302257.0,利润为513357.0\n",
      "第84次训练\n",
      "执行时间: 103.68984532356262 秒,60次未求解，当前强化学习值为-185397.0,利润为108603.0\n",
      "第85次训练\n",
      "执行时间: 104.7415132522583 秒,60次未求解，当前强化学习值为-187524.0,利润为108376.0\n",
      "第86次训练\n",
      "执行时间: 105.96233081817627 秒,51次未求解，当前强化学习值为35023.0,利润为293323.0\n",
      "第87次训练\n",
      "执行时间: 106.89941883087158 秒,60次未求解，当前强化学习值为-185897.0,利润为108603.0\n",
      "第88次训练\n",
      "执行时间: 107.8716242313385 秒,60次未求解，当前强化学习值为-186790.0,利润为112310.0\n",
      "第89次训练\n",
      "执行时间: 108.86753177642822 秒,60次未求解，当前强化学习值为-189598.0,利润为108602.0\n",
      "第90次训练\n",
      "执行时间: 110.29196691513062 秒,43次未求解，当前强化学习值为176044.0,利润为412544.0\n",
      "第91次训练\n",
      "执行时间: 111.2357029914856 秒,60次未求解，当前强化学习值为-187525.0,利润为108375.0\n",
      "第92次训练\n",
      "执行时间: 112.51746869087219 秒,50次未求解，当前强化学习值为55062.0,利润为308862.0\n",
      "第93次训练\n",
      "执行时间: 113.96824717521667 秒,43次未求解，当前强化学习值为179440.0,利润为413340.0\n",
      "第94次训练\n",
      "执行时间: 115.20708656311035 秒,54次未求解，当前强化学习值为-46443.0,利润为228857.0\n",
      "第95次训练\n",
      "执行时间: 116.19106864929199 秒,60次未求解，当前强化学习值为-191197.0,利润为108603.0\n",
      "第96次训练\n",
      "执行时间: 117.2844717502594 秒,54次未求解，当前强化学习值为-49093.0,利润为226707.0\n",
      "第97次训练\n",
      "执行时间: 118.26419067382812 秒,60次未求解，当前强化学习值为-188684.0,利润为105716.0\n",
      "第98次训练\n",
      "执行时间: 119.45181608200073 秒,51次未求解，当前强化学习值为25755.0,利润为288455.0\n",
      "第99次训练\n",
      "执行时间: 120.3560893535614 秒,60次未求解，当前强化学习值为-184166.0,利润为107834.0\n",
      "第100次训练\n",
      "最优模型为35\n",
      "执行时间: 121.3204836845398 秒,60次未求解，当前强化学习值为-196920.0,利润为104480.0\n",
      "第101次训练\n",
      "最优模型为35\n",
      "执行时间: 122.23788475990295 秒,60次未求解，当前强化学习值为-192184.0,利润为105716.0\n",
      "第102次训练\n",
      "最优模型为35\n",
      "执行时间: 123.19744944572449 秒,60次未求解，当前强化学习值为-185214.0,利润为112086.0\n",
      "第103次训练\n",
      "最优模型为35\n",
      "执行时间: 124.13381385803223 秒,60次未求解，当前强化学习值为-191384.0,利润为105716.0\n",
      "第104次训练\n",
      "最优模型为35\n",
      "执行时间: 125.08623027801514 秒,60次未求解，当前强化学习值为-186925.0,利润为108375.0\n",
      "第105次训练\n",
      "最优模型为35\n",
      "执行时间: 126.30799269676208 秒,48次未求解，当前强化学习值为79958.0,利润为334558.0\n",
      "第106次训练\n",
      "最优模型为35\n",
      "执行时间: 128.32657599449158 秒,23次未求解，当前强化学习值为639598.0,利润为791798.0\n",
      "第107次训练\n",
      "最优模型为35\n",
      "执行时间: 129.66857504844666 秒,47次未求解，当前强化学习值为98970.99993335613,利润为357870.9999333561\n",
      "第108次训练\n",
      "最优模型为35\n",
      "执行时间: 130.82310009002686 秒,58次未求解，当前强化学习值为-143817.0,利润为146083.0\n",
      "第109次训练\n",
      "最优模型为35\n",
      "执行时间: 131.8901650905609 秒,58次未求解，当前强化学习值为-136629.0,利润为152071.0\n",
      "第110次训练\n",
      "最优模型为35\n",
      "执行时间: 134.40908408164978 秒,7次未求解，当前强化学习值为983056.0,利润为1087856.0\n",
      "第111次训练\n",
      "最优模型为35\n",
      "执行时间: 135.56054759025574 秒,58次未求解，当前强化学习值为-137837.0,利润为150663.0\n",
      "第112次训练\n",
      "最优模型为35\n",
      "执行时间: 136.64506363868713 秒,60次未求解，当前强化学习值为-186825.0,利润为108375.0\n",
      "第113次训练\n",
      "最优模型为35\n",
      "执行时间: 137.77993845939636 秒,54次未求解，当前强化学习值为-60230.0,利润为219370.0\n",
      "第114次训练\n",
      "最优模型为35\n",
      "执行时间: 139.00611233711243 秒,59次未求解，当前强化学习值为-165156.0,利润为127244.0\n",
      "第115次训练\n",
      "最优模型为35\n",
      "执行时间: 140.6410892009735 秒,48次未求解，当前强化学习值为86041.0,利润为339341.0\n",
      "第116次训练\n",
      "最优模型为35\n",
      "执行时间: 142.40051746368408 秒,35次未求解，当前强化学习值为376823.0,利润为571523.0\n",
      "第117次训练\n",
      "最优模型为35\n",
      "执行时间: 143.57704281806946 秒,53次未求解，当前强化学习值为-48617.0,利润为224883.0\n",
      "第118次训练\n",
      "最优模型为35\n",
      "执行时间: 145.75464940071106 秒,21次未求解，当前强化学习值为670918.0,利润为821018.0\n",
      "第119次训练\n",
      "最优模型为35\n",
      "执行时间: 147.02079129219055 秒,50次未求解，当前强化学习值为50265.0,利润为305165.0\n",
      "第120次训练\n",
      "最优模型为35\n",
      "执行时间: 148.03165912628174 秒,60次未求解，当前强化学习值为-191624.0,利润为108376.0\n",
      "第121次训练\n",
      "最优模型为35\n",
      "执行时间: 149.89130783081055 秒,27次未求解，当前强化学习值为552953.0,利润为719653.0\n",
      "第122次训练\n",
      "最优模型为35\n",
      "执行时间: 151.05049443244934 秒,45次未求解，当前强化学习值为160090.0,利润为396590.0\n",
      "第123次训练\n",
      "最优模型为35\n",
      "执行时间: 151.98367047309875 秒,60次未求解，当前强化学习值为-191197.0,利润为108603.0\n",
      "第124次训练\n",
      "最优模型为35\n",
      "执行时间: 152.91549134254456 秒,58次未求解，当前强化学习值为-134437.0,利润为150663.0\n",
      "第125次训练\n",
      "最优模型为35\n",
      "执行时间: 154.26281690597534 秒,38次未求解，当前强化学习值为302843.0,利润为512143.0\n",
      "第126次训练\n",
      "最优模型为35\n",
      "执行时间: 155.22281908988953 秒,58次未求解，当前强化学习值为-136437.0,利润为150663.0\n",
      "第127次训练\n",
      "最优模型为35\n",
      "执行时间: 156.25933933258057 秒,52次未求解，当前强化学习值为-13601.0,利润为256899.0\n",
      "第128次训练\n",
      "最优模型为35\n",
      "执行时间: 157.1459345817566 秒,58次未求解，当前强化学习值为-143435.0,利润为143665.0\n",
      "第129次训练\n",
      "最优模型为35\n",
      "执行时间: 158.03922820091248 秒,60次未求解，当前强化学习值为-187925.0,利润为108375.0\n",
      "第130次训练\n",
      "最优模型为35\n",
      "执行时间: 158.92097401618958 秒,60次未求解，当前强化学习值为-193284.0,利润为105716.0\n",
      "第131次训练\n",
      "最优模型为35\n",
      "执行时间: 159.84232568740845 秒,60次未求解，当前强化学习值为-185489.0,利润为112311.0\n",
      "第132次训练\n",
      "最优模型为35\n",
      "执行时间: 160.9287087917328 秒,54次未求解，当前强化学习值为-55947.0,利润为224053.0\n",
      "第133次训练\n",
      "最优模型为35\n",
      "执行时间: 162.09905314445496 秒,51次未求解，当前强化学习值为27708.0,利润为290308.0\n",
      "第134次训练\n",
      "最优模型为35\n",
      "执行时间: 163.93159818649292 秒,18次未求解，当前强化学习值为766780.0,利润为900680.0\n",
      "第135次训练\n",
      "最优模型为35\n",
      "执行时间: 164.9489631652832 秒,60次未求解，当前强化学习值为-187066.0,利润为107834.0\n",
      "第136次训练\n",
      "最优模型为35\n",
      "执行时间: 166.57515454292297 秒,37次未求解，当前强化学习值为310584.0,利润为519484.0\n",
      "第137次训练\n",
      "最优模型为35\n",
      "执行时间: 167.5441632270813 秒,60次未求解，当前强化学习值为-183913.0,利润为112087.0\n",
      "第138次训练\n",
      "最优模型为35\n",
      "执行时间: 168.517085313797 秒,60次未求解，当前强化学习值为-181515.0,利润为112085.0\n",
      "第139次训练\n",
      "最优模型为35\n",
      "执行时间: 169.47752785682678 秒,60次未求解，当前强化学习值为-189025.0,利润为108375.0\n",
      "第140次训练\n",
      "最优模型为35\n",
      "执行时间: 170.4704098701477 秒,58次未求解，当前强化学习值为-145145.0,利润为143955.0\n",
      "第141次训练\n",
      "最优模型为35\n",
      "执行时间: 171.61494302749634 秒,57次未求解，当前强化学习值为-123261.0,利润为163339.0\n",
      "第142次训练\n",
      "最优模型为35\n",
      "执行时间: 172.6714940071106 秒,56次未求解，当前强化学习值为-82394.0,利润为196206.0\n",
      "第143次训练\n",
      "最优模型为35\n",
      "执行时间: 174.24093580245972 秒,40次未求解，当前强化学习值为265885.0,利润为482185.0\n",
      "第144次训练\n",
      "最优模型为35\n",
      "执行时间: 175.2280457019806 秒,59次未求解，当前强化学习值为-164656.0,利润为127244.0\n",
      "第145次训练\n",
      "最优模型为35\n",
      "执行时间: 177.02452993392944 秒,30次未求解，当前强化学习值为494887.0,利润为671987.0\n",
      "第146次训练\n",
      "最优模型为35\n",
      "执行时间: 178.81011962890625 秒,32次未求解，当前强化学习值为454655.0,利润为637055.0\n",
      "第147次训练\n",
      "最优模型为35\n",
      "执行时间: 179.76635479927063 秒,60次未求解，当前强化学习值为-184214.0,利润为112086.0\n",
      "第148次训练\n",
      "最优模型为35\n",
      "执行时间: 180.75130462646484 秒,60次未求解，当前强化学习值为-184889.0,利润为112311.0\n",
      "第149次训练\n",
      "最优模型为35\n",
      "执行时间: 181.89295649528503 秒,51次未求解，当前强化学习值为17677.0,利润为282877.0\n",
      "第150次训练\n",
      "最优模型为35\n",
      "执行时间: 182.9985318183899 秒,54次未求解，当前强化学习值为-56781.0,利润为219019.0\n",
      "第151次训练\n",
      "最优模型为35\n",
      "执行时间: 184.2939157485962 秒,48次未求解，当前强化学习值为107515.0,利润为354815.0\n",
      "第152次训练\n",
      "最优模型为35\n",
      "执行时间: 185.66520404815674 秒,43次未求解，当前强化学习值为196426.0,利润为428626.0\n",
      "第153次训练\n",
      "最优模型为35\n",
      "执行时间: 186.6812460422516 秒,58次未求解，当前强化学习值为-137489.0,利润为147511.0\n",
      "第154次训练\n",
      "最优模型为35\n",
      "执行时间: 187.61258220672607 秒,60次未求解，当前强化学习值为-192720.0,利润为104480.0\n",
      "第155次训练\n",
      "最优模型为35\n",
      "执行时间: 188.56989240646362 秒,60次未求解，当前强化学习值为-195420.0,利润为104480.0\n",
      "第156次训练\n",
      "最优模型为35\n",
      "执行时间: 189.5723340511322 秒,58次未求解，当前强化学习值为-135482.0,利润为147518.0\n",
      "第157次训练\n",
      "最优模型为35\n",
      "执行时间: 190.57703518867493 秒,59次未求解，当前强化学习值为-162056.0,利润为127244.0\n",
      "第158次训练\n",
      "最优模型为35\n",
      "执行时间: 191.57029938697815 秒,58次未求解，当前强化学习值为-131497.0,利润为151303.0\n",
      "第159次训练\n",
      "最优模型为35\n",
      "执行时间: 193.18104481697083 秒,37次未求解，当前强化学习值为314272.0,利润为528572.0\n",
      "第160次训练\n",
      "最优模型为35\n",
      "执行时间: 194.1245415210724 秒,60次未求解，当前强化学习值为-192543.0,利润为104257.0\n",
      "第161次训练\n",
      "最优模型为35\n",
      "执行时间: 196.5143163204193 秒,9次未求解，当前强化学习值为990179.0,利润为1085879.0\n",
      "第162次训练\n",
      "最优模型为35\n",
      "执行时间: 197.64509797096252 秒,58次未求解，当前强化学习值为-129827.0,利润为155773.0\n",
      "第163次训练\n",
      "最优模型为35\n",
      "执行时间: 198.55867409706116 秒,60次未求解，当前强化学习值为-183925.0,利润为108375.0\n",
      "第164次训练\n",
      "最优模型为35\n",
      "执行时间: 199.6782088279724 秒,58次未求解，当前强化学习值为-146245.0,利润为143955.0\n",
      "第165次训练\n",
      "最优模型为35\n",
      "执行时间: 200.66531991958618 秒,59次未求解，当前强化学习值为-166653.0,利润为127247.0\n",
      "第166次训练\n",
      "最优模型为35\n",
      "执行时间: 201.62729954719543 秒,60次未求解，当前强化学习值为-186589.0,利润为112311.0\n",
      "第167次训练\n",
      "最优模型为35\n",
      "执行时间: 202.84313583374023 秒,51次未求解，当前强化学习值为36776.0,利润为297976.0\n",
      "第168次训练\n",
      "最优模型为35\n",
      "执行时间: 203.75165796279907 秒,60次未求解，当前强化学习值为-185115.0,利润为112085.0\n",
      "第169次训练\n",
      "最优模型为35\n",
      "执行时间: 205.69684290885925 秒,6次未求解，当前强化学习值为1047551.0,利润为1136751.0\n",
      "第170次训练\n",
      "最优模型为35\n",
      "执行时间: 206.5575942993164 秒,60次未求解，当前强化学习值为-188198.0,利润为108602.0\n",
      "第171次训练\n",
      "最优模型为35\n",
      "执行时间: 207.49178290367126 秒,58次未求解，当前强化学习值为-148817.0,利润为146083.0\n",
      "第172次训练\n",
      "最优模型为35\n",
      "执行时间: 208.49127411842346 秒,60次未求解，当前强化学习值为-189425.0,利润为108375.0\n",
      "第173次训练\n",
      "最优模型为35\n",
      "执行时间: 209.3841781616211 秒,58次未求解，当前强化学习值为-139403.0,利润为152997.0\n",
      "第174次训练\n",
      "最优模型为35\n",
      "执行时间: 210.64575910568237 秒,43次未求解，当前强化学习值为169834.0,利润为408134.0\n",
      "第175次训练\n",
      "最优模型为35\n",
      "执行时间: 211.51969575881958 秒,60次未求解，当前强化学习值为-193766.0,利润为107234.0\n",
      "第176次训练\n",
      "最优模型为35\n",
      "执行时间: 212.39733839035034 秒,58次未求解，当前强化学习值为-138903.0,利润为145397.0\n",
      "第177次训练\n",
      "最优模型为35\n",
      "执行时间: 213.3943166732788 秒,54次未求解，当前强化学习值为-56505.0,利润为221195.0\n",
      "第178次训练\n",
      "最优模型为35\n",
      "执行时间: 214.3564896583557 秒,54次未求解，当前强化学习值为-58473.0,利润为222027.0\n",
      "第179次训练\n",
      "最优模型为35\n",
      "执行时间: 215.46778202056885 秒,45次未求解，当前强化学习值为152563.0,利润为390763.0\n",
      "第180次训练\n",
      "最优模型为35\n",
      "执行时间: 216.35304832458496 秒,60次未求解，当前强化学习值为-190566.0,利润为107834.0\n",
      "第181次训练\n",
      "最优模型为35\n",
      "执行时间: 217.1901683807373 秒,60次未求解，当前强化学习值为-189566.0,利润为107834.0\n",
      "第182次训练\n",
      "最优模型为35\n",
      "执行时间: 218.28790140151978 秒,47次未求解，当前强化学习值为102841.0,利润为351441.0\n",
      "第183次训练\n",
      "最优模型为35\n",
      "执行时间: 219.1589617729187 秒,60次未求解，当前强化学习值为-186188.0,利润为112312.0\n",
      "第184次训练\n",
      "最优模型为35\n",
      "执行时间: 220.04464960098267 秒,58次未求解，当前强化学习值为-137583.0,利润为149517.0\n",
      "第185次训练\n",
      "最优模型为35\n",
      "执行时间: 220.87177443504333 秒,60次未求解，当前强化学习值为-185797.0,利润为108603.0\n",
      "第186次训练\n",
      "最优模型为35\n",
      "执行时间: 221.76338267326355 秒,58次未求解，当前强化学习值为-136153.0,利润为149647.0\n",
      "第187次训练\n",
      "最优模型为35\n",
      "执行时间: 222.57552814483643 秒,60次未求解，当前强化学习值为-195184.0,利润为105716.0\n",
      "第188次训练\n",
      "最优模型为35\n",
      "执行时间: 223.44258451461792 秒,59次未求解，当前强化学习值为-161479.0,利润为129321.0\n",
      "第189次训练\n",
      "最优模型为35\n",
      "执行时间: 224.59621119499207 秒,43次未求解，当前强化学习值为178706.0,利润为416006.0\n",
      "第190次训练\n",
      "最优模型为35\n",
      "执行时间: 225.45598483085632 秒,60次未求解，当前强化学习值为-192425.0,利润为108375.0\n",
      "第191次训练\n",
      "最优模型为35\n",
      "执行时间: 226.28961968421936 秒,60次未求解，当前强化学习值为-189966.0,利润为107834.0\n",
      "第192次训练\n",
      "最优模型为35\n",
      "执行时间: 228.23953676223755 秒,31次未求解，当前强化学习值为482222.0,利润为660722.0\n",
      "第193次训练\n",
      "最优模型为35\n",
      "执行时间: 229.31199431419373 秒,59次未求解，当前强化学习值为-165180.0,利润为129320.0\n",
      "第194次训练\n",
      "最优模型为35\n",
      "执行时间: 230.2102234363556 秒,60次未求解，当前强化学习值为-182688.0,利润为112312.0\n",
      "第195次训练\n",
      "最优模型为35\n",
      "执行时间: 231.1345226764679 秒,60次未求解，当前强化学习值为-181087.0,利润为112313.0\n",
      "第196次训练\n",
      "最优模型为35\n",
      "执行时间: 232.0563771724701 秒,60次未求解，当前强化学习值为-192484.0,利润为105716.0\n",
      "第197次训练\n",
      "最优模型为35\n",
      "执行时间: 233.13523602485657 秒,51次未求解，当前强化学习值为38378.0,利润为297678.0\n",
      "第198次训练\n",
      "最优模型为35\n",
      "执行时间: 233.98434162139893 秒,60次未求解，当前强化学习值为-191384.0,利润为105716.0\n",
      "第199次训练\n",
      "最优模型为35\n",
      "执行时间: 235.4276783466339 秒,34次未求解，当前强化学习值为398089.0,利润为596089.0\n",
      "第200次训练\n",
      "最优模型为35\n",
      "执行时间: 236.49652671813965 秒,51次未求解，当前强化学习值为42467.0,利润为296067.0\n",
      "第201次训练\n",
      "最优模型为35\n",
      "执行时间: 237.3912591934204 秒,58次未求解，当前强化学习值为-137200.0,利润为151300.0\n",
      "第202次训练\n",
      "最优模型为35\n",
      "执行时间: 238.45400261878967 秒,52次未求解，当前强化学习值为-18412.0,利润为255788.0\n",
      "第203次训练\n",
      "最优模型为35\n",
      "执行时间: 239.55296993255615 秒,47次未求解，当前强化学习值为117494.0,利润为364394.0\n",
      "第204次训练\n",
      "最优模型为35\n",
      "执行时间: 240.5828664302826 秒,51次未求解，当前强化学习值为9457.0,利润为275757.0\n",
      "第205次训练\n",
      "最优模型为35\n",
      "执行时间: 241.49330592155457 秒,60次未求解，当前强化学习值为-190498.0,利润为108602.0\n",
      "第206次训练\n",
      "最优模型为35\n",
      "执行时间: 242.33976364135742 秒,60次未求解，当前强化学习值为-183399.0,利润为108601.0\n",
      "第207次训练\n",
      "最优模型为35\n",
      "执行时间: 243.2067174911499 秒,60次未求解，当前强化学习值为-191443.0,利润为104257.0\n",
      "第208次训练\n",
      "最优模型为35\n",
      "执行时间: 244.13494753837585 秒,59次未求解，当前强化学习值为-161679.0,利润为129321.0\n",
      "第209次训练\n",
      "最优模型为35\n",
      "执行时间: 245.13988399505615 秒,51次未求解，当前强化学习值为23954.0,利润为286754.0\n",
      "第210次训练\n",
      "最优模型为35\n",
      "执行时间: 246.03105187416077 秒,60次未求解，当前强化学习值为-183013.0,利润为112087.0\n",
      "第211次训练\n",
      "最优模型为35\n",
      "执行时间: 247.0749294757843 秒,48次未求解，当前强化学习值为76701.0,利润为330601.0\n",
      "第212次训练\n",
      "最优模型为35\n",
      "执行时间: 248.1468517780304 秒,49次未求解，当前强化学习值为59010.0,利润为318010.0\n",
      "第213次训练\n",
      "最优模型为35\n",
      "执行时间: 249.4684054851532 秒,41次未求解，当前强化学习值为202808.0,利润为437508.0\n",
      "第214次训练\n",
      "最优模型为35\n",
      "执行时间: 250.35275077819824 秒,60次未求解，当前强化学习值为-189198.0,利润为108602.0\n",
      "第215次训练\n",
      "最优模型为35\n",
      "执行时间: 252.0311381816864 秒,24次未求解，当前强化学习值为648412.0,利润为803012.0\n",
      "第216次训练\n",
      "最优模型为35\n",
      "执行时间: 253.1603446006775 秒,51次未求解，当前强化学习值为48759.0,利润为307659.0\n",
      "第217次训练\n",
      "最优模型为35\n",
      "执行时间: 254.5544149875641 秒,40次未求解，当前强化学习值为270620.0,利润为488320.0\n",
      "第218次训练\n",
      "最优模型为35\n",
      "执行时间: 255.55295395851135 秒,59次未求解，当前强化学习值为-163656.0,利润为127244.0\n",
      "第219次训练\n",
      "最优模型为35\n",
      "执行时间: 256.5672302246094 秒,58次未求解，当前强化学习值为-140753.0,利润为149647.0\n",
      "第220次训练\n",
      "最优模型为35\n",
      "执行时间: 257.70841789245605 秒,48次未求解，当前强化学习值为73917.0,利润为328117.0\n",
      "第221次训练\n",
      "最优模型为35\n",
      "执行时间: 258.69633173942566 秒,59次未求解，当前强化学习值为-163358.0,利润为127242.0\n",
      "第222次训练\n",
      "最优模型为35\n",
      "执行时间: 260.21940994262695 秒,43次未求解，当前强化学习值为198826.0,利润为425426.0\n",
      "第223次训练\n",
      "最优模型为35\n",
      "执行时间: 261.903112411499 秒,33次未求解，当前强化学习值为405308.0,利润为600808.0\n",
      "第224次训练\n",
      "最优模型为35\n",
      "执行时间: 262.8095097541809 秒,60次未求解，当前强化学习值为-185625.0,利润为108375.0\n",
      "第225次训练\n",
      "最优模型为35\n",
      "执行时间: 263.83556151390076 秒,57次未求解，当前强化学习值为-123961.0,利润为163339.0\n",
      "第226次训练\n",
      "最优模型为35\n",
      "执行时间: 265.05682849884033 秒,51次未求解，当前强化学习值为40659.0,利润为299259.0\n",
      "第227次训练\n",
      "最优模型为35\n",
      "执行时间: 265.98778200149536 秒,60次未求解，当前强化学习值为-182715.0,利润为112085.0\n",
      "第228次训练\n",
      "最优模型为35\n",
      "执行时间: 267.057181596756 秒,55次未求解，当前强化学习值为-48599.0,利润为218201.0\n",
      "第229次训练\n",
      "最优模型为35\n",
      "执行时间: 267.9928729534149 秒,60次未求解，当前强化学习值为-194184.0,利润为105716.0\n",
      "第230次训练\n",
      "最优模型为35\n",
      "执行时间: 268.964234828949 秒,60次未求解，当前强化学习值为-187866.0,利润为107834.0\n",
      "第231次训练\n",
      "最优模型为35\n",
      "执行时间: 269.9218215942383 秒,60次未求解，当前强化学习值为-193943.0,利润为104257.0\n",
      "第232次训练\n",
      "最优模型为35\n",
      "执行时间: 270.89600133895874 秒,60次未求解，当前强化学习值为-191198.0,利润为108602.0\n",
      "第233次训练\n",
      "最优模型为35\n",
      "执行时间: 272.1765356063843 秒,48次未求解，当前强化学习值为79180.0,利润为333880.0\n",
      "第234次训练\n",
      "最优模型为35\n",
      "执行时间: 273.50864362716675 秒,47次未求解，当前强化学习值为109717.0,利润为353317.0\n",
      "第235次训练\n",
      "最优模型为35\n",
      "执行时间: 274.6288814544678 秒,54次未求解，当前强化学习值为-55096.0,利润为221304.0\n",
      "第236次训练\n",
      "最优模型为35\n",
      "执行时间: 275.55756282806396 秒,60次未求解，当前强化学习值为-181387.0,利润为112313.0\n",
      "第237次训练\n",
      "最优模型为35\n",
      "执行时间: 276.5299873352051 秒,60次未求解，当前强化学习值为-185989.0,利润为112311.0\n",
      "第238次训练\n",
      "最优模型为35\n",
      "执行时间: 277.5316972732544 秒,60次未求解，当前强化学习值为-186115.0,利润为112085.0\n",
      "第239次训练\n",
      "最优模型为35\n",
      "执行时间: 278.9533004760742 秒,42次未求解，当前强化学习值为211392.0,利润为440392.0\n",
      "第240次训练\n",
      "最优模型为35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 151\u001b[0m\n\u001b[0;32m    149\u001b[0m     next_order_states \u001b[38;5;241m=\u001b[39m vectorization_order(orders_unmatched)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# 这里防止梯度爆炸缩小了reward\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgrid_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_vehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_order_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     env\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m=\u001b[39m time\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:167\u001b[0m, in \u001b[0;36mMultiAgentAC.update\u001b[1;34m(self, vehicle_states, order_states, actions, rewards, next_vehicle_states, next_order_states, dones)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 167\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dirty_test\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dirty_test\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid >= invalid_time:\n",
    "                    agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "            # 一个循环代码让我达到最优\n",
    "            # 屁股后面的代码是为了让我达到最优\n",
    "            # 这里是为了让我达到最优\n",
    "            # 现在放弃了重采样\n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        torch.save(agent, f\"model_checkpoint_{episode}.pth\")\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:alns.ALNS:Finished iterating in 0.15s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best routes: [[0, 1, 0], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0]]\n",
      "Best cost: 858.1198159028768\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from alns import ALNS, State\n",
    "from alns.accept import SimulatedAnnealing\n",
    "from alns.stop import MaxIterations\n",
    "from alns.select import RouletteWheel\n",
    "import random\n",
    "\n",
    "# Problem Data\n",
    "np.random.seed(42)\n",
    "num_customers = 10\n",
    "num_vehicles = 3\n",
    "capacity = 100\n",
    "\n",
    "# 随机生成坐标和需求\n",
    "depot = np.array([50, 50])\n",
    "nodes = np.random.randint(0, 100, (num_customers, 2))\n",
    "demands = np.random.randint(5, 20, num_customers)\n",
    "\n",
    "# 计算欧几里得距离\n",
    "def distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "distance_matrix = np.zeros((num_customers + 1, num_customers + 1))\n",
    "nodes_full = np.vstack([depot, nodes])  # 加入仓库\n",
    "for i in range(len(nodes_full)):\n",
    "    for j in range(len(nodes_full)):\n",
    "        distance_matrix[i, j] = distance(nodes_full[i], nodes_full[j])\n",
    "\n",
    "# 初始状态\n",
    "class VRPState(State):\n",
    "    def __init__(self, routes):\n",
    "        self.routes = routes\n",
    "    \n",
    "    def objective(self):\n",
    "        total_cost = sum(\n",
    "            distance_matrix[route[i], route[i+1]]\n",
    "            for route in self.routes for i in range(len(route)-1)\n",
    "        )\n",
    "        return total_cost\n",
    "    \n",
    "    def copy(self):\n",
    "        return VRPState([route[:] for route in self.routes])\n",
    "\n",
    "# 破坏算子\n",
    "def random_removal(state, rng, num_remove=2):\n",
    "    if isinstance(num_remove, np.random.Generator):  \n",
    "        num_remove = rng.integers(1, 4)  \n",
    "\n",
    "    new_state = state.copy()\n",
    "    for _ in range(num_remove):\n",
    "        if any(new_state.routes):\n",
    "            route = random.choice(new_state.routes)\n",
    "            if len(route) > 2:  # 只有在长度 > 2 时才移除\n",
    "                idx = rng.integers(1, max(2, len(route) - 1))  # 确保 idx 合法\n",
    "                route.pop(idx)\n",
    "    return new_state\n",
    "\n",
    "\n",
    "\n",
    "# 修复算子\n",
    "def greedy_insert(state, rng):\n",
    "    new_state = state.copy()\n",
    "    unassigned = [i for i in range(1, num_customers + 1) if not any(i in r for r in new_state.routes)]\n",
    "    for i in unassigned:\n",
    "        best_cost = float('inf')\n",
    "        best_route = None\n",
    "        best_position = None\n",
    "        for route in new_state.routes:\n",
    "            for pos in range(1, len(route)):\n",
    "                temp_route = route[:pos] + [i] + route[pos:]\n",
    "                cost = sum(distance_matrix[temp_route[j], temp_route[j+1]] for j in range(len(temp_route)-1))\n",
    "                if cost < best_cost:\n",
    "                    best_cost, best_route, best_position = cost, route, pos\n",
    "        if best_route is not None:\n",
    "            best_route.insert(best_position, i)\n",
    "    return new_state\n",
    "\n",
    "# ALNS 运行\n",
    "initial_routes = [[0, i, 0] for i in range(1, num_customers + 1)]  # 每个客户单独一辆车\n",
    "initial_state = VRPState(initial_routes)\n",
    "alns = ALNS()\n",
    "alns.add_destroy_operator(random_removal)\n",
    "alns.add_repair_operator(greedy_insert)\n",
    "\n",
    "# 设定接受准则（模拟退火）\n",
    "accept = SimulatedAnnealing(1000, 1, 500, method=\"linear\")\n",
    "select = RouletteWheel([1] * 4, 0.8, 1, 1)\n",
    "stop = MaxIterations(1000)\n",
    "\n",
    "result = alns.iterate(initial_state, select, accept, stop)\n",
    "\n",
    "# 输出最优解\n",
    "best_state = result.best_state\n",
    "print(\"Best routes:\", best_state.routes)\n",
    "print(\"Best cost:\", best_state.objective())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据固定的逐步调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# 假设有如下初始化函数\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 3\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 生成固定样本\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G, speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 保存样本到本地\n",
    "with open('sample_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'Vehicles': Vehicles,\n",
    "        'Total_order': Total_order,\n",
    "        'G':G\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为216022.0,117次未求解\n",
      "第1次训练\n",
      "执行时间: 1.9699275493621826 秒,123次未求解，当前强化学习值为95054.0,利润为95054.0\n",
      "第2次训练\n",
      "执行时间: 3.004671096801758 秒,116次未求解，当前强化学习值为216801.0,利润为216801.0\n",
      "第3次训练\n",
      "执行时间: 4.760244846343994 秒,96次未求解，当前强化学习值为578463.0,利润为578463.0\n",
      "第4次训练\n",
      "执行时间: 5.334847688674927 秒,133次未求解，当前强化学习值为-82127.0,利润为-82127.0\n",
      "第5次训练\n",
      "执行时间: 6.015341520309448 秒,129次未求解，当前强化学习值为-19336.0,利润为-19336.0\n",
      "第6次训练\n",
      "执行时间: 6.77469277381897 秒,130次未求解，当前强化学习值为-29743.0,利润为-29743.0\n",
      "第7次训练\n",
      "执行时间: 7.300514221191406 秒,133次未求解，当前强化学习值为-92538.0,利润为-92538.0\n",
      "第8次训练\n",
      "执行时间: 8.26507830619812 秒,122次未求解，当前强化学习值为111367.0,利润为111367.0\n",
      "第9次训练\n",
      "执行时间: 9.195048332214355 秒,119次未求解，当前强化学习值为156902.0,利润为156902.0\n",
      "第10次训练\n",
      "执行时间: 10.180094957351685 秒,118次未求解，当前强化学习值为190720.0,利润为190720.0\n",
      "第11次训练\n",
      "执行时间: 10.724729776382446 秒,133次未求解，当前强化学习值为-82135.0,利润为-82135.0\n",
      "第12次训练\n",
      "执行时间: 11.731079816818237 秒,123次未求解，当前强化学习值为77547.0,利润为77547.0\n",
      "第13次训练\n",
      "执行时间: 13.304919481277466 秒,108次未求解，当前强化学习值为353126.0,利润为353126.0\n",
      "第14次训练\n",
      "执行时间: 14.441745042800903 秒,115次未求解，当前强化学习值为231502.0,利润为231502.0\n",
      "第15次训练\n",
      "执行时间: 16.554659843444824 秒,86次未求解，当前强化学习值为756722.0,利润为756722.0\n",
      "第16次训练\n",
      "执行时间: 17.121607542037964 秒,133次未求解，当前强化学习值为-82191.0,利润为-82191.0\n",
      "第17次训练\n",
      "执行时间: 18.26468825340271 秒,114次未求解，当前强化学习值为245465.0,利润为245465.0\n",
      "第18次训练\n",
      "执行时间: 18.863013982772827 秒,133次未求解，当前强化学习值为-76383.0,利润为-76383.0\n",
      "第19次训练\n",
      "执行时间: 19.46042275428772 秒,133次未求解，当前强化学习值为-92538.0,利润为-92538.0\n",
      "第20次训练\n",
      "执行时间: 21.15739941596985 秒,101次未求解，当前强化学习值为491124.0,利润为491124.0\n",
      "第21次训练\n",
      "执行时间: 21.676100492477417 秒,133次未求解，当前强化学习值为-88971.0,利润为-88971.0\n",
      "第22次训练\n",
      "执行时间: 22.429863452911377 秒,129次未求解，当前强化学习值为-22698.0,利润为-22698.0\n",
      "第23次训练\n",
      "执行时间: 23.496100187301636 秒,115次未求解，当前强化学习值为235110.0,利润为235110.0\n",
      "第24次训练\n",
      "执行时间: 25.14485001564026 秒,100次未求解，当前强化学习值为500635.0,利润为500635.0\n",
      "第25次训练\n",
      "执行时间: 25.70893383026123 秒,133次未求解，当前强化学习值为-82186.0,利润为-82186.0\n",
      "第26次训练\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        \n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            if last_vehicle:\n",
    "                agent.update_third(vehicle_states, order_states, action, selected_log_probs, log_probs, probs,\n",
    "                            grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            if len(group[0]) != 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                mask = env.get_mask(orders_unmatched)\n",
    "                action, selected_log_probs, log_probs, probs = agent.take_action_third(vehicle_states, order_states, mask, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "                ACTIONS.append(action) \n",
    "\n",
    "        \n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else :\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            grid_reward =   objval/1000\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "           \n",
    "       \n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验记录\n",
    "\n",
    "1. 基模型：不收敛，差！\n",
    "2. greedy_epsilon二次收敛，模型永远更新参数：不收敛，差！\n",
    "3. 改变了参数的传递，使得take_action_third()返回当前掩码下的概率分布，同时将这些概率传入update_third()，因此在对应使用中不再是之前的重新计算。\n",
    "4. 优化模型不求解则强化学习亦不更新参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新智能体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**新智能体的设计逻辑**\n",
    "1. 将订单按照最终可行的城市集分为多类\n",
    "2. 为每一个类设计一个智能体\n",
    "3. 智能体只在有对应订单时才进行强化学习\n",
    "4. 智能体的奖励来自于订单执行后的利润"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次训练\n",
      "未加强化学习利润为1208657.0,0次未求解\n",
      "第1次训练\n",
      "045 0 0\n",
      "045 4 0\n",
      "045 9 0\n",
      "045 10 3701\n",
      "045 12 0\n",
      "045 16 0\n",
      "045 17 9657\n",
      "045 22 0\n",
      "045 24 0\n",
      "045 26 0\n",
      "045 33 0\n",
      "045 45 0\n",
      "045 47 0\n",
      "045 49 0\n",
      "045 51 0\n",
      "045 53 0\n",
      "045 64 0\n",
      "045 65 4828\n",
      "045 78 0\n",
      "045 80 0\n",
      "045 83 0\n",
      "045 87 0\n",
      "045 90 0\n",
      "045 94 0\n",
      "045 96 0\n",
      "045 97 1814\n",
      "045 101 0\n",
      "045 111 0\n",
      "045 129 0\n",
      "045 130 4931\n",
      "045 132 0\n",
      "045 140 0\n",
      "执行时间: 3.6535768508911133 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第2次训练\n",
      "045 0 0\n",
      "045 4 0\n",
      "045 9 0\n",
      "045 10 3770\n",
      "045 12 0\n",
      "045 16 0\n",
      "045 17 9630\n",
      "045 22 0\n",
      "045 24 0\n",
      "045 26 0\n",
      "045 33 0\n",
      "045 45 0\n",
      "045 47 0\n",
      "045 49 0\n",
      "045 51 0\n",
      "045 53 0\n",
      "045 64 0\n",
      "045 65 4884\n",
      "045 78 0\n",
      "045 80 0\n",
      "045 83 0\n",
      "045 87 0\n",
      "045 90 0\n",
      "045 94 0\n",
      "045 96 0\n",
      "045 97 1827\n",
      "045 101 0\n",
      "045 111 0\n",
      "045 129 0\n",
      "045 130 4912\n",
      "045 132 0\n",
      "045 140 0\n",
      "执行时间: 5.419111490249634 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第3次训练\n",
      "045 0 0\n",
      "045 4 0\n",
      "045 9 0\n",
      "045 10 3722\n",
      "045 12 0\n",
      "045 16 0\n",
      "045 17 9710\n",
      "045 22 0\n",
      "045 24 0\n",
      "045 26 0\n",
      "045 33 0\n",
      "045 45 0\n",
      "045 47 0\n",
      "045 49 0\n",
      "045 51 0\n",
      "045 53 0\n",
      "045 64 0\n",
      "045 65 4820\n",
      "045 78 0\n",
      "045 80 0\n",
      "045 83 0\n",
      "045 87 0\n",
      "045 90 0\n",
      "045 94 0\n",
      "045 96 0\n",
      "045 97 1845\n",
      "045 101 0\n",
      "045 111 0\n",
      "045 129 0\n",
      "045 130 4946\n",
      "045 132 0\n",
      "045 140 0\n",
      "执行时间: 7.169511556625366 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第4次训练\n",
      "045 0 0\n",
      "045 4 0\n",
      "045 9 0\n",
      "045 10 3772\n",
      "045 12 0\n",
      "045 16 0\n",
      "045 17 9735\n",
      "045 22 0\n",
      "045 24 0\n",
      "045 26 0\n",
      "045 33 0\n",
      "045 45 0\n",
      "045 47 0\n",
      "045 49 0\n",
      "045 51 0\n",
      "045 53 0\n",
      "045 64 0\n",
      "045 65 4893\n",
      "045 78 0\n",
      "045 80 0\n",
      "045 83 0\n",
      "045 87 0\n",
      "045 90 0\n",
      "045 94 0\n",
      "045 96 0\n",
      "045 97 1757\n",
      "045 101 0\n",
      "045 111 0\n",
      "045 129 0\n",
      "045 130 4954\n",
      "045 132 0\n",
      "045 140 0\n",
      "执行时间: 8.940927505493164 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第5次训练\n",
      "045 0 0\n",
      "045 4 0\n",
      "045 9 0\n",
      "045 10 3713\n",
      "045 12 0\n",
      "045 16 0\n",
      "045 17 9633\n",
      "045 22 0\n",
      "045 24 0\n",
      "045 26 0\n",
      "045 33 0\n",
      "045 45 0\n",
      "045 47 0\n",
      "045 49 0\n",
      "045 51 0\n",
      "045 53 0\n",
      "045 64 0\n",
      "045 65 4845\n",
      "045 78 0\n",
      "045 80 0\n",
      "045 83 0\n",
      "045 87 0\n",
      "045 90 0\n",
      "045 94 0\n",
      "045 96 0\n",
      "045 97 1824\n",
      "045 101 0\n",
      "045 111 0\n",
      "045 129 0\n",
      "045 130 4948\n",
      "045 132 0\n",
      "045 140 0\n",
      "执行时间: 10.684049129486084 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第6次训练\n",
      "045 0 0\n",
      "045 4 0\n",
      "045 9 0\n",
      "045 10 3775\n",
      "045 12 0\n",
      "045 16 0\n",
      "045 17 9632\n",
      "045 22 0\n",
      "045 24 0\n",
      "045 26 0\n",
      "045 33 0\n",
      "045 45 0\n",
      "045 47 0\n",
      "045 49 0\n",
      "045 51 0\n",
      "045 53 0\n",
      "045 64 0\n",
      "045 65 4829\n",
      "045 78 0\n",
      "045 80 0\n",
      "045 83 0\n",
      "045 87 0\n",
      "045 90 0\n",
      "045 94 0\n",
      "045 96 0\n",
      "045 97 1779\n",
      "045 101 0\n",
      "045 111 0\n",
      "045 129 0\n",
      "045 130 4930\n",
      "045 132 0\n",
      "045 140 0\n",
      "执行时间: 12.533880472183228 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第7次训练\n",
      "045 0 0\n",
      "045 4 0\n",
      "045 9 0\n",
      "045 10 3777\n",
      "045 12 0\n",
      "045 16 0\n",
      "045 17 9642\n",
      "045 22 0\n",
      "045 24 0\n",
      "045 26 0\n",
      "045 33 0\n",
      "045 45 0\n",
      "045 47 0\n",
      "045 49 0\n",
      "045 51 0\n",
      "045 53 0\n",
      "045 64 0\n",
      "045 65 4820\n",
      "045 78 0\n",
      "045 80 0\n",
      "045 83 0\n",
      "045 87 0\n",
      "045 90 0\n",
      "045 94 0\n",
      "045 96 0\n",
      "045 97 1840\n",
      "045 101 0\n",
      "045 111 0\n",
      "045 129 0\n",
      "045 130 4903\n",
      "045 132 0\n",
      "045 140 0\n",
      "执行时间: 14.373746633529663 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第8次训练\n",
      "045 0 0\n",
      "045 4 0\n",
      "045 9 0\n",
      "045 10 3768\n",
      "045 12 0\n",
      "045 16 0\n",
      "045 17 9671\n",
      "045 22 0\n",
      "045 24 0\n",
      "045 26 0\n",
      "045 33 0\n",
      "045 45 0\n",
      "045 47 0\n",
      "045 49 0\n",
      "045 51 0\n",
      "045 53 0\n",
      "045 64 0\n",
      "045 65 4835\n",
      "045 78 0\n",
      "045 80 0\n",
      "045 83 0\n",
      "045 87 0\n",
      "045 90 0\n",
      "045 94 0\n",
      "045 96 0\n",
      "045 97 1754\n",
      "045 101 0\n",
      "045 111 0\n",
      "045 129 0\n",
      "045 130 4966\n",
      "045 132 0\n",
      "045 140 0\n",
      "执行时间: 16.199292182922363 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第9次训练\n",
      "045 0 0\n",
      "045 4 0\n",
      "045 9 0\n",
      "045 10 3800\n",
      "045 12 0\n",
      "045 16 0\n",
      "045 17 9644\n",
      "045 22 0\n",
      "045 24 0\n",
      "045 26 0\n",
      "045 33 0\n",
      "045 45 0\n",
      "045 47 0\n",
      "045 49 0\n",
      "045 51 0\n",
      "045 53 0\n",
      "045 64 0\n",
      "045 65 4810\n",
      "045 78 0\n",
      "045 80 0\n",
      "045 83 0\n",
      "045 87 0\n",
      "045 90 0\n",
      "045 94 0\n",
      "045 96 0\n",
      "045 97 1849\n",
      "045 101 0\n",
      "045 111 0\n",
      "045 129 0\n",
      "045 130 4970\n",
      "045 132 0\n",
      "045 140 0\n",
      "执行时间: 17.953927755355835 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 10     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "action_types = order_same_action(Total_order, num_city, G)\n",
    "AGENT = {}\n",
    "for action_type, same_orders in action_types.items():\n",
    "    agent = magent.MultiAgentAC(\n",
    "        device = DEVICE,\n",
    "        VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "        ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "        NUM_CITIES = len(action_type), \n",
    "        HIDDEN_DIM = HIDDEN_DIM, \n",
    "        STATE_DIM = STATE_DIM,\n",
    "        action_key = action_type\n",
    "    )\n",
    "    AGENT[action_type] = agent\n",
    "\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        # 判断订单当前是否激活，并且赋值当前激活的订单给current_order\n",
    "        for agent in AGENT.values():\n",
    "            active_test(agent.action_key, agent, orders_unmatched)\n",
    "            get_multi_reward(agent)\n",
    "\n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            \n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            if last_vehicle:\n",
    "                for agent in AGENT.values():\n",
    "                    if agent.active:\n",
    "                        next_order_states = vectorization_order(agent.current_order)\n",
    "                        if agent.last_order:\n",
    "                            order_states = vectorization_order(agent.last_order)\n",
    "                        else:\n",
    "                            break\n",
    "                            # 过去time=0确保了order_states存在，现在需要第一次训练后才有\n",
    "                        # 改一下grid_reward\n",
    "                        \n",
    "                        agent.update(agent.v_states, order_states, agent.action, \n",
    "                                    agent.reward, next_vehicle_states, next_order_states , if_end)\n",
    "                    break\n",
    "            env.time = time\n",
    "\n",
    "        \n",
    "\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            if len(group[0]) != 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                # if greedy > greedy_epsilon:\n",
    "                greedy = False\n",
    "                vehicle_states = vectorization_vehicle(Vehicles)\n",
    "                # 这里也改了\n",
    "                \n",
    "                for agent in AGENT.values():\n",
    "                    if agent.active:\n",
    "                        agent.v_states = vehicle_states\n",
    "                        order_states = vectorization_order(agent.current_order)\n",
    "                        print(agent.action_key,time, agent.reward)\n",
    "                        agent.action= agent.take_action_skyrim(agent.v_states, order_states, explore, greedy)\n",
    "                        ACTIONS.append(agent.action) \n",
    "                    break\n",
    "                \n",
    "\n",
    "        \n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            grid_reward =   objval/1000\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 2], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0, 2], [0], [0], [0], [0, 2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n"
     ]
    }
   ],
   "source": [
    "print(ACTIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dirty_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
