{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带掩码的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为119282.0,58次未求解\n",
      "第1次训练\n",
      "执行时间: 1.8399901390075684 秒,50次未求解，当前强化学习值为262721.0,利润为262721.0\n",
      "第2次训练\n",
      "执行时间: 3.9947502613067627 秒,14次未求解，当前强化学习值为876163.0,利润为876163.0\n",
      "第3次训练\n",
      "执行时间: 5.620569705963135 秒,29次未求解，当前强化学习值为630302.0,利润为630302.0\n",
      "第4次训练\n",
      "执行时间: 8.040527582168579 秒,0次未求解，当前强化学习值为1137983.0,利润为1137983.0\n",
      "第5次训练\n",
      "执行时间: 9.552980184555054 秒,36次未求解，当前强化学习值为493615.0,利润为493615.0\n",
      "第6次训练\n",
      "执行时间: 11.84005880355835 秒,0次未求解，当前强化学习值为1119662.0,利润为1119662.0\n",
      "第7次训练\n",
      "执行时间: 13.321484804153442 秒,35次未求解，当前强化学习值为516251.0,利润为516251.0\n",
      "第8次训练\n",
      "执行时间: 15.030028820037842 秒,23次未求解，当前强化学习值为737416.0,利润为737416.0\n",
      "第9次训练\n",
      "执行时间: 17.870023250579834 秒,49次未求解，当前强化学习值为292569.0,利润为292569.0\n",
      "第10次训练\n",
      "执行时间: 20.277883291244507 秒,0次未求解，当前强化学习值为1122293.0,利润为1122293.0\n",
      "第11次训练\n",
      "执行时间: 21.34235954284668 秒,62次未求解，当前强化学习值为47739.0,利润为47739.0\n",
      "第12次训练\n",
      "执行时间: 23.63964319229126 秒,0次未求解，当前强化学习值为1129111.0,利润为1129111.0\n",
      "第13次训练\n",
      "执行时间: 24.700026035308838 秒,61次未求解，当前强化学习值为64000.0,利润为64000.0\n",
      "第14次训练\n",
      "执行时间: 25.829976558685303 秒,55次未求解，当前强化学习值为166136.0,利润为166136.0\n",
      "第15次训练\n",
      "执行时间: 26.759884119033813 秒,62次未求解，当前强化学习值为49557.0,利润为49557.0\n",
      "第16次训练\n",
      "执行时间: 27.697844743728638 秒,62次未求解，当前强化学习值为49557.0,利润为49557.0\n",
      "第17次训练\n",
      "执行时间: 29.32480239868164 秒,30次未求解，当前强化学习值为619153.0,利润为619153.0\n",
      "第18次训练\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action = agent.take_action_mask(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试版 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为333835.0,48次未求解\n",
      "第1次训练\n",
      "执行时间: 1.7765674591064453 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第2次训练\n",
      "执行时间: 2.8372347354888916 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第3次训练\n",
      "执行时间: 3.9000909328460693 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第4次训练\n",
      "执行时间: 4.9130539894104 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第5次训练\n",
      "执行时间: 5.973233938217163 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第6次训练\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.2500],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'actions' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m     greedy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    189\u001b[0m mask \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_mask(orders_unmatched)\n\u001b[1;32m--> 190\u001b[0m action , logits \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# reward = env.test_step(orders_unmatched,action)\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03mCOUNT = 1000\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03mmax_reward = -999999\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m    reward = env.test_step(orders_unmatched, max_action)\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:174\u001b[0m, in \u001b[0;36mMultiAgentAC.take_action_mask\u001b[1;34m(self, vehicle_states, order_states, mask, explore, greedy)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    172\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask 全 0，导致 softmax 计算无意义！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactions\u001b[49m , logits\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'actions' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 30\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0\n",
    "            explore = False\n",
    "            \n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            \"\"\"\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "            \"\"\"\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action , logits = agent.take_action_mask(vehicle_states, order_states, mask, explore, greedy)\n",
    "            # reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            # grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward +=  objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        # save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        # torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试版2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "未加强化学习利润为200830.0,53次未求解\n",
      "第1次训练\n",
      "v_encoded 出现 NaN，输入状态可能异常！\n",
      "o_encoded 出现 NaN，输入状态可能异常！\n",
      "actor_input 出现 NaN，输入状态可能异常！\n",
      "global vehicle 出现 NaN，输入状态可能异常！\n",
      "repeated global 出现 NaN，输入状态可能异常！\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward0>)\n",
      "logits 出现 NaN，输入状态可能异常！\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'actions' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 187\u001b[0m\n\u001b[0;32m    185\u001b[0m     greedy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    186\u001b[0m mask \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_mask(orders_unmatched)\n\u001b[1;32m--> 187\u001b[0m action, logits \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action_mask_special\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mtest_step(orders_unmatched,action)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mCOUNT = 1000\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03mmax_reward = -999999\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    reward = env.test_step(orders_unmatched, max_action)\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:192\u001b[0m, in \u001b[0;36mMultiAgentAC.take_action_mask_special\u001b[1;34m(self, vehicle_states, order_states, mask, explore, greedy)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask 全 0，导致 softmax 计算无意义！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactions\u001b[49m , logits\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'actions' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update_logtis(vehicle_states, order_states, logits,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action, logits = agent.take_action_mask_special(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例1 burn in后探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例2 burn in后不探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例3 更新条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带跳过的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为351240.0,48次未求解\n",
      "第1次训练\n",
      "执行时间: 1.5951976776123047 秒,66次未求解，当前强化学习值为-316088.0,利润为-1388.0\n",
      "第2次训练\n",
      "执行时间: 2.8220858573913574 秒,40次未求解，当前强化学习值为255049.0,利润为471649.0\n",
      "第3次训练\n",
      "执行时间: 3.92783522605896 秒,48次未求解，当前强化学习值为76507.0,利润为333107.0\n",
      "第4次训练\n",
      "执行时间: 4.801097393035889 秒,60次未求解，当前强化学习值为-187197.0,利润为108603.0\n",
      "第5次训练\n",
      "执行时间: 6.016491651535034 秒,45次未求解，当前强化学习值为168199.0,利润为405999.0\n",
      "第6次训练\n",
      "执行时间: 6.894177436828613 秒,60次未求解，当前强化学习值为-183913.0,利润为112087.0\n",
      "第7次训练\n",
      "执行时间: 7.8495588302612305 秒,60次未求解，当前强化学习值为-186198.0,利润为108602.0\n",
      "第8次训练\n",
      "执行时间: 9.370670318603516 秒,43次未求解，当前强化学习值为195294.0,利润为423494.0\n",
      "第9次训练\n",
      "执行时间: 10.391007423400879 秒,59次未求解，当前强化学习值为-181762.0,利润为112638.0\n",
      "第10次训练\n",
      "执行时间: 11.553189754486084 秒,57次未求解，当前强化学习值为-102270.0,利润为178130.0\n",
      "第11次训练\n",
      "执行时间: 13.124795198440552 秒,47次未求解，当前强化学习值为114564.0,利润为361264.0\n",
      "第12次训练\n",
      "执行时间: 14.306801080703735 秒,60次未求解，当前强化学习值为-180413.0,利润为112087.0\n",
      "第13次训练\n",
      "执行时间: 15.57046389579773 秒,48次未求解，当前强化学习值为68873.0,利润为326673.0\n",
      "第14次训练\n",
      "执行时间: 16.64451813697815 秒,57次未求解，当前强化学习值为-110057.0,利润为170643.0\n",
      "第15次训练\n",
      "执行时间: 17.640617847442627 秒,59次未求解，当前强化学习值为-159380.0,利润为131420.0\n",
      "第16次训练\n",
      "执行时间: 18.647897958755493 秒,59次未求解，当前强化学习值为-168057.0,利润为127243.0\n",
      "第17次训练\n",
      "执行时间: 20.608492374420166 秒,28次未求解，当前强化学习值为534251.0,利润为696851.0\n",
      "第18次训练\n",
      "执行时间: 21.51712131500244 秒,60次未求解，当前强化学习值为-190784.0,利润为105716.0\n",
      "第19次训练\n",
      "执行时间: 22.778335571289062 秒,48次未求解，当前强化学习值为80027.0,利润为333427.0\n",
      "第20次训练\n",
      "执行时间: 23.744807720184326 秒,60次未求解，当前强化学习值为-184613.0,利润为112087.0\n",
      "第21次训练\n",
      "执行时间: 24.99462080001831 秒,51次未求解，当前强化学习值为23549.0,利润为288049.0\n",
      "第22次训练\n",
      "执行时间: 25.945756196975708 秒,60次未求解，当前强化学习值为-187566.0,利润为107234.0\n",
      "第23次训练\n",
      "执行时间: 27.99422001838684 秒,23次未求解，当前强化学习值为649226.0,利润为800126.0\n",
      "第24次训练\n",
      "执行时间: 29.752153396606445 秒,32次未求解，当前强化学习值为436038.0,利润为620738.0\n",
      "第25次训练\n",
      "执行时间: 31.028327465057373 秒,47次未求解，当前强化学习值为129301.0,利润为373001.0\n",
      "第26次训练\n",
      "执行时间: 32.439204454422 秒,45次未求解，当前强化学习值为158061.0,利润为394861.0\n",
      "第27次训练\n",
      "执行时间: 33.439168214797974 秒,57次未求解，当前强化学习值为-118950.0,利润为164950.0\n",
      "第28次训练\n",
      "执行时间: 34.552332639694214 秒,54次未求解，当前强化学习值为-59392.0,利润为217108.0\n",
      "第29次训练\n",
      "执行时间: 35.700438261032104 秒,54次未求解，当前强化学习值为-63310.0,利润为215190.0\n",
      "第30次训练\n",
      "执行时间: 36.633397817611694 秒,60次未求解，当前强化学习值为-187284.0,利润为105716.0\n",
      "第31次训练\n",
      "执行时间: 38.67721676826477 秒,22次未求解，当前强化学习值为680635.0,利润为824735.0\n",
      "第32次训练\n",
      "执行时间: 39.634028673172 秒,60次未求解，当前强化学习值为-187925.0,利润为108375.0\n",
      "第33次训练\n",
      "执行时间: 41.42440843582153 秒,35次未求解，当前强化学习值为369561.0,利润为567161.0\n",
      "第34次训练\n",
      "执行时间: 42.37771916389465 秒,60次未求解，当前强化学习值为-193384.0,利润为105716.0\n",
      "第35次训练\n",
      "执行时间: 43.3869833946228 秒,58次未求解，当前强化学习值为-129827.0,利润为155773.0\n",
      "第36次训练\n",
      "执行时间: 45.71844553947449 秒,11次未求解，当前强化学习值为902113.0,利润为1014313.0\n",
      "第37次训练\n",
      "执行时间: 46.916133403778076 秒,50次未求解，当前强化学习值为43715.0,利润为306115.0\n",
      "第38次训练\n",
      "执行时间: 48.783586502075195 秒,28次未求解，当前强化学习值为548164.0,利润为711764.0\n",
      "第39次训练\n",
      "执行时间: 49.75050187110901 秒,60次未求解，当前强化学习值为-191724.0,利润为108376.0\n",
      "第40次训练\n",
      "执行时间: 51.13849878311157 秒,43次未求解，当前强化学习值为195591.0,利润为421091.0\n",
      "第41次训练\n",
      "执行时间: 52.136146783828735 秒,57次未求解，当前强化学习值为-116542.0,利润为164658.0\n",
      "第42次训练\n",
      "执行时间: 53.34069466590881 秒,51次未求解，当前强化学习值为22358.0,利润为286358.0\n",
      "第43次训练\n",
      "执行时间: 54.45970129966736 秒,53次未求解，当前强化学习值为-46247.0,利润为229853.0\n",
      "第44次训练\n",
      "执行时间: 55.403913497924805 秒,60次未求解，当前强化学习值为-184125.0,利润为108375.0\n",
      "第45次训练\n",
      "执行时间: 56.32914471626282 秒,60次未求解，当前强化学习值为-194084.0,利润为105716.0\n",
      "第46次训练\n",
      "执行时间: 57.52802777290344 秒,50次未求解，当前强化学习值为22843.0,利润为285343.0\n",
      "第47次训练\n",
      "执行时间: 59.19414758682251 秒,34次未求解，当前强化学习值为410426.0,利润为602826.0\n",
      "第48次训练\n",
      "执行时间: 60.40188717842102 秒,50次未求解，当前强化学习值为54377.0,利润为313777.0\n",
      "第49次训练\n",
      "执行时间: 62.1056182384491 秒,36次未求解，当前强化学习值为353559.0,利润为554559.0\n",
      "第50次训练\n",
      "执行时间: 63.049800157547 秒,60次未求解，当前强化学习值为-187625.0,利润为108375.0\n",
      "第51次训练\n",
      "执行时间: 64.5289478302002 秒,41次未求解，当前强化学习值为236401.0,利润为457601.0\n",
      "第52次训练\n",
      "执行时间: 65.51738786697388 秒,58次未求解，当前强化学习值为-135383.0,利润为147517.0\n",
      "第53次训练\n",
      "执行时间: 67.07062864303589 秒,38次未求解，当前强化学习值为285800.0,利润为499800.0\n",
      "第54次训练\n",
      "执行时间: 68.92661738395691 秒,31次未求解，当前强化学习值为477191.0,利润为654391.0\n",
      "第55次训练\n",
      "执行时间: 70.24348044395447 秒,48次未求解，当前强化学习值为86877.0,利润为339677.0\n",
      "第56次训练\n",
      "执行时间: 71.25049686431885 秒,60次未求解，当前强化学习值为-189797.0,利润为108603.0\n",
      "第57次训练\n",
      "执行时间: 72.19648313522339 秒,60次未求解，当前强化学习值为-186590.0,利润为112310.0\n",
      "第58次训练\n",
      "执行时间: 73.2587080001831 秒,55次未求解，当前强化学习值为-77222.0,利润为201478.0\n",
      "第59次训练\n",
      "执行时间: 74.27636647224426 秒,58次未求解，当前强化学习值为-131425.0,利润为155775.0\n",
      "第60次训练\n",
      "执行时间: 75.9955050945282 秒,32次未求解，当前强化学习值为424336.0,利润为609836.0\n",
      "第61次训练\n",
      "执行时间: 77.16693782806396 秒,54次未求解，当前强化学习值为-50641.0,利润为226659.0\n",
      "第62次训练\n",
      "执行时间: 78.11392188072205 秒,60次未求解，当前强化学习值为-188324.0,利润为108376.0\n",
      "第63次训练\n",
      "执行时间: 79.20863389968872 秒,57次未求解，当前强化学习值为-113257.0,利润为170643.0\n",
      "第64次训练\n",
      "执行时间: 80.21722507476807 秒,58次未求解，当前强化学习值为-134936.0,利润为150664.0\n",
      "第65次训练\n",
      "执行时间: 81.21145439147949 秒,58次未求解，当前强化学习值为-133954.0,利润为149646.0\n",
      "第66次训练\n",
      "执行时间: 82.88465523719788 秒,35次未求解，当前强化学习值为388026.0,利润为581726.0\n",
      "第67次训练\n",
      "执行时间: 83.94495129585266 秒,55次未求解，当前强化学习值为-82790.0,利润为194710.0\n",
      "第68次训练\n",
      "执行时间: 84.91909193992615 秒,60次未求解，当前强化学习值为-189397.0,利润为108603.0\n",
      "第69次训练\n",
      "执行时间: 86.38045382499695 秒,41次未求解，当前强化学习值为235517.0,利润为455017.0\n",
      "第70次训练\n",
      "执行时间: 87.39861249923706 秒,60次未求解，当前强化学习值为-183013.0,利润为112087.0\n",
      "第71次训练\n",
      "执行时间: 88.33405995368958 秒,59次未求解，当前强化学习值为-181862.0,利润为112638.0\n",
      "第72次训练\n",
      "执行时间: 89.58133697509766 秒,51次未求解，当前强化学习值为19955.0,利润为288055.0\n",
      "第73次训练\n",
      "执行时间: 91.56137347221375 秒,23次未求解，当前强化学习值为667000.0,利润为816100.0\n",
      "第74次训练\n",
      "执行时间: 92.61986470222473 秒,58次未求解，当前强化学习值为-135307.0,利润为152993.0\n",
      "第75次训练\n",
      "执行时间: 93.57529735565186 秒,60次未求解，当前强化学习值为-189684.0,利润为105716.0\n",
      "第76次训练\n",
      "执行时间: 94.529616355896 秒,60次未求解，当前强化学习值为-188425.0,利润为108375.0\n",
      "第77次训练\n",
      "执行时间: 95.49549078941345 秒,60次未求解，当前强化学习值为-187397.0,利润为108603.0\n",
      "第78次训练\n",
      "执行时间: 96.7206883430481 秒,47次未求解，当前强化学习值为87333.0,利润为338433.0\n",
      "第79次训练\n",
      "执行时间: 97.80291867256165 秒,55次未求解，当前强化学习值为-72993.0,利润为206507.0\n",
      "第80次训练\n",
      "执行时间: 98.76064229011536 秒,60次未求解，当前强化学习值为-185213.0,利润为112087.0\n",
      "第81次训练\n",
      "执行时间: 99.71186685562134 秒,60次未求解，当前强化学习值为-188866.0,利润为107834.0\n",
      "第82次训练\n",
      "执行时间: 101.1367130279541 秒,44次未求解，当前强化学习值为168940.0,利润为403940.0\n",
      "第83次训练\n",
      "执行时间: 102.72883558273315 秒,37次未求解，当前强化学习值为302257.0,利润为513357.0\n",
      "第84次训练\n",
      "执行时间: 103.68984532356262 秒,60次未求解，当前强化学习值为-185397.0,利润为108603.0\n",
      "第85次训练\n",
      "执行时间: 104.7415132522583 秒,60次未求解，当前强化学习值为-187524.0,利润为108376.0\n",
      "第86次训练\n",
      "执行时间: 105.96233081817627 秒,51次未求解，当前强化学习值为35023.0,利润为293323.0\n",
      "第87次训练\n",
      "执行时间: 106.89941883087158 秒,60次未求解，当前强化学习值为-185897.0,利润为108603.0\n",
      "第88次训练\n",
      "执行时间: 107.8716242313385 秒,60次未求解，当前强化学习值为-186790.0,利润为112310.0\n",
      "第89次训练\n",
      "执行时间: 108.86753177642822 秒,60次未求解，当前强化学习值为-189598.0,利润为108602.0\n",
      "第90次训练\n",
      "执行时间: 110.29196691513062 秒,43次未求解，当前强化学习值为176044.0,利润为412544.0\n",
      "第91次训练\n",
      "执行时间: 111.2357029914856 秒,60次未求解，当前强化学习值为-187525.0,利润为108375.0\n",
      "第92次训练\n",
      "执行时间: 112.51746869087219 秒,50次未求解，当前强化学习值为55062.0,利润为308862.0\n",
      "第93次训练\n",
      "执行时间: 113.96824717521667 秒,43次未求解，当前强化学习值为179440.0,利润为413340.0\n",
      "第94次训练\n",
      "执行时间: 115.20708656311035 秒,54次未求解，当前强化学习值为-46443.0,利润为228857.0\n",
      "第95次训练\n",
      "执行时间: 116.19106864929199 秒,60次未求解，当前强化学习值为-191197.0,利润为108603.0\n",
      "第96次训练\n",
      "执行时间: 117.2844717502594 秒,54次未求解，当前强化学习值为-49093.0,利润为226707.0\n",
      "第97次训练\n",
      "执行时间: 118.26419067382812 秒,60次未求解，当前强化学习值为-188684.0,利润为105716.0\n",
      "第98次训练\n",
      "执行时间: 119.45181608200073 秒,51次未求解，当前强化学习值为25755.0,利润为288455.0\n",
      "第99次训练\n",
      "执行时间: 120.3560893535614 秒,60次未求解，当前强化学习值为-184166.0,利润为107834.0\n",
      "第100次训练\n",
      "最优模型为35\n",
      "执行时间: 121.3204836845398 秒,60次未求解，当前强化学习值为-196920.0,利润为104480.0\n",
      "第101次训练\n",
      "最优模型为35\n",
      "执行时间: 122.23788475990295 秒,60次未求解，当前强化学习值为-192184.0,利润为105716.0\n",
      "第102次训练\n",
      "最优模型为35\n",
      "执行时间: 123.19744944572449 秒,60次未求解，当前强化学习值为-185214.0,利润为112086.0\n",
      "第103次训练\n",
      "最优模型为35\n",
      "执行时间: 124.13381385803223 秒,60次未求解，当前强化学习值为-191384.0,利润为105716.0\n",
      "第104次训练\n",
      "最优模型为35\n",
      "执行时间: 125.08623027801514 秒,60次未求解，当前强化学习值为-186925.0,利润为108375.0\n",
      "第105次训练\n",
      "最优模型为35\n",
      "执行时间: 126.30799269676208 秒,48次未求解，当前强化学习值为79958.0,利润为334558.0\n",
      "第106次训练\n",
      "最优模型为35\n",
      "执行时间: 128.32657599449158 秒,23次未求解，当前强化学习值为639598.0,利润为791798.0\n",
      "第107次训练\n",
      "最优模型为35\n",
      "执行时间: 129.66857504844666 秒,47次未求解，当前强化学习值为98970.99993335613,利润为357870.9999333561\n",
      "第108次训练\n",
      "最优模型为35\n",
      "执行时间: 130.82310009002686 秒,58次未求解，当前强化学习值为-143817.0,利润为146083.0\n",
      "第109次训练\n",
      "最优模型为35\n",
      "执行时间: 131.8901650905609 秒,58次未求解，当前强化学习值为-136629.0,利润为152071.0\n",
      "第110次训练\n",
      "最优模型为35\n",
      "执行时间: 134.40908408164978 秒,7次未求解，当前强化学习值为983056.0,利润为1087856.0\n",
      "第111次训练\n",
      "最优模型为35\n",
      "执行时间: 135.56054759025574 秒,58次未求解，当前强化学习值为-137837.0,利润为150663.0\n",
      "第112次训练\n",
      "最优模型为35\n",
      "执行时间: 136.64506363868713 秒,60次未求解，当前强化学习值为-186825.0,利润为108375.0\n",
      "第113次训练\n",
      "最优模型为35\n",
      "执行时间: 137.77993845939636 秒,54次未求解，当前强化学习值为-60230.0,利润为219370.0\n",
      "第114次训练\n",
      "最优模型为35\n",
      "执行时间: 139.00611233711243 秒,59次未求解，当前强化学习值为-165156.0,利润为127244.0\n",
      "第115次训练\n",
      "最优模型为35\n",
      "执行时间: 140.6410892009735 秒,48次未求解，当前强化学习值为86041.0,利润为339341.0\n",
      "第116次训练\n",
      "最优模型为35\n",
      "执行时间: 142.40051746368408 秒,35次未求解，当前强化学习值为376823.0,利润为571523.0\n",
      "第117次训练\n",
      "最优模型为35\n",
      "执行时间: 143.57704281806946 秒,53次未求解，当前强化学习值为-48617.0,利润为224883.0\n",
      "第118次训练\n",
      "最优模型为35\n",
      "执行时间: 145.75464940071106 秒,21次未求解，当前强化学习值为670918.0,利润为821018.0\n",
      "第119次训练\n",
      "最优模型为35\n",
      "执行时间: 147.02079129219055 秒,50次未求解，当前强化学习值为50265.0,利润为305165.0\n",
      "第120次训练\n",
      "最优模型为35\n",
      "执行时间: 148.03165912628174 秒,60次未求解，当前强化学习值为-191624.0,利润为108376.0\n",
      "第121次训练\n",
      "最优模型为35\n",
      "执行时间: 149.89130783081055 秒,27次未求解，当前强化学习值为552953.0,利润为719653.0\n",
      "第122次训练\n",
      "最优模型为35\n",
      "执行时间: 151.05049443244934 秒,45次未求解，当前强化学习值为160090.0,利润为396590.0\n",
      "第123次训练\n",
      "最优模型为35\n",
      "执行时间: 151.98367047309875 秒,60次未求解，当前强化学习值为-191197.0,利润为108603.0\n",
      "第124次训练\n",
      "最优模型为35\n",
      "执行时间: 152.91549134254456 秒,58次未求解，当前强化学习值为-134437.0,利润为150663.0\n",
      "第125次训练\n",
      "最优模型为35\n",
      "执行时间: 154.26281690597534 秒,38次未求解，当前强化学习值为302843.0,利润为512143.0\n",
      "第126次训练\n",
      "最优模型为35\n",
      "执行时间: 155.22281908988953 秒,58次未求解，当前强化学习值为-136437.0,利润为150663.0\n",
      "第127次训练\n",
      "最优模型为35\n",
      "执行时间: 156.25933933258057 秒,52次未求解，当前强化学习值为-13601.0,利润为256899.0\n",
      "第128次训练\n",
      "最优模型为35\n",
      "执行时间: 157.1459345817566 秒,58次未求解，当前强化学习值为-143435.0,利润为143665.0\n",
      "第129次训练\n",
      "最优模型为35\n",
      "执行时间: 158.03922820091248 秒,60次未求解，当前强化学习值为-187925.0,利润为108375.0\n",
      "第130次训练\n",
      "最优模型为35\n",
      "执行时间: 158.92097401618958 秒,60次未求解，当前强化学习值为-193284.0,利润为105716.0\n",
      "第131次训练\n",
      "最优模型为35\n",
      "执行时间: 159.84232568740845 秒,60次未求解，当前强化学习值为-185489.0,利润为112311.0\n",
      "第132次训练\n",
      "最优模型为35\n",
      "执行时间: 160.9287087917328 秒,54次未求解，当前强化学习值为-55947.0,利润为224053.0\n",
      "第133次训练\n",
      "最优模型为35\n",
      "执行时间: 162.09905314445496 秒,51次未求解，当前强化学习值为27708.0,利润为290308.0\n",
      "第134次训练\n",
      "最优模型为35\n",
      "执行时间: 163.93159818649292 秒,18次未求解，当前强化学习值为766780.0,利润为900680.0\n",
      "第135次训练\n",
      "最优模型为35\n",
      "执行时间: 164.9489631652832 秒,60次未求解，当前强化学习值为-187066.0,利润为107834.0\n",
      "第136次训练\n",
      "最优模型为35\n",
      "执行时间: 166.57515454292297 秒,37次未求解，当前强化学习值为310584.0,利润为519484.0\n",
      "第137次训练\n",
      "最优模型为35\n",
      "执行时间: 167.5441632270813 秒,60次未求解，当前强化学习值为-183913.0,利润为112087.0\n",
      "第138次训练\n",
      "最优模型为35\n",
      "执行时间: 168.517085313797 秒,60次未求解，当前强化学习值为-181515.0,利润为112085.0\n",
      "第139次训练\n",
      "最优模型为35\n",
      "执行时间: 169.47752785682678 秒,60次未求解，当前强化学习值为-189025.0,利润为108375.0\n",
      "第140次训练\n",
      "最优模型为35\n",
      "执行时间: 170.4704098701477 秒,58次未求解，当前强化学习值为-145145.0,利润为143955.0\n",
      "第141次训练\n",
      "最优模型为35\n",
      "执行时间: 171.61494302749634 秒,57次未求解，当前强化学习值为-123261.0,利润为163339.0\n",
      "第142次训练\n",
      "最优模型为35\n",
      "执行时间: 172.6714940071106 秒,56次未求解，当前强化学习值为-82394.0,利润为196206.0\n",
      "第143次训练\n",
      "最优模型为35\n",
      "执行时间: 174.24093580245972 秒,40次未求解，当前强化学习值为265885.0,利润为482185.0\n",
      "第144次训练\n",
      "最优模型为35\n",
      "执行时间: 175.2280457019806 秒,59次未求解，当前强化学习值为-164656.0,利润为127244.0\n",
      "第145次训练\n",
      "最优模型为35\n",
      "执行时间: 177.02452993392944 秒,30次未求解，当前强化学习值为494887.0,利润为671987.0\n",
      "第146次训练\n",
      "最优模型为35\n",
      "执行时间: 178.81011962890625 秒,32次未求解，当前强化学习值为454655.0,利润为637055.0\n",
      "第147次训练\n",
      "最优模型为35\n",
      "执行时间: 179.76635479927063 秒,60次未求解，当前强化学习值为-184214.0,利润为112086.0\n",
      "第148次训练\n",
      "最优模型为35\n",
      "执行时间: 180.75130462646484 秒,60次未求解，当前强化学习值为-184889.0,利润为112311.0\n",
      "第149次训练\n",
      "最优模型为35\n",
      "执行时间: 181.89295649528503 秒,51次未求解，当前强化学习值为17677.0,利润为282877.0\n",
      "第150次训练\n",
      "最优模型为35\n",
      "执行时间: 182.9985318183899 秒,54次未求解，当前强化学习值为-56781.0,利润为219019.0\n",
      "第151次训练\n",
      "最优模型为35\n",
      "执行时间: 184.2939157485962 秒,48次未求解，当前强化学习值为107515.0,利润为354815.0\n",
      "第152次训练\n",
      "最优模型为35\n",
      "执行时间: 185.66520404815674 秒,43次未求解，当前强化学习值为196426.0,利润为428626.0\n",
      "第153次训练\n",
      "最优模型为35\n",
      "执行时间: 186.6812460422516 秒,58次未求解，当前强化学习值为-137489.0,利润为147511.0\n",
      "第154次训练\n",
      "最优模型为35\n",
      "执行时间: 187.61258220672607 秒,60次未求解，当前强化学习值为-192720.0,利润为104480.0\n",
      "第155次训练\n",
      "最优模型为35\n",
      "执行时间: 188.56989240646362 秒,60次未求解，当前强化学习值为-195420.0,利润为104480.0\n",
      "第156次训练\n",
      "最优模型为35\n",
      "执行时间: 189.5723340511322 秒,58次未求解，当前强化学习值为-135482.0,利润为147518.0\n",
      "第157次训练\n",
      "最优模型为35\n",
      "执行时间: 190.57703518867493 秒,59次未求解，当前强化学习值为-162056.0,利润为127244.0\n",
      "第158次训练\n",
      "最优模型为35\n",
      "执行时间: 191.57029938697815 秒,58次未求解，当前强化学习值为-131497.0,利润为151303.0\n",
      "第159次训练\n",
      "最优模型为35\n",
      "执行时间: 193.18104481697083 秒,37次未求解，当前强化学习值为314272.0,利润为528572.0\n",
      "第160次训练\n",
      "最优模型为35\n",
      "执行时间: 194.1245415210724 秒,60次未求解，当前强化学习值为-192543.0,利润为104257.0\n",
      "第161次训练\n",
      "最优模型为35\n",
      "执行时间: 196.5143163204193 秒,9次未求解，当前强化学习值为990179.0,利润为1085879.0\n",
      "第162次训练\n",
      "最优模型为35\n",
      "执行时间: 197.64509797096252 秒,58次未求解，当前强化学习值为-129827.0,利润为155773.0\n",
      "第163次训练\n",
      "最优模型为35\n",
      "执行时间: 198.55867409706116 秒,60次未求解，当前强化学习值为-183925.0,利润为108375.0\n",
      "第164次训练\n",
      "最优模型为35\n",
      "执行时间: 199.6782088279724 秒,58次未求解，当前强化学习值为-146245.0,利润为143955.0\n",
      "第165次训练\n",
      "最优模型为35\n",
      "执行时间: 200.66531991958618 秒,59次未求解，当前强化学习值为-166653.0,利润为127247.0\n",
      "第166次训练\n",
      "最优模型为35\n",
      "执行时间: 201.62729954719543 秒,60次未求解，当前强化学习值为-186589.0,利润为112311.0\n",
      "第167次训练\n",
      "最优模型为35\n",
      "执行时间: 202.84313583374023 秒,51次未求解，当前强化学习值为36776.0,利润为297976.0\n",
      "第168次训练\n",
      "最优模型为35\n",
      "执行时间: 203.75165796279907 秒,60次未求解，当前强化学习值为-185115.0,利润为112085.0\n",
      "第169次训练\n",
      "最优模型为35\n",
      "执行时间: 205.69684290885925 秒,6次未求解，当前强化学习值为1047551.0,利润为1136751.0\n",
      "第170次训练\n",
      "最优模型为35\n",
      "执行时间: 206.5575942993164 秒,60次未求解，当前强化学习值为-188198.0,利润为108602.0\n",
      "第171次训练\n",
      "最优模型为35\n",
      "执行时间: 207.49178290367126 秒,58次未求解，当前强化学习值为-148817.0,利润为146083.0\n",
      "第172次训练\n",
      "最优模型为35\n",
      "执行时间: 208.49127411842346 秒,60次未求解，当前强化学习值为-189425.0,利润为108375.0\n",
      "第173次训练\n",
      "最优模型为35\n",
      "执行时间: 209.3841781616211 秒,58次未求解，当前强化学习值为-139403.0,利润为152997.0\n",
      "第174次训练\n",
      "最优模型为35\n",
      "执行时间: 210.64575910568237 秒,43次未求解，当前强化学习值为169834.0,利润为408134.0\n",
      "第175次训练\n",
      "最优模型为35\n",
      "执行时间: 211.51969575881958 秒,60次未求解，当前强化学习值为-193766.0,利润为107234.0\n",
      "第176次训练\n",
      "最优模型为35\n",
      "执行时间: 212.39733839035034 秒,58次未求解，当前强化学习值为-138903.0,利润为145397.0\n",
      "第177次训练\n",
      "最优模型为35\n",
      "执行时间: 213.3943166732788 秒,54次未求解，当前强化学习值为-56505.0,利润为221195.0\n",
      "第178次训练\n",
      "最优模型为35\n",
      "执行时间: 214.3564896583557 秒,54次未求解，当前强化学习值为-58473.0,利润为222027.0\n",
      "第179次训练\n",
      "最优模型为35\n",
      "执行时间: 215.46778202056885 秒,45次未求解，当前强化学习值为152563.0,利润为390763.0\n",
      "第180次训练\n",
      "最优模型为35\n",
      "执行时间: 216.35304832458496 秒,60次未求解，当前强化学习值为-190566.0,利润为107834.0\n",
      "第181次训练\n",
      "最优模型为35\n",
      "执行时间: 217.1901683807373 秒,60次未求解，当前强化学习值为-189566.0,利润为107834.0\n",
      "第182次训练\n",
      "最优模型为35\n",
      "执行时间: 218.28790140151978 秒,47次未求解，当前强化学习值为102841.0,利润为351441.0\n",
      "第183次训练\n",
      "最优模型为35\n",
      "执行时间: 219.1589617729187 秒,60次未求解，当前强化学习值为-186188.0,利润为112312.0\n",
      "第184次训练\n",
      "最优模型为35\n",
      "执行时间: 220.04464960098267 秒,58次未求解，当前强化学习值为-137583.0,利润为149517.0\n",
      "第185次训练\n",
      "最优模型为35\n",
      "执行时间: 220.87177443504333 秒,60次未求解，当前强化学习值为-185797.0,利润为108603.0\n",
      "第186次训练\n",
      "最优模型为35\n",
      "执行时间: 221.76338267326355 秒,58次未求解，当前强化学习值为-136153.0,利润为149647.0\n",
      "第187次训练\n",
      "最优模型为35\n",
      "执行时间: 222.57552814483643 秒,60次未求解，当前强化学习值为-195184.0,利润为105716.0\n",
      "第188次训练\n",
      "最优模型为35\n",
      "执行时间: 223.44258451461792 秒,59次未求解，当前强化学习值为-161479.0,利润为129321.0\n",
      "第189次训练\n",
      "最优模型为35\n",
      "执行时间: 224.59621119499207 秒,43次未求解，当前强化学习值为178706.0,利润为416006.0\n",
      "第190次训练\n",
      "最优模型为35\n",
      "执行时间: 225.45598483085632 秒,60次未求解，当前强化学习值为-192425.0,利润为108375.0\n",
      "第191次训练\n",
      "最优模型为35\n",
      "执行时间: 226.28961968421936 秒,60次未求解，当前强化学习值为-189966.0,利润为107834.0\n",
      "第192次训练\n",
      "最优模型为35\n",
      "执行时间: 228.23953676223755 秒,31次未求解，当前强化学习值为482222.0,利润为660722.0\n",
      "第193次训练\n",
      "最优模型为35\n",
      "执行时间: 229.31199431419373 秒,59次未求解，当前强化学习值为-165180.0,利润为129320.0\n",
      "第194次训练\n",
      "最优模型为35\n",
      "执行时间: 230.2102234363556 秒,60次未求解，当前强化学习值为-182688.0,利润为112312.0\n",
      "第195次训练\n",
      "最优模型为35\n",
      "执行时间: 231.1345226764679 秒,60次未求解，当前强化学习值为-181087.0,利润为112313.0\n",
      "第196次训练\n",
      "最优模型为35\n",
      "执行时间: 232.0563771724701 秒,60次未求解，当前强化学习值为-192484.0,利润为105716.0\n",
      "第197次训练\n",
      "最优模型为35\n",
      "执行时间: 233.13523602485657 秒,51次未求解，当前强化学习值为38378.0,利润为297678.0\n",
      "第198次训练\n",
      "最优模型为35\n",
      "执行时间: 233.98434162139893 秒,60次未求解，当前强化学习值为-191384.0,利润为105716.0\n",
      "第199次训练\n",
      "最优模型为35\n",
      "执行时间: 235.4276783466339 秒,34次未求解，当前强化学习值为398089.0,利润为596089.0\n",
      "第200次训练\n",
      "最优模型为35\n",
      "执行时间: 236.49652671813965 秒,51次未求解，当前强化学习值为42467.0,利润为296067.0\n",
      "第201次训练\n",
      "最优模型为35\n",
      "执行时间: 237.3912591934204 秒,58次未求解，当前强化学习值为-137200.0,利润为151300.0\n",
      "第202次训练\n",
      "最优模型为35\n",
      "执行时间: 238.45400261878967 秒,52次未求解，当前强化学习值为-18412.0,利润为255788.0\n",
      "第203次训练\n",
      "最优模型为35\n",
      "执行时间: 239.55296993255615 秒,47次未求解，当前强化学习值为117494.0,利润为364394.0\n",
      "第204次训练\n",
      "最优模型为35\n",
      "执行时间: 240.5828664302826 秒,51次未求解，当前强化学习值为9457.0,利润为275757.0\n",
      "第205次训练\n",
      "最优模型为35\n",
      "执行时间: 241.49330592155457 秒,60次未求解，当前强化学习值为-190498.0,利润为108602.0\n",
      "第206次训练\n",
      "最优模型为35\n",
      "执行时间: 242.33976364135742 秒,60次未求解，当前强化学习值为-183399.0,利润为108601.0\n",
      "第207次训练\n",
      "最优模型为35\n",
      "执行时间: 243.2067174911499 秒,60次未求解，当前强化学习值为-191443.0,利润为104257.0\n",
      "第208次训练\n",
      "最优模型为35\n",
      "执行时间: 244.13494753837585 秒,59次未求解，当前强化学习值为-161679.0,利润为129321.0\n",
      "第209次训练\n",
      "最优模型为35\n",
      "执行时间: 245.13988399505615 秒,51次未求解，当前强化学习值为23954.0,利润为286754.0\n",
      "第210次训练\n",
      "最优模型为35\n",
      "执行时间: 246.03105187416077 秒,60次未求解，当前强化学习值为-183013.0,利润为112087.0\n",
      "第211次训练\n",
      "最优模型为35\n",
      "执行时间: 247.0749294757843 秒,48次未求解，当前强化学习值为76701.0,利润为330601.0\n",
      "第212次训练\n",
      "最优模型为35\n",
      "执行时间: 248.1468517780304 秒,49次未求解，当前强化学习值为59010.0,利润为318010.0\n",
      "第213次训练\n",
      "最优模型为35\n",
      "执行时间: 249.4684054851532 秒,41次未求解，当前强化学习值为202808.0,利润为437508.0\n",
      "第214次训练\n",
      "最优模型为35\n",
      "执行时间: 250.35275077819824 秒,60次未求解，当前强化学习值为-189198.0,利润为108602.0\n",
      "第215次训练\n",
      "最优模型为35\n",
      "执行时间: 252.0311381816864 秒,24次未求解，当前强化学习值为648412.0,利润为803012.0\n",
      "第216次训练\n",
      "最优模型为35\n",
      "执行时间: 253.1603446006775 秒,51次未求解，当前强化学习值为48759.0,利润为307659.0\n",
      "第217次训练\n",
      "最优模型为35\n",
      "执行时间: 254.5544149875641 秒,40次未求解，当前强化学习值为270620.0,利润为488320.0\n",
      "第218次训练\n",
      "最优模型为35\n",
      "执行时间: 255.55295395851135 秒,59次未求解，当前强化学习值为-163656.0,利润为127244.0\n",
      "第219次训练\n",
      "最优模型为35\n",
      "执行时间: 256.5672302246094 秒,58次未求解，当前强化学习值为-140753.0,利润为149647.0\n",
      "第220次训练\n",
      "最优模型为35\n",
      "执行时间: 257.70841789245605 秒,48次未求解，当前强化学习值为73917.0,利润为328117.0\n",
      "第221次训练\n",
      "最优模型为35\n",
      "执行时间: 258.69633173942566 秒,59次未求解，当前强化学习值为-163358.0,利润为127242.0\n",
      "第222次训练\n",
      "最优模型为35\n",
      "执行时间: 260.21940994262695 秒,43次未求解，当前强化学习值为198826.0,利润为425426.0\n",
      "第223次训练\n",
      "最优模型为35\n",
      "执行时间: 261.903112411499 秒,33次未求解，当前强化学习值为405308.0,利润为600808.0\n",
      "第224次训练\n",
      "最优模型为35\n",
      "执行时间: 262.8095097541809 秒,60次未求解，当前强化学习值为-185625.0,利润为108375.0\n",
      "第225次训练\n",
      "最优模型为35\n",
      "执行时间: 263.83556151390076 秒,57次未求解，当前强化学习值为-123961.0,利润为163339.0\n",
      "第226次训练\n",
      "最优模型为35\n",
      "执行时间: 265.05682849884033 秒,51次未求解，当前强化学习值为40659.0,利润为299259.0\n",
      "第227次训练\n",
      "最优模型为35\n",
      "执行时间: 265.98778200149536 秒,60次未求解，当前强化学习值为-182715.0,利润为112085.0\n",
      "第228次训练\n",
      "最优模型为35\n",
      "执行时间: 267.057181596756 秒,55次未求解，当前强化学习值为-48599.0,利润为218201.0\n",
      "第229次训练\n",
      "最优模型为35\n",
      "执行时间: 267.9928729534149 秒,60次未求解，当前强化学习值为-194184.0,利润为105716.0\n",
      "第230次训练\n",
      "最优模型为35\n",
      "执行时间: 268.964234828949 秒,60次未求解，当前强化学习值为-187866.0,利润为107834.0\n",
      "第231次训练\n",
      "最优模型为35\n",
      "执行时间: 269.9218215942383 秒,60次未求解，当前强化学习值为-193943.0,利润为104257.0\n",
      "第232次训练\n",
      "最优模型为35\n",
      "执行时间: 270.89600133895874 秒,60次未求解，当前强化学习值为-191198.0,利润为108602.0\n",
      "第233次训练\n",
      "最优模型为35\n",
      "执行时间: 272.1765356063843 秒,48次未求解，当前强化学习值为79180.0,利润为333880.0\n",
      "第234次训练\n",
      "最优模型为35\n",
      "执行时间: 273.50864362716675 秒,47次未求解，当前强化学习值为109717.0,利润为353317.0\n",
      "第235次训练\n",
      "最优模型为35\n",
      "执行时间: 274.6288814544678 秒,54次未求解，当前强化学习值为-55096.0,利润为221304.0\n",
      "第236次训练\n",
      "最优模型为35\n",
      "执行时间: 275.55756282806396 秒,60次未求解，当前强化学习值为-181387.0,利润为112313.0\n",
      "第237次训练\n",
      "最优模型为35\n",
      "执行时间: 276.5299873352051 秒,60次未求解，当前强化学习值为-185989.0,利润为112311.0\n",
      "第238次训练\n",
      "最优模型为35\n",
      "执行时间: 277.5316972732544 秒,60次未求解，当前强化学习值为-186115.0,利润为112085.0\n",
      "第239次训练\n",
      "最优模型为35\n",
      "执行时间: 278.9533004760742 秒,42次未求解，当前强化学习值为211392.0,利润为440392.0\n",
      "第240次训练\n",
      "最优模型为35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 151\u001b[0m\n\u001b[0;32m    149\u001b[0m     next_order_states \u001b[38;5;241m=\u001b[39m vectorization_order(orders_unmatched)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# 这里防止梯度爆炸缩小了reward\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgrid_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_vehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_order_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     env\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m=\u001b[39m time\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:167\u001b[0m, in \u001b[0;36mMultiAgentAC.update\u001b[1;34m(self, vehicle_states, order_states, actions, rewards, next_vehicle_states, next_order_states, dones)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 167\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dirty_test\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dirty_test\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid >= invalid_time:\n",
    "                    agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "            # 一个循环代码让我达到最优\n",
    "            # 屁股后面的代码是为了让我达到最优\n",
    "            # 这里是为了让我达到最优\n",
    "            # 现在放弃了重采样\n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        torch.save(agent, f\"model_checkpoint_{episode}.pth\")\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:alns.ALNS:Finished iterating in 0.15s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best routes: [[0, 1, 0], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0]]\n",
      "Best cost: 858.1198159028768\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from alns import ALNS, State\n",
    "from alns.accept import SimulatedAnnealing\n",
    "from alns.stop import MaxIterations\n",
    "from alns.select import RouletteWheel\n",
    "import random\n",
    "\n",
    "# Problem Data\n",
    "np.random.seed(42)\n",
    "num_customers = 10\n",
    "num_vehicles = 3\n",
    "capacity = 100\n",
    "\n",
    "# 随机生成坐标和需求\n",
    "depot = np.array([50, 50])\n",
    "nodes = np.random.randint(0, 100, (num_customers, 2))\n",
    "demands = np.random.randint(5, 20, num_customers)\n",
    "\n",
    "# 计算欧几里得距离\n",
    "def distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "distance_matrix = np.zeros((num_customers + 1, num_customers + 1))\n",
    "nodes_full = np.vstack([depot, nodes])  # 加入仓库\n",
    "for i in range(len(nodes_full)):\n",
    "    for j in range(len(nodes_full)):\n",
    "        distance_matrix[i, j] = distance(nodes_full[i], nodes_full[j])\n",
    "\n",
    "# 初始状态\n",
    "class VRPState(State):\n",
    "    def __init__(self, routes):\n",
    "        self.routes = routes\n",
    "    \n",
    "    def objective(self):\n",
    "        total_cost = sum(\n",
    "            distance_matrix[route[i], route[i+1]]\n",
    "            for route in self.routes for i in range(len(route)-1)\n",
    "        )\n",
    "        return total_cost\n",
    "    \n",
    "    def copy(self):\n",
    "        return VRPState([route[:] for route in self.routes])\n",
    "\n",
    "# 破坏算子\n",
    "def random_removal(state, rng, num_remove=2):\n",
    "    if isinstance(num_remove, np.random.Generator):  \n",
    "        num_remove = rng.integers(1, 4)  \n",
    "\n",
    "    new_state = state.copy()\n",
    "    for _ in range(num_remove):\n",
    "        if any(new_state.routes):\n",
    "            route = random.choice(new_state.routes)\n",
    "            if len(route) > 2:  # 只有在长度 > 2 时才移除\n",
    "                idx = rng.integers(1, max(2, len(route) - 1))  # 确保 idx 合法\n",
    "                route.pop(idx)\n",
    "    return new_state\n",
    "\n",
    "\n",
    "\n",
    "# 修复算子\n",
    "def greedy_insert(state, rng):\n",
    "    new_state = state.copy()\n",
    "    unassigned = [i for i in range(1, num_customers + 1) if not any(i in r for r in new_state.routes)]\n",
    "    for i in unassigned:\n",
    "        best_cost = float('inf')\n",
    "        best_route = None\n",
    "        best_position = None\n",
    "        for route in new_state.routes:\n",
    "            for pos in range(1, len(route)):\n",
    "                temp_route = route[:pos] + [i] + route[pos:]\n",
    "                cost = sum(distance_matrix[temp_route[j], temp_route[j+1]] for j in range(len(temp_route)-1))\n",
    "                if cost < best_cost:\n",
    "                    best_cost, best_route, best_position = cost, route, pos\n",
    "        if best_route is not None:\n",
    "            best_route.insert(best_position, i)\n",
    "    return new_state\n",
    "\n",
    "# ALNS 运行\n",
    "initial_routes = [[0, i, 0] for i in range(1, num_customers + 1)]  # 每个客户单独一辆车\n",
    "initial_state = VRPState(initial_routes)\n",
    "alns = ALNS()\n",
    "alns.add_destroy_operator(random_removal)\n",
    "alns.add_repair_operator(greedy_insert)\n",
    "\n",
    "# 设定接受准则（模拟退火）\n",
    "accept = SimulatedAnnealing(1000, 1, 500, method=\"linear\")\n",
    "select = RouletteWheel([1] * 4, 0.8, 1, 1)\n",
    "stop = MaxIterations(1000)\n",
    "\n",
    "result = alns.iterate(initial_state, select, accept, stop)\n",
    "\n",
    "# 输出最优解\n",
    "best_state = result.best_state\n",
    "print(\"Best routes:\", best_state.routes)\n",
    "print(\"Best cost:\", best_state.objective())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据固定的逐步调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# 假设有如下初始化函数\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 生成固定样本\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G, speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 保存样本到本地\n",
    "with open('sample_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'Vehicles': Vehicles,\n",
    "        'Total_order': Total_order,\n",
    "        'G':G\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为37447.0,126次未求解\n",
      "第1次训练\n",
      "执行时间: 2.8703413009643555 秒,122次未求解，当前强化学习值为107993.0,利润为107993.0\n",
      "第2次训练\n",
      "执行时间: 5.286675930023193 秒,94次未求解，当前强化学习值为633913.0,利润为633913.0\n",
      "第3次训练\n",
      "执行时间: 7.190031051635742 秒,121次未求解，当前强化学习值为128925.0,利润为128925.0\n",
      "第4次训练\n",
      "执行时间: 10.92548656463623 秒,21次未求解，当前强化学习值为1970629.0,利润为1970629.0\n",
      "第5次训练\n",
      "执行时间: 13.03323769569397 秒,103次未求解，当前强化学习值为472592.0,利润为472592.0\n",
      "第6次训练\n",
      "执行时间: 15.078312397003174 秒,106次未求解，当前强化学习值为421859.0,利润为421859.0\n",
      "第7次训练\n",
      "执行时间: 16.832274436950684 秒,126次未求解，当前强化学习值为43717.0,利润为43717.0\n",
      "第8次训练\n",
      "执行时间: 18.718597888946533 秒,125次未求解，当前强化学习值为61479.0,利润为61479.0\n",
      "第9次训练\n",
      "执行时间: 21.722222566604614 秒,75次未求解，当前强化学习值为1000500.0,利润为1000500.0\n",
      "第10次训练\n",
      "执行时间: 23.95868444442749 秒,118次未求解，当前强化学习值为209451.0,利润为209451.0\n",
      "第11次训练\n",
      "执行时间: 26.204545974731445 秒,110次未求解，当前强化学习值为332313.0,利润为332313.0\n",
      "第12次训练\n",
      "执行时间: 30.566417694091797 秒,82次未求解，当前强化学习值为865682.0,利润为865682.0\n",
      "第13次训练\n",
      "执行时间: 35.45875430107117 秒,0次未求解，当前强化学习值为2330603.0,利润为2330603.0\n",
      "第14次训练\n",
      "执行时间: 37.47613525390625 秒,125次未求解，当前强化学习值为60024.0,利润为60024.0\n",
      "第15次训练\n",
      "执行时间: 39.85452580451965 秒,104次未求解，当前强化学习值为462774.0,利润为462774.0\n",
      "第16次训练\n",
      "执行时间: 41.96865129470825 秒,129次未求解，当前强化学习值为-11112.0,利润为-11112.0\n",
      "第17次训练\n",
      "执行时间: 44.83132481575012 秒,95次未求解，当前强化学习值为632883.0,利润为632883.0\n",
      "第18次训练\n",
      "执行时间: 46.95280313491821 秒,127次未求解，当前强化学习值为22041.0,利润为22041.0\n",
      "第19次训练\n",
      "执行时间: 48.89888787269592 秒,122次未求解，当前强化学习值为117728.0,利润为117728.0\n",
      "第20次训练\n",
      "执行时间: 50.7765793800354 秒,130次未求解，当前强化学习值为-27768.0,利润为-27768.0\n",
      "第21次训练\n",
      "执行时间: 54.26593351364136 秒,58次未求解，当前强化学习值为1278020.0,利润为1278020.0\n",
      "第22次训练\n",
      "执行时间: 56.14102840423584 秒,131次未求解，当前强化学习值为-40260.0,利润为-40260.0\n",
      "第23次训练\n",
      "执行时间: 58.231706619262695 秒,121次未求解，当前强化学习值为130321.0,利润为130321.0\n",
      "第24次训练\n",
      "执行时间: 60.148709297180176 秒,130次未求解，当前强化学习值为-25285.0,利润为-25285.0\n",
      "第25次训练\n",
      "执行时间: 63.600624322891235 秒,61次未求解，当前强化学习值为1196705.0,利润为1196705.0\n",
      "第26次训练\n",
      "执行时间: 67.59428358078003 秒,55次未求解，当前强化学习值为1315851.0,利润为1315851.0\n",
      "第27次训练\n",
      "执行时间: 70.90751576423645 秒,82次未求解，当前强化学习值为831870.0,利润为831870.0\n",
      "第28次训练\n",
      "执行时间: 73.82751178741455 秒,93次未求解，当前强化学习值为628459.0,利润为628459.0\n",
      "第29次训练\n",
      "执行时间: 77.99390721321106 秒,15次未求解，当前强化学习值为1999963.0,利润为1999963.0\n",
      "第30次训练\n",
      "执行时间: 80.38131928443909 秒,104次未求解，当前强化学习值为445631.0,利润为445631.0\n",
      "第31次训练\n",
      "执行时间: 82.46754741668701 秒,131次未求解，当前强化学习值为-44743.0,利润为-44743.0\n",
      "第32次训练\n",
      "执行时间: 85.74827861785889 秒,94次未求解，当前强化学习值为598174.0,利润为598174.0\n",
      "第33次训练\n",
      "执行时间: 87.92352628707886 秒,127次未求解，当前强化学习值为9017.0,利润为9017.0\n",
      "第34次训练\n",
      "执行时间: 90.60340547561646 秒,93次未求解，当前强化学习值为628633.0,利润为628633.0\n",
      "第35次训练\n",
      "执行时间: 92.78000712394714 秒,122次未求解，当前强化学习值为109916.0,利润为109916.0\n",
      "第36次训练\n",
      "执行时间: 95.01948404312134 秒,115次未求解，当前强化学习值为240592.0,利润为240592.0\n",
      "第37次训练\n",
      "执行时间: 96.93955278396606 秒,131次未求解，当前强化学习值为-45956.0,利润为-45956.0\n",
      "第38次训练\n",
      "执行时间: 98.78029918670654 秒,134次未求解，当前强化学习值为-99395.0,利润为-99395.0\n",
      "第39次训练\n",
      "执行时间: 103.3986325263977 秒,6次未求解，当前强化学习值为2165732.0,利润为2165732.0\n",
      "第40次训练\n",
      "执行时间: 105.12556457519531 秒,131次未求解，当前强化学习值为-47930.0,利润为-47930.0\n",
      "第41次训练\n",
      "执行时间: 107.39919519424438 秒,117次未求解，当前强化学习值为201893.0,利润为201893.0\n",
      "第42次训练\n",
      "执行时间: 110.69235324859619 秒,71次未求解，当前强化学习值为1056765.0,利润为1056765.0\n",
      "第43次训练\n",
      "执行时间: 113.62142157554626 秒,87次未求解，当前强化学习值为746351.0,利润为746351.0\n",
      "第44次训练\n",
      "执行时间: 116.00703692436218 秒,115次未求解，当前强化学习值为241432.0,利润为241432.0\n",
      "第45次训练\n",
      "执行时间: 119.74334859848022 秒,75次未求解，当前强化学习值为978696.0,利润为978696.0\n",
      "第46次训练\n",
      "执行时间: 122.062504529953 秒,126次未求解，当前强化学习值为35274.0,利润为35274.0\n",
      "第47次训练\n",
      "执行时间: 124.1991617679596 秒,121次未求解，当前强化学习值为128202.0,利润为128202.0\n",
      "第48次训练\n",
      "执行时间: 127.0389928817749 秒,86次未求解，当前强化学习值为774315.0,利润为774315.0\n",
      "第49次训练\n",
      "执行时间: 129.29026222229004 秒,126次未求解，当前强化学习值为29458.0,利润为29458.0\n",
      "第50次训练\n",
      "执行时间: 132.05732822418213 秒,101次未求解，当前强化学习值为494340.0,利润为494340.0\n",
      "第51次训练\n",
      "执行时间: 134.36561489105225 秒,122次未求解，当前强化学习值为108111.0,利润为108111.0\n",
      "第52次训练\n",
      "执行时间: 136.62965059280396 秒,113次未求解，当前强化学习值为264454.0,利润为264454.0\n",
      "第53次训练\n",
      "执行时间: 139.5014190673828 秒,99次未求解，当前强化学习值为523013.0,利润为523013.0\n",
      "第54次训练\n",
      "执行时间: 142.30254316329956 秒,92次未求解，当前强化学习值为638521.0,利润为638521.0\n",
      "第55次训练\n",
      "执行时间: 145.57233715057373 秒,81次未求解，当前强化学习值为852873.0,利润为852873.0\n",
      "第56次训练\n",
      "执行时间: 147.69618773460388 秒,126次未求解，当前强化学习值为31199.0,利润为31199.0\n",
      "第57次训练\n",
      "执行时间: 153.0731418132782 秒,17次未求解，当前强化学习值为1983796.0,利润为1983796.0\n",
      "第58次训练\n",
      "执行时间: 156.19536805152893 秒,93次未求解，当前强化学习值为632832.0,利润为632832.0\n",
      "第59次训练\n",
      "执行时间: 158.34574365615845 秒,126次未求解，当前强化学习值为34698.0,利润为34698.0\n",
      "第60次训练\n",
      "执行时间: 161.35550570487976 秒,95次未求解，当前强化学习值为613539.0,利润为613539.0\n",
      "第61次训练\n",
      "执行时间: 165.64911651611328 秒,49次未求解，当前强化学习值为1428848.0,利润为1428848.0\n",
      "第62次训练\n",
      "执行时间: 169.35521864891052 秒,70次未求解，当前强化学习值为1066258.0,利润为1066258.0\n",
      "第63次训练\n",
      "执行时间: 172.09895658493042 秒,99次未求解，当前强化学习值为531725.0,利润为531725.0\n",
      "第64次训练\n",
      "执行时间: 174.85338163375854 秒,103次未求解，当前强化学习值为453136.0,利润为453136.0\n",
      "第65次训练\n",
      "执行时间: 178.51508784294128 秒,70次未求解，当前强化学习值为1030603.0,利润为1030603.0\n",
      "第66次训练\n",
      "执行时间: 181.33731198310852 秒,101次未求解，当前强化学习值为498525.0,利润为498525.0\n",
      "第67次训练\n",
      "执行时间: 183.60068917274475 秒,126次未求解，当前强化学习值为41907.0,利润为41907.0\n",
      "第68次训练\n",
      "执行时间: 185.8698422908783 秒,122次未求解，当前强化学习值为110128.0,利润为110128.0\n",
      "第69次训练\n",
      "执行时间: 188.2423872947693 秒,126次未求解，当前强化学习值为43590.0,利润为43590.0\n",
      "第70次训练\n",
      "执行时间: 191.9740867614746 秒,76次未求解，当前强化学习值为936695.0,利润为936695.0\n",
      "第71次训练\n",
      "执行时间: 195.83435010910034 秒,55次未求解，当前强化学习值为1322020.0,利润为1322020.0\n",
      "第72次训练\n",
      "执行时间: 199.21612167358398 秒,80次未求解，当前强化学习值为882386.0,利润为882386.0\n",
      "第73次训练\n",
      "执行时间: 201.66252827644348 秒,110次未求解，当前强化学习值为320222.0,利润为320222.0\n",
      "第74次训练\n",
      "执行时间: 204.00811219215393 秒,119次未求解，当前强化学习值为170681.0,利润为170681.0\n",
      "第75次训练\n",
      "执行时间: 206.4359872341156 秒,115次未求解，当前强化学习值为233262.0,利润为233262.0\n",
      "第76次训练\n",
      "执行时间: 208.4693100452423 秒,130次未求解，当前强化学习值为-42684.0,利润为-42684.0\n",
      "第77次训练\n",
      "执行时间: 210.6660521030426 秒,122次未求解，当前强化学习值为111681.0,利润为111681.0\n",
      "第78次训练\n",
      "执行时间: 213.1940553188324 秒,106次未求解，当前强化学习值为402665.0,利润为402665.0\n",
      "第79次训练\n",
      "执行时间: 215.56296515464783 秒,125次未求解，当前强化学习值为51570.0,利润为51570.0\n",
      "第80次训练\n",
      "执行时间: 220.35098600387573 秒,53次未求解，当前强化学习值为1343466.0,利润为1343466.0\n",
      "第81次训练\n",
      "执行时间: 222.39638590812683 秒,128次未求解，当前强化学习值为-1231.0,利润为-1231.0\n",
      "第82次训练\n",
      "执行时间: 224.8851935863495 秒,113次未求解，当前强化学习值为284270.0,利润为284270.0\n",
      "第83次训练\n",
      "执行时间: 227.51669931411743 秒,108次未求解，当前强化学习值为366205.0,利润为366205.0\n",
      "第84次训练\n",
      "执行时间: 229.60908818244934 秒,126次未求解，当前强化学习值为31999.0,利润为31999.0\n",
      "第85次训练\n",
      "执行时间: 235.0182008743286 秒,0次未求解，当前强化学习值为2268962.0,利润为2268962.0\n",
      "第86次训练\n",
      "执行时间: 237.17723035812378 秒,121次未求解，当前强化学习值为126285.0,利润为126285.0\n",
      "第87次训练\n",
      "执行时间: 242.66893935203552 秒,0次未求解，当前强化学习值为2255358.0,利润为2255358.0\n",
      "第88次训练\n",
      "执行时间: 244.89613604545593 秒,121次未求解，当前强化学习值为132455.0,利润为132455.0\n",
      "第89次训练\n",
      "执行时间: 247.60759925842285 秒,100次未求解，当前强化学习值为510773.0,利润为510773.0\n",
      "第90次训练\n",
      "执行时间: 251.12944197654724 秒,81次未求解，当前强化学习值为843575.0,利润为843575.0\n",
      "第91次训练\n",
      "执行时间: 254.7397153377533 秒,85次未求解，当前强化学习值为776460.0,利润为776460.0\n",
      "第92次训练\n",
      "执行时间: 259.3161497116089 秒,39次未求解，当前强化学习值为1623846.0,利润为1623846.0\n",
      "第93次训练\n",
      "执行时间: 262.50400829315186 秒,91次未求解，当前强化学习值为681168.0,利润为681168.0\n",
      "第94次训练\n",
      "执行时间: 265.029061794281 秒,127次未求解，当前强化学习值为20224.0,利润为20224.0\n",
      "第95次训练\n",
      "执行时间: 267.22857904434204 秒,127次未求解，当前强化学习值为15183.0,利润为15183.0\n",
      "第96次训练\n",
      "执行时间: 269.4452340602875 秒,131次未求解，当前强化学习值为-53246.0,利润为-53246.0\n",
      "第97次训练\n",
      "执行时间: 271.9857864379883 秒,122次未求解，当前强化学习值为109226.0,利润为109226.0\n",
      "第98次训练\n",
      "执行时间: 275.3638381958008 秒,85次未求解，当前强化学习值为791111.0,利润为791111.0\n",
      "第99次训练\n",
      "执行时间: 279.8115985393524 秒,43次未求解，当前强化学习值为1548071.0,利润为1548071.0\n",
      "第100次训练\n",
      "执行时间: 282.1495509147644 秒,126次未求解，当前强化学习值为30565.0,利润为30565.0\n",
      "第101次训练\n",
      "执行时间: 284.5657124519348 秒,122次未求解，当前强化学习值为98518.0,利润为98518.0\n",
      "第102次训练\n",
      "执行时间: 287.84085631370544 秒,90次未求解，当前强化学习值为686351.0,利润为686351.0\n",
      "第103次训练\n",
      "执行时间: 291.0714910030365 秒,87次未求解，当前强化学习值为737586.0,利润为737586.0\n",
      "第104次训练\n",
      "执行时间: 293.4092402458191 秒,124次未求解，当前强化学习值为67384.0,利润为67384.0\n",
      "第105次训练\n",
      "执行时间: 296.3018009662628 秒,101次未求解，当前强化学习值为494010.0,利润为494010.0\n",
      "第106次训练\n",
      "执行时间: 299.1088538169861 秒,108次未求解，当前强化学习值为359400.0,利润为359400.0\n",
      "第107次训练\n",
      "执行时间: 301.44647121429443 秒,126次未求解，当前强化学习值为30002.0,利润为30002.0\n",
      "第108次训练\n",
      "执行时间: 304.01879620552063 秒,115次未求解，当前强化学习值为237056.0,利润为237056.0\n",
      "第109次训练\n",
      "执行时间: 306.419677734375 秒,122次未求解，当前强化学习值为116335.0,利润为116335.0\n",
      "第110次训练\n",
      "执行时间: 308.70088720321655 秒,127次未求解，当前强化学习值为14088.0,利润为14088.0\n",
      "第111次训练\n",
      "执行时间: 311.68016266822815 秒,99次未求解，当前强化学习值为519269.0,利润为519269.0\n",
      "第112次训练\n",
      "执行时间: 316.03793716430664 秒,54次未求解，当前强化学习值为1334308.0,利润为1334308.0\n",
      "第113次训练\n",
      "执行时间: 318.5761742591858 秒,117次未求解，当前强化学习值为216047.0,利润为216047.0\n",
      "第114次训练\n",
      "执行时间: 322.25148463249207 秒,82次未求解，当前强化学习值为832068.0,利润为832068.0\n",
      "第115次训练\n",
      "执行时间: 324.6561875343323 秒,121次未求解，当前强化学习值为130211.0,利润为130211.0\n",
      "第116次训练\n",
      "执行时间: 327.17281579971313 秒,116次未求解，当前强化学习值为214168.0,利润为214168.0\n",
      "第117次训练\n",
      "执行时间: 330.1634612083435 秒,93次未求解，当前强化学习值为638182.0,利润为638182.0\n",
      "第118次训练\n",
      "执行时间: 332.277925491333 秒,127次未求解，当前强化学习值为12014.0,利润为12014.0\n",
      "第119次训练\n",
      "执行时间: 334.46060943603516 秒,118次未求解，当前强化学习值为205725.0,利润为205725.0\n",
      "第120次训练\n",
      "执行时间: 337.4218258857727 秒,96次未求解，当前强化学习值为597622.0,利润为597622.0\n",
      "第121次训练\n",
      "执行时间: 340.1616303920746 秒,102次未求解，当前强化学习值为486443.0,利润为486443.0\n",
      "第122次训练\n",
      "执行时间: 342.2781705856323 秒,122次未求解，当前强化学习值为115975.0,利润为115975.0\n",
      "第123次训练\n",
      "执行时间: 345.4575922489166 秒,85次未求解，当前强化学习值为794622.9999602442,利润为794622.9999602442\n",
      "第124次训练\n",
      "执行时间: 348.1045641899109 秒,110次未求解，当前强化学习值为330817.0,利润为330817.0\n",
      "第125次训练\n",
      "执行时间: 350.1804759502411 秒,126次未求解，当前强化学习值为28927.0,利润为28927.0\n",
      "第126次训练\n",
      "执行时间: 355.2294673919678 秒,17次未求解，当前强化学习值为1959516.0,利润为1959516.0\n",
      "第127次训练\n",
      "执行时间: 358.2959110736847 秒,94次未求解，当前强化学习值为624504.0,利润为624504.0\n",
      "第128次训练\n",
      "执行时间: 360.2608811855316 秒,127次未求解，当前强化学习值为17502.0,利润为17502.0\n",
      "第129次训练\n",
      "执行时间: 362.27247858047485 秒,126次未求解，当前强化学习值为31666.0,利润为31666.0\n",
      "第130次训练\n",
      "执行时间: 366.06329345703125 秒,60次未求解，当前强化学习值为1222974.0,利润为1222974.0\n",
      "第131次训练\n",
      "执行时间: 368.5651240348816 秒,106次未求解，当前强化学习值为404339.0,利润为404339.0\n",
      "第132次训练\n",
      "执行时间: 372.0803942680359 秒,81次未求解，当前强化学习值为853834.0,利润为853834.0\n",
      "第133次训练\n",
      "执行时间: 374.19484853744507 秒,126次未求解，当前强化学习值为32409.0,利润为32409.0\n",
      "第134次训练\n",
      "执行时间: 376.59462571144104 秒,112次未求解，当前强化学习值为291915.0001864364,利润为291915.0001864364\n",
      "第135次训练\n",
      "执行时间: 379.5437831878662 秒,94次未求解，当前强化学习值为606579.0,利润为606579.0\n",
      "第136次训练\n",
      "执行时间: 381.58539867401123 秒,126次未求解，当前强化学习值为37417.0,利润为37417.0\n",
      "第137次训练\n",
      "执行时间: 383.83077120780945 秒,125次未求解，当前强化学习值为44837.0,利润为44837.0\n",
      "第138次训练\n",
      "执行时间: 386.07515478134155 秒,123次未求解，当前强化学习值为92997.0,利润为92997.0\n",
      "第139次训练\n",
      "执行时间: 388.50457239151 秒,116次未求解，当前强化学习值为228610.0,利润为228610.0\n",
      "第140次训练\n",
      "执行时间: 392.1636209487915 秒,66次未求解，当前强化学习值为1121257.0,利润为1121257.0\n",
      "第141次训练\n",
      "执行时间: 394.0834159851074 秒,134次未求解，当前强化学习值为-103840.0,利润为-103840.0\n",
      "第142次训练\n",
      "执行时间: 399.3887097835541 秒,6次未求解，当前强化学习值为2157147.0,利润为2157147.0\n",
      "第143次训练\n",
      "执行时间: 401.78265380859375 秒,117次未求解，当前强化学习值为208003.0,利润为208003.0\n",
      "第144次训练\n",
      "执行时间: 405.6719162464142 秒,76次未求解，当前强化学习值为952162.0,利润为952162.0\n",
      "第145次训练\n",
      "执行时间: 408.95107102394104 秒,82次未求解，当前强化学习值为828957.0,利润为828957.0\n",
      "第146次训练\n",
      "执行时间: 411.1608316898346 秒,126次未求解，当前强化学习值为35281.0,利润为35281.0\n",
      "第147次训练\n",
      "执行时间: 413.3737015724182 秒,130次未求解，当前强化学习值为-27756.0,利润为-27756.0\n",
      "第148次训练\n",
      "执行时间: 415.78751254081726 秒,122次未求解，当前强化学习值为101431.0,利润为101431.0\n",
      "第149次训练\n",
      "执行时间: 419.01369190216064 秒,96次未求解，当前强化学习值为593137.0,利润为593137.0\n",
      "第150次训练\n",
      "执行时间: 423.22519755363464 秒,54次未求解，当前强化学习值为1335023.0,利润为1335023.0\n",
      "第151次训练\n",
      "执行时间: 426.5465188026428 秒,93次未求解，当前强化学习值为645622.0,利润为645622.0\n",
      "第152次训练\n",
      "执行时间: 428.65453577041626 秒,134次未求解，当前强化学习值为-104989.0,利润为-104989.0\n",
      "第153次训练\n",
      "执行时间: 433.39773416519165 秒,36次未求解，当前强化学习值为1653101.0,利润为1653101.0\n",
      "第154次训练\n",
      "执行时间: 435.94488406181335 秒,117次未求解，当前强化学习值为209623.0,利润为209623.0\n",
      "第155次训练\n",
      "执行时间: 439.26588129997253 秒,100次未求解，当前强化学习值为483745.0,利润为483745.0\n",
      "第156次训练\n",
      "执行时间: 441.55820059776306 秒,126次未求解，当前强化学习值为37443.0,利润为37443.0\n",
      "第157次训练\n",
      "执行时间: 444.45712661743164 秒,101次未求解，当前强化学习值为506276.0,利润为506276.0\n",
      "第158次训练\n",
      "执行时间: 446.77455735206604 秒,124次未求解，当前强化学习值为75811.0,利润为75811.0\n",
      "第159次训练\n",
      "执行时间: 448.8832302093506 秒,132次未求解，当前强化学习值为-69089.0,利润为-69089.0\n",
      "第160次训练\n",
      "执行时间: 452.1082820892334 秒,87次未求解，当前强化学习值为744421.0,利润为744421.0\n",
      "第161次训练\n",
      "执行时间: 454.5276825428009 秒,122次未求解，当前强化学习值为114296.0,利润为114296.0\n",
      "第162次训练\n",
      "执行时间: 457.13955521583557 秒,115次未求解，当前强化学习值为247945.0,利润为247945.0\n",
      "第163次训练\n",
      "执行时间: 459.2762324810028 秒,126次未求解，当前强化学习值为47718.0,利润为47718.0\n",
      "第164次训练\n",
      "执行时间: 462.26011395454407 秒,94次未求解，当前强化学习值为633972.0,利润为633972.0\n",
      "第165次训练\n",
      "执行时间: 465.0803985595703 秒,114次未求解，当前强化学习值为275728.0,利润为275728.0\n",
      "第166次训练\n",
      "执行时间: 467.9418637752533 秒,101次未求解，当前强化学习值为497963.0,利润为497963.0\n",
      "第167次训练\n",
      "执行时间: 470.4872989654541 秒,121次未求解，当前强化学习值为125255.0,利润为125255.0\n",
      "第168次训练\n",
      "执行时间: 472.5701289176941 秒,133次未求解，当前强化学习值为-78946.0,利润为-78946.0\n",
      "第169次训练\n",
      "执行时间: 475.0510356426239 秒,115次未求解，当前强化学习值为245474.0,利润为245474.0\n",
      "第170次训练\n",
      "执行时间: 477.97651720046997 秒,100次未求解，当前强化学习值为517474.0,利润为517474.0\n",
      "第171次训练\n",
      "执行时间: 481.05826902389526 秒,100次未求解，当前强化学习值为502973.0,利润为502973.0\n",
      "第172次训练\n",
      "执行时间: 486.6321270465851 秒,0次未求解，当前强化学习值为2319110.0004132874,利润为2319110.0004132874\n",
      "第173次训练\n",
      "执行时间: 488.9379880428314 秒,121次未求解，当前强化学习值为130786.0,利润为130786.0\n",
      "第174次训练\n",
      "执行时间: 490.9811918735504 秒,134次未求解，当前强化学习值为-98710.0,利润为-98710.0\n",
      "第175次训练\n",
      "执行时间: 493.4623398780823 秒,116次未求解，当前强化学习值为217451.0,利润为217451.0\n",
      "第176次训练\n",
      "执行时间: 498.2526648044586 秒,40次未求解，当前强化学习值为1594276.0,利润为1594276.0\n",
      "第177次训练\n",
      "执行时间: 500.4218056201935 秒,129次未求解，当前强化学习值为-11123.0,利润为-11123.0\n",
      "第178次训练\n",
      "执行时间: 506.26245880126953 秒,0次未求解，当前强化学习值为2230330.0,利润为2230330.0\n",
      "第179次训练\n",
      "执行时间: 508.7895109653473 秒,121次未求解，当前强化学习值为117752.0,利润为117752.0\n",
      "第180次训练\n",
      "执行时间: 511.1795697212219 秒,126次未求解，当前强化学习值为47707.0,利润为47707.0\n",
      "第181次训练\n",
      "执行时间: 514.8841462135315 秒,77次未求解，当前强化学习值为936479.0,利润为936479.0\n",
      "第182次训练\n",
      "执行时间: 517.2690846920013 秒,126次未求解，当前强化学习值为35953.0,利润为35953.0\n",
      "第183次训练\n",
      "执行时间: 519.35569024086 秒,134次未求解，当前强化学习值为-101919.0,利润为-101919.0\n",
      "第184次训练\n",
      "执行时间: 521.3928508758545 秒,134次未求解，当前强化学习值为-96184.0,利润为-96184.0\n",
      "第185次训练\n",
      "执行时间: 524.5659954547882 秒,95次未求解，当前强化学习值为610990.0,利润为610990.0\n",
      "第186次训练\n",
      "执行时间: 526.6122510433197 秒,136次未求解，当前强化学习值为-134576.0,利润为-134576.0\n",
      "第187次训练\n",
      "执行时间: 528.9727234840393 秒,125次未求解，当前强化学习值为53642.0,利润为53642.0\n",
      "第188次训练\n",
      "执行时间: 531.4823310375214 秒,122次未求解，当前强化学习值为119931.0,利润为119931.0\n",
      "第189次训练\n",
      "执行时间: 533.7815098762512 秒,130次未求解，当前强化学习值为-34694.0,利润为-34694.0\n",
      "第190次训练\n",
      "执行时间: 535.7961766719818 秒,134次未求解，当前强化学习值为-99406.0,利润为-99406.0\n",
      "第191次训练\n",
      "执行时间: 538.3785724639893 秒,114次未求解，当前强化学习值为257029.0,利润为257029.0\n",
      "第192次训练\n",
      "执行时间: 540.6967763900757 秒,119次未求解，当前强化学习值为176589.0,利润为176589.0\n",
      "第193次训练\n",
      "执行时间: 543.2378978729248 秒,115次未求解，当前强化学习值为241401.0,利润为241401.0\n",
      "第194次训练\n",
      "执行时间: 545.4830124378204 秒,122次未求解，当前强化学习值为109745.0,利润为109745.0\n",
      "第195次训练\n",
      "执行时间: 548.1219966411591 秒,103次未求解，当前强化学习值为457615.0,利润为457615.0\n",
      "第196次训练\n",
      "执行时间: 550.5528411865234 秒,119次未求解，当前强化学习值为177389.0,利润为177389.0\n",
      "第197次训练\n",
      "执行时间: 552.5948390960693 秒,134次未求解，当前强化学习值为-101920.0,利润为-101920.0\n",
      "第198次训练\n",
      "执行时间: 554.6549122333527 秒,134次未求解，当前强化学习值为-96187.0,利润为-96187.0\n",
      "第199次训练\n",
      "执行时间: 556.7224571704865 秒,134次未求解，当前强化学习值为-96186.0,利润为-96186.0\n",
      "第200次训练\n",
      "执行时间: 558.8824510574341 秒,128次未求解，当前强化学习值为-5489.0,利润为-5489.0\n",
      "第201次训练\n",
      "执行时间: 561.0553390979767 秒,127次未求解，当前强化学习值为18931.0,利润为18931.0\n",
      "第202次训练\n",
      "执行时间: 563.8856062889099 秒,121次未求解，当前强化学习值为128865.0,利润为128865.0\n",
      "第203次训练\n",
      "执行时间: 566.3078608512878 秒,122次未求解，当前强化学习值为102033.0,利润为102033.0\n",
      "第204次训练\n",
      "执行时间: 568.5163013935089 秒,131次未求解，当前强化学习值为-45886.0,利润为-45886.0\n",
      "第205次训练\n",
      "执行时间: 571.0048913955688 秒,115次未求解，当前强化学习值为230442.0,利润为230442.0\n",
      "第206次训练\n",
      "执行时间: 573.0799293518066 秒,133次未求解，当前强化学习值为-78101.0,利润为-78101.0\n",
      "第207次训练\n",
      "执行时间: 575.2669429779053 秒,130次未求解，当前强化学习值为-25013.0,利润为-25013.0\n",
      "第208次训练\n",
      "执行时间: 578.0496485233307 秒,105次未求解，当前强化学习值为426820.0,利润为426820.0\n",
      "第209次训练\n",
      "执行时间: 580.7407581806183 秒,123次未求解，当前强化学习值为105771.99980780401,利润为105771.99980780401\n",
      "第210次训练\n",
      "执行时间: 583.9463663101196 秒,87次未求解，当前强化学习值为755181.0,利润为755181.0\n",
      "第211次训练\n",
      "执行时间: 586.2761075496674 秒,122次未求解，当前强化学习值为105259.0,利润为105259.0\n",
      "第212次训练\n",
      "执行时间: 588.3608803749084 秒,133次未求解，当前强化学习值为-78946.0,利润为-78946.0\n",
      "第213次训练\n",
      "执行时间: 590.656507730484 秒,121次未求解，当前强化学习值为127736.0,利润为127736.0\n",
      "第214次训练\n",
      "执行时间: 593.0653355121613 秒,122次未求解，当前强化学习值为111709.0,利润为111709.0\n",
      "第215次训练\n",
      "执行时间: 595.1539969444275 秒,134次未求解，当前强化学习值为-101920.0,利润为-101920.0\n",
      "第216次训练\n",
      "执行时间: 598.3183522224426 秒,93次未求解，当前强化学习值为628833.0,利润为628833.0\n",
      "第217次训练\n",
      "执行时间: 602.038298368454 秒,68次未求解，当前强化学习值为1110601.0,利润为1110601.0\n",
      "第218次训练\n",
      "执行时间: 604.2941706180573 秒,127次未求解，当前强化学习值为13832.0,利润为13832.0\n",
      "第219次训练\n",
      "执行时间: 606.5991098880768 秒,122次未求解，当前强化学习值为111683.0,利润为111683.0\n",
      "第220次训练\n",
      "执行时间: 608.9177474975586 秒,125次未求解，当前强化学习值为62094.0,利润为62094.0\n",
      "第221次训练\n",
      "执行时间: 612.3266606330872 秒,87次未求解，当前强化学习值为739569.0,利润为739569.0\n",
      "第222次训练\n",
      "执行时间: 616.2812306880951 秒,74次未求解，当前强化学习值为980024.0,利润为980024.0\n",
      "第223次训练\n",
      "执行时间: 618.3453028202057 秒,133次未求解，当前强化学习值为-85806.0,利润为-85806.0\n",
      "第224次训练\n",
      "执行时间: 621.3626170158386 秒,101次未求解，当前强化学习值为497186.0,利润为497186.0\n",
      "第225次训练\n",
      "执行时间: 623.9365389347076 秒,116次未求解，当前强化学习值为234187.0,利润为234187.0\n",
      "第226次训练\n",
      "执行时间: 626.3217113018036 秒,123次未求解，当前强化学习值为99377.0,利润为99377.0\n",
      "第227次训练\n",
      "执行时间: 629.0590667724609 秒,115次未求解，当前强化学习值为249059.0,利润为249059.0\n",
      "第228次训练\n",
      "执行时间: 632.2267813682556 秒,93次未求解，当前强化学习值为632696.0,利润为632696.0\n",
      "第229次训练\n",
      "执行时间: 636.2362439632416 秒,69次未求解，当前强化学习值为1072752.0,利润为1072752.0\n",
      "第230次训练\n",
      "执行时间: 639.2454390525818 秒,101次未求解，当前强化学习值为473630.0,利润为473630.0\n",
      "第231次训练\n",
      "执行时间: 641.3945224285126 秒,130次未求解，当前强化学习值为-25279.0,利润为-25279.0\n",
      "第232次训练\n",
      "执行时间: 644.7985150814056 秒,83次未求解，当前强化学习值为822906.0,利润为822906.0\n",
      "第233次训练\n",
      "执行时间: 647.2770009040833 秒,114次未求解，当前强化学习值为257447.0,利润为257447.0\n",
      "第234次训练\n",
      "执行时间: 651.2537705898285 秒,72次未求解，当前强化学习值为1040382.0,利润为1040382.0\n",
      "第235次训练\n",
      "执行时间: 654.4040668010712 秒,101次未求解，当前强化学习值为503627.0,利润为503627.0\n",
      "第236次训练\n",
      "执行时间: 658.0498442649841 秒,74次未求解，当前强化学习值为1004775.0,利润为1004775.0\n",
      "第237次训练\n",
      "执行时间: 662.294682264328 秒,62次未求解，当前强化学习值为1207374.0,利润为1207374.0\n",
      "第238次训练\n",
      "执行时间: 664.818478345871 秒,120次未求解，当前强化学习值为152027.0,利润为152027.0\n",
      "第239次训练\n",
      "执行时间: 666.999870300293 秒,129次未求解，当前强化学习值为-11126.0,利润为-11126.0\n",
      "第240次训练\n",
      "执行时间: 669.5404260158539 秒,116次未求解，当前强化学习值为227670.0,利润为227670.0\n",
      "第241次训练\n",
      "执行时间: 672.0697886943817 秒,120次未求解，当前强化学习值为149514.0,利润为149514.0\n",
      "第242次训练\n",
      "执行时间: 674.6304001808167 秒,113次未求解，当前强化学习值为281423.0,利润为281423.0\n",
      "第243次训练\n",
      "执行时间: 677.9681806564331 秒,96次未求解，当前强化学习值为581567.0,利润为581567.0\n",
      "第244次训练\n",
      "执行时间: 680.9403147697449 秒,100次未求解，当前强化学习值为499659.0,利润为499659.0\n",
      "第245次训练\n",
      "执行时间: 682.9612238407135 秒,134次未求解，当前强化学习值为-96193.0,利润为-96193.0\n",
      "第246次训练\n",
      "执行时间: 685.3526639938354 秒,121次未求解，当前强化学习值为128440.0,利润为128440.0\n",
      "第247次训练\n",
      "执行时间: 687.953381061554 秒,125次未求解，当前强化学习值为50841.0,利润为50841.0\n",
      "第248次训练\n",
      "执行时间: 691.7267160415649 秒,71次未求解，当前强化学习值为1052805.0,利润为1052805.0\n",
      "第249次训练\n",
      "执行时间: 693.9982001781464 秒,121次未求解，当前强化学习值为134660.0,利润为134660.0\n",
      "第250次训练\n",
      "执行时间: 699.3909442424774 秒,0次未求解，当前强化学习值为2303944.0,利润为2303944.0\n",
      "第251次训练\n",
      "执行时间: 702.0920870304108 秒,112次未求解，当前强化学习值为296207.0,利润为296207.0\n",
      "第252次训练\n",
      "执行时间: 704.5310614109039 秒,122次未求解，当前强化学习值为113712.0,利润为113712.0\n",
      "第253次训练\n",
      "执行时间: 710.0413599014282 秒,0次未求解，当前强化学习值为2305558.0,利润为2305558.0\n",
      "第254次训练\n",
      "执行时间: 712.1250398159027 秒,134次未求解，当前强化学习值为-96194.0,利润为-96194.0\n",
      "第255次训练\n",
      "执行时间: 716.1439170837402 秒,63次未求解，当前强化学习值为1165051.0,利润为1165051.0\n",
      "第256次训练\n",
      "执行时间: 718.7396605014801 秒,115次未求解，当前强化学习值为237561.0,利润为237561.0\n",
      "第257次训练\n",
      "执行时间: 722.2653226852417 秒,81次未求解，当前强化学习值为874349.0,利润为874349.0\n",
      "第258次训练\n",
      "执行时间: 724.3832859992981 秒,134次未求解，当前强化学习值为-99402.0,利润为-99402.0\n",
      "第259次训练\n",
      "执行时间: 727.4535026550293 秒,95次未求解，当前强化学习值为608581.0,利润为608581.0\n",
      "第260次训练\n",
      "执行时间: 730.5189545154572 秒,102次未求解，当前强化学习值为494377.0,利润为494377.0\n",
      "第261次训练\n",
      "执行时间: 732.8806824684143 秒,122次未求解，当前强化学习值为101613.0,利润为101613.0\n",
      "第262次训练\n",
      "执行时间: 734.8687043190002 秒,136次未求解，当前强化学习值为-137167.0,利润为-137167.0\n",
      "第263次训练\n",
      "执行时间: 737.3282458782196 秒,115次未求解，当前强化学习值为246845.0,利润为246845.0\n",
      "第264次训练\n",
      "执行时间: 739.5621864795685 秒,129次未求解，当前强化学习值为-17518.0,利润为-17518.0\n",
      "第265次训练\n",
      "执行时间: 742.5823795795441 秒,94次未求解，当前强化学习值为628800.0,利润为628800.0\n",
      "第266次训练\n",
      "执行时间: 745.921618938446 秒,87次未求解，当前强化学习值为757596.0,利润为757596.0\n",
      "第267次训练\n",
      "执行时间: 749.776534318924 秒,70次未求解，当前强化学习值为1066388.0,利润为1066388.0\n",
      "第268次训练\n",
      "执行时间: 752.1954572200775 秒,127次未求解，当前强化学习值为22038.0,利润为22038.0\n",
      "第269次训练\n",
      "执行时间: 755.8596606254578 秒,79次未求解，当前强化学习值为900100.0,利润为900100.0\n",
      "第270次训练\n",
      "执行时间: 758.3373908996582 秒,113次未求解，当前强化学习值为268271.0,利润为268271.0\n",
      "第271次训练\n",
      "执行时间: 760.9589903354645 秒,120次未求解，当前强化学习值为147228.0,利润为147228.0\n",
      "第272次训练\n",
      "执行时间: 765.1132457256317 秒,44次未求解，当前强化学习值为1542527.0,利润为1542527.0\n",
      "第273次训练\n",
      "执行时间: 767.6685922145844 秒,115次未求解，当前强化学习值为246260.0,利润为246260.0\n",
      "第274次训练\n",
      "执行时间: 771.0692610740662 秒,81次未求解，当前强化学习值为854650.0,利润为854650.0\n",
      "第275次训练\n",
      "执行时间: 774.9816918373108 秒,72次未求解，当前强化学习值为1022451.0,利润为1022451.0\n",
      "第276次训练\n",
      "执行时间: 778.60564661026 秒,79次未求解，当前强化学习值为913662.0,利润为913662.0\n",
      "第277次训练\n",
      "执行时间: 781.2232890129089 秒,116次未求解，当前强化学习值为233707.0,利润为233707.0\n",
      "第278次训练\n",
      "执行时间: 783.2678217887878 秒,134次未求解，当前强化学习值为-96181.0,利润为-96181.0\n",
      "第279次训练\n",
      "执行时间: 785.6680586338043 秒,122次未求解，当前强化学习值为106938.0,利润为106938.0\n",
      "第280次训练\n",
      "执行时间: 787.6849851608276 秒,136次未求解，当前强化学习值为-137167.0,利润为-137167.0\n",
      "第281次训练\n",
      "执行时间: 792.8942387104034 秒,16次未求解，当前强化学习值为2001637.0,利润为2001637.0\n",
      "第282次训练\n",
      "执行时间: 797.0559868812561 秒,60次未求解，当前强化学习值为1214654.0,利润为1214654.0\n",
      "第283次训练\n",
      "执行时间: 799.2984178066254 秒,126次未求解，当前强化学习值为35298.0,利润为35298.0\n",
      "第284次训练\n",
      "执行时间: 802.8391306400299 秒,82次未求解，当前强化学习值为838957.0,利润为838957.0\n",
      "第285次训练\n",
      "执行时间: 805.2634928226471 秒,126次未求解，当前强化学习值为39561.0,利润为39561.0\n",
      "第286次训练\n",
      "执行时间: 808.413735628128 秒,95次未求解，当前强化学习值为610904.0,利润为610904.0\n",
      "第287次训练\n",
      "执行时间: 811.8463687896729 秒,77次未求解，当前强化学习值为938782.0,利润为938782.0\n",
      "第288次训练\n",
      "执行时间: 814.09872174263 秒,126次未求解，当前强化学习值为35290.0,利润为35290.0\n",
      "第289次训练\n",
      "执行时间: 817.0444624423981 秒,106次未求解，当前强化学习值为403595.0,利润为403595.0\n",
      "第290次训练\n",
      "执行时间: 819.5843987464905 秒,113次未求解，当前强化学习值为272436.0,利润为272436.0\n",
      "第291次训练\n",
      "执行时间: 822.2222619056702 秒,115次未求解，当前强化学习值为245834.0,利润为245834.0\n",
      "第292次训练\n",
      "执行时间: 824.7210245132446 秒,123次未求解，当前强化学习值为103393.0,利润为103393.0\n",
      "第293次训练\n",
      "执行时间: 827.8999507427216 秒,91次未求解，当前强化学习值为670609.0,利润为670609.0\n",
      "第294次训练\n",
      "执行时间: 830.6202421188354 秒,110次未求解，当前强化学习值为324879.0,利润为324879.0\n",
      "第295次训练\n",
      "执行时间: 835.0002648830414 秒,53次未求解，当前强化学习值为1367623.0,利润为1367623.0\n",
      "第296次训练\n",
      "执行时间: 837.2071709632874 秒,130次未求解，当前强化学习值为-25276.0,利润为-25276.0\n",
      "第297次训练\n",
      "执行时间: 839.6772217750549 秒,119次未求解，当前强化学习值为184642.0,利润为184642.0\n",
      "第298次训练\n",
      "执行时间: 841.7320437431335 秒,134次未求解，当前强化学习值为-98717.0,利润为-98717.0\n",
      "第299次训练\n",
      "执行时间: 844.1279027462006 秒,122次未求解，当前强化学习值为107743.0,利润为107743.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update_third(vehicle_states, order_states, action, selected_log_probs, log_probs, probs,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action, selected_log_probs, log_probs, probs = agent.take_action_third(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            ACTIONS.append(action) \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "           \n",
    "       \n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验记录\n",
    "\n",
    "1. 基模型：不收敛，差！\n",
    "2. greedy_epsilon二次收敛，模型永远更新参数：不收敛，差！\n",
    "3. 改变了参数的传递，使得take_action_third()返回当前掩码下的概率分布，同时将这些概率传入update_third()，因此在对应使用中不再是之前的重新计算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['[0, 1, 2, 5, 7]', '[0, 2, 4, 7]', '[0, 5, 6, 7]', '[1, 3, 6, 7]', '[0, 1, 2, 7]', '[2, 4, 6]', '[2, 3, 6, 7]', '[0, 3, 5, 7]', '[1, 2, 6, 7]', '[1, 2, 4]', '[1, 2, 6]', '[1, 2, 3, 7]', '[5, 6]', '[0, 2, 4, 5, 7]', '[0, 3, 6, 7]', '[3, 5, 6, 7]', '[0, 1, 4, 7]', '[1, 4, 6]', '[1, 2, 4, 5, 7]', '[3, 6]', '[0, 1, 4, 5, 7]', '[3, 5]', '[0, 1, 2, 4, 7]', '[0, 3, 5, 6]', '[1, 2, 3, 6]', '[1, 2, 4, 7]', '[0, 1, 2, 4]'])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "print(order_same_action(Total_order, num_city, G).keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dirty_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
