{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为619304.0,0次未求解\n",
      "第1次训练\n",
      "执行时间: 2.7062771320343018 秒,0次未求解，当前强化学习值为618727.0,利润为618727.0\n",
      "第2次训练\n",
      "执行时间: 167.0326371192932 秒,91次未求解，当前强化学习值为184484.0,利润为290184.0\n",
      "第3次训练\n",
      "执行时间: 168.90956950187683 秒,0次未求解，当前强化学习值为618641.0,利润为618641.0\n",
      "第4次训练\n",
      "执行时间: 171.83974146842957 秒,0次未求解，当前强化学习值为618455.0,利润为618655.0\n",
      "第5次训练\n",
      "执行时间: 280.2519998550415 秒,111次未求解，当前强化学习值为78127.0,利润为215327.0\n",
      "第6次训练\n",
      "执行时间: 404.32373547554016 秒,124次未求解，当前强化学习值为19505.0,利润为169605.0\n",
      "第7次训练\n",
      "执行时间: 532.7008144855499 秒,130次未求解，当前强化学习值为-5123.0,利润为152577.0\n",
      "第8次训练\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 166\u001b[0m\n\u001b[0;32m    164\u001b[0m min_action \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m reward \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m COUNT \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 166\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mtest_step(orders_unmatched,action)\n\u001b[0;32m    168\u001b[0m     COUNT \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:119\u001b[0m, in \u001b[0;36mMultiAgentAC.take_action\u001b[1;34m(self, vehicle_states, order_states, explore)\u001b[0m\n\u001b[0;32m    115\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m explore \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# print(probs)\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# 采样动作\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m actions \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mmultinomial(p, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m probs]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# print(actions)\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actions\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:119\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m explore \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# print(probs)\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# 采样动作\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m actions \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m probs]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# print(actions)\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actions\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 8\n",
    "num_order = 2\n",
    "num_city = 4\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 100      # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 1:\n",
    "        # agent.load_model(load_path)\n",
    "        env.time = 0\n",
    "\n",
    "    for time in range(TIME):\n",
    "    \n",
    "        group = [[], []]\n",
    "\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, grid_reward, next_vehicle_states, next_order_states, True ,action)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            action = agent.take_action(vehicle_states, order_states)\n",
    "        \n",
    "            ACTIONS.append(action)            \n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "            # 一个循环代码让我达到最优\n",
    "            # 屁股后面的代码是为了让我达到最优\n",
    "            # 这里是为了让我达到最优\n",
    "           \n",
    "            change_time = 0\n",
    "            COUNT = 999\n",
    "            min_reward = -999999\n",
    "            min_action = action\n",
    "            while reward != 0 and COUNT > 0:\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > min_reward:\n",
    "                    min_reward = reward\n",
    "                    min_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched,min_action)\n",
    "            \n",
    "            \"\"\"\n",
    "            COUNT = 999\n",
    "            min_reward = -999999\n",
    "            min_action = action\n",
    "            while reward != 0 and COUNT > 0:\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > min_reward:\n",
    "                    min_reward = reward\n",
    "                    min_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched,min_action)\n",
    "            for env_order in env.orders.values():\n",
    "                for order_unmatched in orders_unmatched.values():\n",
    "                    if env_order.id == order_unmatched.id:\n",
    "                        order_unmatched.virtual_departure = env_order.virtual_departure\n",
    "                        change_time += 1\n",
    "            print(change_time)\n",
    "            city_update_base_drl(env.cities, orders_unmatched ,time)\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            temp_Lower_Layer.get_decision()\n",
    "            temp_Lower_Layer.constrain_1()\n",
    "            temp_Lower_Layer.constrain_2()\n",
    "            temp_Lower_Layer.constrain_3()\n",
    "            temp_Lower_Layer.constrain_4()\n",
    "            temp_Lower_Layer.constrain_5()\n",
    "            temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "            total_penalty = cancel_penalty * order_canceled\n",
    "            temp_Lower_Layer.set_objective(matrix)\n",
    "        \n",
    "            temp_Lower_Layer.model.optimize()\n",
    "\n",
    "            if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                # save_results(temp_Lower_Layer,time)\n",
    "                # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                objval = temp_Lower_Layer.model.objVal \n",
    "            else:\n",
    "                temp_Lower_Layer.model.computeIIS()\n",
    "                temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                # print(f\"{time}次，No optimal solution found.\")\n",
    "                self_update(Vehicles, G)\n",
    "                objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                \n",
    "            \n",
    "            _, var_order = temp_Lower_Layer.get_decision()\n",
    "            update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "            vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =  objval + reward\n",
    "            grid_rewards.append(grid_reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # agent.save_model(save_path)\n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态订单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为297320.0,139次未求解\n",
      "第1次训练\n",
      "(1, 720)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 1 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 156\u001b[0m\n\u001b[0;32m    154\u001b[0m mask \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mgenerate_mask()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 156\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(action)\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\dynamic_multiagent.py:151\u001b[0m, in \u001b[0;36mDynamicMultiAgentAC.take_action\u001b[1;34m(self, vehicle_states, order_states, explore)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m active_indices:\n\u001b[0;32m    150\u001b[0m     order_feat \u001b[38;5;241m=\u001b[39m o_encoded[i]\n\u001b[1;32m--> 151\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mglobal_vehicle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_feat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     actor_inputs\u001b[38;5;241m.\u001b[39mappend(combined)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# 批量处理\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 1 and 2"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import dynamic_multiagent as dm \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 16\n",
    "num_order = 5\n",
    "num_city = 7\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 100      # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = dm.DynamicMultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    vehicle_state_dim =  STATE_DIM_VEHICLE,\n",
    "    order_state_dim = STATE_DIM_ORDER, \n",
    "    num_cities = num_city, \n",
    "    hidden_dim = HIDDEN_DIM, \n",
    "    max_orders= num_order*TIME\n",
    ")\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    agent.total_orders = Total_order\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 1:\n",
    "        # agent.load_model(load_path)\n",
    "        env.time = 0\n",
    "\n",
    "    for time in range(TIME):\n",
    "    \n",
    "        group = [[], []]\n",
    "\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(Total_order)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, next_vehicle_states, next_order_states, action, grid_reward , mask)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(Total_order)\n",
    "            # 这里也加了\n",
    "            mask = agent.generate_mask()\n",
    "            print(mask.shape)\n",
    "            action = agent.take_action(vehicle_states, order_states)\n",
    "            if action:\n",
    "                print(action)\n",
    "            ACTIONS.append(action)            \n",
    "            reward = env.dynamic_step(Total_order,action,mask)\n",
    "            # 一个循环代码让我达到最优\n",
    "            # 屁股后面的代码是为了让我达到最优\n",
    "            # 这里是为了让我达到最优\n",
    "           \n",
    "            change_time = 0\n",
    "            COUNT = 999\n",
    "            min_reward = -999999\n",
    "            min_action = action\n",
    "            while reward != 0 and COUNT > 0:\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.dynamic_step(Total_order,action,mask)\n",
    "                COUNT -= 1\n",
    "                if reward > min_reward:\n",
    "                    min_reward = reward\n",
    "                    min_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.dynamic_step(Total_order,action,mask)\n",
    "            \n",
    "            \"\"\"\n",
    "            COUNT = 999\n",
    "            min_reward = -999999\n",
    "            min_action = action\n",
    "            while reward != 0 and COUNT > 0:\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > min_reward:\n",
    "                    min_reward = reward\n",
    "                    min_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched,min_action)\n",
    "            for env_order in env.orders.values():\n",
    "                for order_unmatched in orders_unmatched.values():\n",
    "                    if env_order.id == order_unmatched.id:\n",
    "                        order_unmatched.virtual_departure = env_order.virtual_departure\n",
    "                        change_time += 1\n",
    "            print(change_time)\n",
    "            city_update_base_drl(env.cities, orders_unmatched ,time)\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            temp_Lower_Layer.get_decision()\n",
    "            temp_Lower_Layer.constrain_1()\n",
    "            temp_Lower_Layer.constrain_2()\n",
    "            temp_Lower_Layer.constrain_3()\n",
    "            temp_Lower_Layer.constrain_4()\n",
    "            temp_Lower_Layer.constrain_5()\n",
    "            temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "            total_penalty = cancel_penalty * order_canceled\n",
    "            temp_Lower_Layer.set_objective(matrix)\n",
    "        \n",
    "            temp_Lower_Layer.model.optimize()\n",
    "\n",
    "            if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                # save_results(temp_Lower_Layer,time)\n",
    "                # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                objval = temp_Lower_Layer.model.objVal \n",
    "            else:\n",
    "                temp_Lower_Layer.model.computeIIS()\n",
    "                temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                # print(f\"{time}次，No optimal solution found.\")\n",
    "                self_update(Vehicles, G)\n",
    "                objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                \n",
    "            \n",
    "            _, var_order = temp_Lower_Layer.get_decision()\n",
    "            update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "            vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =  objval + reward\n",
    "            grid_rewards.append(grid_reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # agent.save_model(save_path)\n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dirty_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
