{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带掩码的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为119282.0,58次未求解\n",
      "第1次训练\n",
      "执行时间: 1.8399901390075684 秒,50次未求解，当前强化学习值为262721.0,利润为262721.0\n",
      "第2次训练\n",
      "执行时间: 3.9947502613067627 秒,14次未求解，当前强化学习值为876163.0,利润为876163.0\n",
      "第3次训练\n",
      "执行时间: 5.620569705963135 秒,29次未求解，当前强化学习值为630302.0,利润为630302.0\n",
      "第4次训练\n",
      "执行时间: 8.040527582168579 秒,0次未求解，当前强化学习值为1137983.0,利润为1137983.0\n",
      "第5次训练\n",
      "执行时间: 9.552980184555054 秒,36次未求解，当前强化学习值为493615.0,利润为493615.0\n",
      "第6次训练\n",
      "执行时间: 11.84005880355835 秒,0次未求解，当前强化学习值为1119662.0,利润为1119662.0\n",
      "第7次训练\n",
      "执行时间: 13.321484804153442 秒,35次未求解，当前强化学习值为516251.0,利润为516251.0\n",
      "第8次训练\n",
      "执行时间: 15.030028820037842 秒,23次未求解，当前强化学习值为737416.0,利润为737416.0\n",
      "第9次训练\n",
      "执行时间: 17.870023250579834 秒,49次未求解，当前强化学习值为292569.0,利润为292569.0\n",
      "第10次训练\n",
      "执行时间: 20.277883291244507 秒,0次未求解，当前强化学习值为1122293.0,利润为1122293.0\n",
      "第11次训练\n",
      "执行时间: 21.34235954284668 秒,62次未求解，当前强化学习值为47739.0,利润为47739.0\n",
      "第12次训练\n",
      "执行时间: 23.63964319229126 秒,0次未求解，当前强化学习值为1129111.0,利润为1129111.0\n",
      "第13次训练\n",
      "执行时间: 24.700026035308838 秒,61次未求解，当前强化学习值为64000.0,利润为64000.0\n",
      "第14次训练\n",
      "执行时间: 25.829976558685303 秒,55次未求解，当前强化学习值为166136.0,利润为166136.0\n",
      "第15次训练\n",
      "执行时间: 26.759884119033813 秒,62次未求解，当前强化学习值为49557.0,利润为49557.0\n",
      "第16次训练\n",
      "执行时间: 27.697844743728638 秒,62次未求解，当前强化学习值为49557.0,利润为49557.0\n",
      "第17次训练\n",
      "执行时间: 29.32480239868164 秒,30次未求解，当前强化学习值为619153.0,利润为619153.0\n",
      "第18次训练\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action = agent.take_action_mask(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试版 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为333835.0,48次未求解\n",
      "第1次训练\n",
      "执行时间: 1.7765674591064453 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第2次训练\n",
      "执行时间: 2.8372347354888916 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第3次训练\n",
      "执行时间: 3.9000909328460693 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第4次训练\n",
      "执行时间: 4.9130539894104 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第5次训练\n",
      "执行时间: 5.973233938217163 秒,48次未求解，当前强化学习值为333835.0,利润为333835.0\n",
      "第6次训练\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.2500],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.1065, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.2500, 0.2500, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.0000, 0.1065, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.7870, 0.0000, 0.1065, 0.0000, 0.1065, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.0000, 0.3333],\n",
      "        [0.2500, 0.0000, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.3333],\n",
      "        [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'actions' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m     greedy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    189\u001b[0m mask \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_mask(orders_unmatched)\n\u001b[1;32m--> 190\u001b[0m action , logits \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# reward = env.test_step(orders_unmatched,action)\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03mCOUNT = 1000\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03mmax_reward = -999999\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m    reward = env.test_step(orders_unmatched, max_action)\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:174\u001b[0m, in \u001b[0;36mMultiAgentAC.take_action_mask\u001b[1;34m(self, vehicle_states, order_states, mask, explore, greedy)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    172\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask 全 0，导致 softmax 计算无意义！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactions\u001b[49m , logits\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'actions' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 30\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0\n",
    "            explore = False\n",
    "            \n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            \"\"\"\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "            \"\"\"\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action , logits = agent.take_action_mask(vehicle_states, order_states, mask, explore, greedy)\n",
    "            # reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            # grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward +=  objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        # save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        # torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试版2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "未加强化学习利润为200830.0,53次未求解\n",
      "第1次训练\n",
      "v_encoded 出现 NaN，输入状态可能异常！\n",
      "o_encoded 出现 NaN，输入状态可能异常！\n",
      "actor_input 出现 NaN，输入状态可能异常！\n",
      "global vehicle 出现 NaN，输入状态可能异常！\n",
      "repeated global 出现 NaN，输入状态可能异常！\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward0>)\n",
      "logits 出现 NaN，输入状态可能异常！\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'actions' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 187\u001b[0m\n\u001b[0;32m    185\u001b[0m     greedy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    186\u001b[0m mask \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_mask(orders_unmatched)\n\u001b[1;32m--> 187\u001b[0m action, logits \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action_mask_special\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mtest_step(orders_unmatched,action)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mCOUNT = 1000\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03mmax_reward = -999999\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    reward = env.test_step(orders_unmatched, max_action)\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:192\u001b[0m, in \u001b[0;36mMultiAgentAC.take_action_mask_special\u001b[1;34m(self, vehicle_states, order_states, mask, explore, greedy)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask 全 0，导致 softmax 计算无意义！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactions\u001b[49m , logits\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'actions' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                load_path = f\"rl_para/model_checkpoint_{best_model+1}.pth\"\n",
    "                agent = torch.load(load_path,map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid < invalid_time:\n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'rl_para/model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update_logtis(vehicle_states, order_states, logits,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            mask = env.get_mask(orders_unmatched)\n",
    "            action, logits = agent.take_action_mask_special(vehicle_states, order_states, mask, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "         \n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例1 burn in后探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAIAHyMAAAAAAhFYoQQAAAADYZA9BAAAAAACcwsAAAAAAwT4yQQAAAAAgHPVAAAAAAIAHyMAAAAAAUkMtQQAAAAAAnMLAAAAAAACcwsAAAAAAAnAvQQAAAADcnitBAAAAAIAHyMAAAAAAQNfzQAAAAAAEriRBAAAAAFxuL0EAAAAAeiwsQQAAAAAgXvRAAAAAAACcwsAAAAAAj3UyQQAAAAAAnMLAAAAAAACcwsAAAAAAAJzCwAAAAABkUiRBAAAAAKiGKkEAAAAAgAfIwAAAAACAB8jAAAAAAAdzMkEAAAAAAJzCwAAAAAB8LBFBAAAAAJydGUEAAAAAgAfIwAAAAACAB8jAAAAAAIAHyMAAAAAARVIyQQAAAAAAnMLAAAAAAGBf9EAAAAAAoMceQQAAAACAB8jAAAAAAACcwsAAAAAAgAfIwAAAAAAAnMLAAAAAAHQ7HEEAAAAAI0wyQQAAAAAAnMLAAAAAAIAHyMAAAAAAgAfIwAAAAACAB8jAAAAAAP76LEEAAAAAOvooQQAAAACAB8jAAAAAAAQjMUEAAAAAgAfIwAAAAACAB8jAAAAAALCg80AAAAAAOCsoQQAAAACAB8jAAAAAAACcwsAAAAAAAJzCwAAAAAAY7ydBAAAAAACcwsAAAAAAgigyQQAAAAAAnMLAAAAAAACcwsAAAAAAAJzCwAAAAACkex9BAAAAAOy3EkEAAAAAAJzCwAAAAAAAnMLAAAAAAIAHyMAAAAAAAJzCwAAAAACAB8jAAAAAAIAHyMAAAAAAAJzCwAAAAAA0DBpBAAAAAIylLUEAAAAAgAfIwAAAAAAQPhpBAAAAABKAIUEAAAAA9NElQQAAAACAB8jAAAAAAIAHyMAAAAAAgAfIwAAAAADQ5SdBAAAAACBe9EAAAAAAAJzCwAAAAACAB8jAAAAAAIAHyMAAAAAAdEwcQQAAAADc4x9BAAAAALCTEkEAAAAAapAhQQAAAADQS/VAAAAAAIxfMkEAAAAAyDQyQQAAAACAB8jAAAAAACA/MkEAAAAAAJT0QAAAAADm+y1BAAAAALxCMkEAAAAAGDUSQQAAAAA1RDJBAAAAAIAHyMAAAAAAKMAuQQAAAADQABNBAAAAAMRFH0EAAAAAtsggQQAAAACCCSxBAAAAAIAHyMAAAAAAgAfIwAAAAACAB8jAAAAAAIAHyMAAAAAA/GcyQQAAAACAB8jAAAAAAMhLDUEAAAAAxGoUQQAAAABSTTFBAAAAALKoMUEAAAAAXKMkQQAAAAB2GjJBAAAAAMAB/UAAAAAAgAfIwAAAAADBHzJBAAAAAN6TLkEAAAAAfLMpQQAAAACYigxBAAAAACTpE0EAAAAA8DcPQQAAAACAB8jAAAAAAOw3HkEAAAAAvGElQQAAAAD4YTJBAAAAAN7DLkEAAAAADPwRQQAAAACAB8jAAAAAAECN/EAAAAAAwKokQQAAAAC2IClBAAAAAIAHyMAAAAAA0GEfQQAAAADokBtBAAAAAF9cMkEAAAAA2MkSQQAAAAA9tjBBAAAAAFpLIkEAAAAAgAfIwAAAAACAB8jAAAAAABxlHEEAAAAAAEMyQQAAAABIAAxBAAAAAMyzMUEAAAAAgAfIwAAAAABefypBAAAAAIAHyMAAAAAAsKkjQQAAAADIQS9BAAAAAIAHyMAAAAAAyEsHQQAAAACAB8jAAAAAAIqRJEEAAAAAgAfIwAAAAABAjfxAAAAAAOJUKEEAAAAAxO4SQQAAAADwDDJBAAAAABNXMkEAAAAABPkhQQAAAACDQTJBAAAAAPaDIUEAAAAABgoyQQAAAACwsRJBAAAAACBAMkEAAAAAKK0RQQAAAAA4CBpBAAAAAB87MkEAAAAAFD8jQQAAAADgYhBBAAAAAJAOG0EAAAAAgAfIwAAAAACsdBJBAAAAAMJhMkEAAAAA+CQIQQAAAACAB8jAAAAAAIAHyMAAAAAASEgrQQAAAABs+i1BAAAAAB82MkEAAAAAgAfIwAAAAADu5SFBAAAAAOwvJ0EAAAAAkoshQQAAAABPwDFBAAAAAMjMJUEAAAAAeJcTQQAAAACAB8jAAAAAAIAHyMAAAAAAJMoxQQAAAAAQqhlBAAAAANYNMkEAAAAAILf9QAAAAACAB8jAAAAAABptKEEAAAAAoAoUQQAAAAAXEzJBAAAAAIAHyMAAAAAA0OMtQQAAAACk1ClBAAAAAIpcIEEAAAAAgAfIwAAAAACI1xpBAAAAAPTsIUEAAAAAYF8lQQAAAACAB8jAAAAAAGgrDkEAAAAAyU8yQQAAAACIoA9BAAAAAIAHyMAAAAAA9PAcQQAAAACAB8jAAAAAAHglL0EAAAAAgAfIwAAAAAAgJyBBAAAAADq/LkEAAAAAgAfIwAAAAAA40SFBAAAAAFgxKkEAAAAAxEYyQQAAAAAwbjJBAAAAAIAHyMAAAAAAOoEqQQAAAAAAPxxBAAAAAKidB0EAAAAAC2sxQQAAAADoJwpBAAAAAMBFLkEAAAAAgAfIwAAAAACAB8jAAAAAAIAHyMAAAAAA/JkWQQAAAAB8nBlBAAAAAIgmMUEAAAAA8IsVQQAAAADqLyBBAAAAAIAHyMAAAAAAOpIqQQAAAACobhxBAAAAAIAHyMAAAAAAO1syQQAAAABAuBJBAAAAAC5PK0EAAAAAdAUgQQAAAAAYoBhBAAAAAIAHyMAAAAAAwwMyQQAAAACAB8jAAAAAACiwDEEAAAAAiKIbQQAAAAAcYClBAAAAAHk+MkEAAAAAhKkRQQAAAADIaRFBAAAAAKAnIkEAAAAAUI38QAAAAACAB8jAAAAAANaoL0EAAAAAaGEyQQAAAACsBBlBAAAAAE7XIEEAAAAAgAfIwAAAAABP8DFBAAAAANNMMkEAAAAAimYuQQAAAADAZzJBAAAAAH0WMEEAAAAAFMkhQQAAAAB2TCFBAAAAAKxtHUEAAAAAgAfIwAAAAACnHDBBAAAAAO5cJUEAAAAAw2syQQAAAACAB8jAAAAAABBMDUEAAAAAkEAMQQAAAAAUEDFBAAAAAIAHyMAAAAAAgAfIwAAAAACm/yhBAAAAAHjHGkEAAAAAsF8SQQAAAADmgSRBAAAAAIAHyMAAAAAAgAfIwAAAAACwMhRBAAAAAFRWMkEAAAAAgAfIwAAAAABcbRJBAAAAANIOIEEAAAAAcF0gQQ==",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例2 burn in后不探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例3 更新条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带跳过的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为351240.0,48次未求解\n",
      "第1次训练\n",
      "执行时间: 1.5951976776123047 秒,66次未求解，当前强化学习值为-316088.0,利润为-1388.0\n",
      "第2次训练\n",
      "执行时间: 2.8220858573913574 秒,40次未求解，当前强化学习值为255049.0,利润为471649.0\n",
      "第3次训练\n",
      "执行时间: 3.92783522605896 秒,48次未求解，当前强化学习值为76507.0,利润为333107.0\n",
      "第4次训练\n",
      "执行时间: 4.801097393035889 秒,60次未求解，当前强化学习值为-187197.0,利润为108603.0\n",
      "第5次训练\n",
      "执行时间: 6.016491651535034 秒,45次未求解，当前强化学习值为168199.0,利润为405999.0\n",
      "第6次训练\n",
      "执行时间: 6.894177436828613 秒,60次未求解，当前强化学习值为-183913.0,利润为112087.0\n",
      "第7次训练\n",
      "执行时间: 7.8495588302612305 秒,60次未求解，当前强化学习值为-186198.0,利润为108602.0\n",
      "第8次训练\n",
      "执行时间: 9.370670318603516 秒,43次未求解，当前强化学习值为195294.0,利润为423494.0\n",
      "第9次训练\n",
      "执行时间: 10.391007423400879 秒,59次未求解，当前强化学习值为-181762.0,利润为112638.0\n",
      "第10次训练\n",
      "执行时间: 11.553189754486084 秒,57次未求解，当前强化学习值为-102270.0,利润为178130.0\n",
      "第11次训练\n",
      "执行时间: 13.124795198440552 秒,47次未求解，当前强化学习值为114564.0,利润为361264.0\n",
      "第12次训练\n",
      "执行时间: 14.306801080703735 秒,60次未求解，当前强化学习值为-180413.0,利润为112087.0\n",
      "第13次训练\n",
      "执行时间: 15.57046389579773 秒,48次未求解，当前强化学习值为68873.0,利润为326673.0\n",
      "第14次训练\n",
      "执行时间: 16.64451813697815 秒,57次未求解，当前强化学习值为-110057.0,利润为170643.0\n",
      "第15次训练\n",
      "执行时间: 17.640617847442627 秒,59次未求解，当前强化学习值为-159380.0,利润为131420.0\n",
      "第16次训练\n",
      "执行时间: 18.647897958755493 秒,59次未求解，当前强化学习值为-168057.0,利润为127243.0\n",
      "第17次训练\n",
      "执行时间: 20.608492374420166 秒,28次未求解，当前强化学习值为534251.0,利润为696851.0\n",
      "第18次训练\n",
      "执行时间: 21.51712131500244 秒,60次未求解，当前强化学习值为-190784.0,利润为105716.0\n",
      "第19次训练\n",
      "执行时间: 22.778335571289062 秒,48次未求解，当前强化学习值为80027.0,利润为333427.0\n",
      "第20次训练\n",
      "执行时间: 23.744807720184326 秒,60次未求解，当前强化学习值为-184613.0,利润为112087.0\n",
      "第21次训练\n",
      "执行时间: 24.99462080001831 秒,51次未求解，当前强化学习值为23549.0,利润为288049.0\n",
      "第22次训练\n",
      "执行时间: 25.945756196975708 秒,60次未求解，当前强化学习值为-187566.0,利润为107234.0\n",
      "第23次训练\n",
      "执行时间: 27.99422001838684 秒,23次未求解，当前强化学习值为649226.0,利润为800126.0\n",
      "第24次训练\n",
      "执行时间: 29.752153396606445 秒,32次未求解，当前强化学习值为436038.0,利润为620738.0\n",
      "第25次训练\n",
      "执行时间: 31.028327465057373 秒,47次未求解，当前强化学习值为129301.0,利润为373001.0\n",
      "第26次训练\n",
      "执行时间: 32.439204454422 秒,45次未求解，当前强化学习值为158061.0,利润为394861.0\n",
      "第27次训练\n",
      "执行时间: 33.439168214797974 秒,57次未求解，当前强化学习值为-118950.0,利润为164950.0\n",
      "第28次训练\n",
      "执行时间: 34.552332639694214 秒,54次未求解，当前强化学习值为-59392.0,利润为217108.0\n",
      "第29次训练\n",
      "执行时间: 35.700438261032104 秒,54次未求解，当前强化学习值为-63310.0,利润为215190.0\n",
      "第30次训练\n",
      "执行时间: 36.633397817611694 秒,60次未求解，当前强化学习值为-187284.0,利润为105716.0\n",
      "第31次训练\n",
      "执行时间: 38.67721676826477 秒,22次未求解，当前强化学习值为680635.0,利润为824735.0\n",
      "第32次训练\n",
      "执行时间: 39.634028673172 秒,60次未求解，当前强化学习值为-187925.0,利润为108375.0\n",
      "第33次训练\n",
      "执行时间: 41.42440843582153 秒,35次未求解，当前强化学习值为369561.0,利润为567161.0\n",
      "第34次训练\n",
      "执行时间: 42.37771916389465 秒,60次未求解，当前强化学习值为-193384.0,利润为105716.0\n",
      "第35次训练\n",
      "执行时间: 43.3869833946228 秒,58次未求解，当前强化学习值为-129827.0,利润为155773.0\n",
      "第36次训练\n",
      "执行时间: 45.71844553947449 秒,11次未求解，当前强化学习值为902113.0,利润为1014313.0\n",
      "第37次训练\n",
      "执行时间: 46.916133403778076 秒,50次未求解，当前强化学习值为43715.0,利润为306115.0\n",
      "第38次训练\n",
      "执行时间: 48.783586502075195 秒,28次未求解，当前强化学习值为548164.0,利润为711764.0\n",
      "第39次训练\n",
      "执行时间: 49.75050187110901 秒,60次未求解，当前强化学习值为-191724.0,利润为108376.0\n",
      "第40次训练\n",
      "执行时间: 51.13849878311157 秒,43次未求解，当前强化学习值为195591.0,利润为421091.0\n",
      "第41次训练\n",
      "执行时间: 52.136146783828735 秒,57次未求解，当前强化学习值为-116542.0,利润为164658.0\n",
      "第42次训练\n",
      "执行时间: 53.34069466590881 秒,51次未求解，当前强化学习值为22358.0,利润为286358.0\n",
      "第43次训练\n",
      "执行时间: 54.45970129966736 秒,53次未求解，当前强化学习值为-46247.0,利润为229853.0\n",
      "第44次训练\n",
      "执行时间: 55.403913497924805 秒,60次未求解，当前强化学习值为-184125.0,利润为108375.0\n",
      "第45次训练\n",
      "执行时间: 56.32914471626282 秒,60次未求解，当前强化学习值为-194084.0,利润为105716.0\n",
      "第46次训练\n",
      "执行时间: 57.52802777290344 秒,50次未求解，当前强化学习值为22843.0,利润为285343.0\n",
      "第47次训练\n",
      "执行时间: 59.19414758682251 秒,34次未求解，当前强化学习值为410426.0,利润为602826.0\n",
      "第48次训练\n",
      "执行时间: 60.40188717842102 秒,50次未求解，当前强化学习值为54377.0,利润为313777.0\n",
      "第49次训练\n",
      "执行时间: 62.1056182384491 秒,36次未求解，当前强化学习值为353559.0,利润为554559.0\n",
      "第50次训练\n",
      "执行时间: 63.049800157547 秒,60次未求解，当前强化学习值为-187625.0,利润为108375.0\n",
      "第51次训练\n",
      "执行时间: 64.5289478302002 秒,41次未求解，当前强化学习值为236401.0,利润为457601.0\n",
      "第52次训练\n",
      "执行时间: 65.51738786697388 秒,58次未求解，当前强化学习值为-135383.0,利润为147517.0\n",
      "第53次训练\n",
      "执行时间: 67.07062864303589 秒,38次未求解，当前强化学习值为285800.0,利润为499800.0\n",
      "第54次训练\n",
      "执行时间: 68.92661738395691 秒,31次未求解，当前强化学习值为477191.0,利润为654391.0\n",
      "第55次训练\n",
      "执行时间: 70.24348044395447 秒,48次未求解，当前强化学习值为86877.0,利润为339677.0\n",
      "第56次训练\n",
      "执行时间: 71.25049686431885 秒,60次未求解，当前强化学习值为-189797.0,利润为108603.0\n",
      "第57次训练\n",
      "执行时间: 72.19648313522339 秒,60次未求解，当前强化学习值为-186590.0,利润为112310.0\n",
      "第58次训练\n",
      "执行时间: 73.2587080001831 秒,55次未求解，当前强化学习值为-77222.0,利润为201478.0\n",
      "第59次训练\n",
      "执行时间: 74.27636647224426 秒,58次未求解，当前强化学习值为-131425.0,利润为155775.0\n",
      "第60次训练\n",
      "执行时间: 75.9955050945282 秒,32次未求解，当前强化学习值为424336.0,利润为609836.0\n",
      "第61次训练\n",
      "执行时间: 77.16693782806396 秒,54次未求解，当前强化学习值为-50641.0,利润为226659.0\n",
      "第62次训练\n",
      "执行时间: 78.11392188072205 秒,60次未求解，当前强化学习值为-188324.0,利润为108376.0\n",
      "第63次训练\n",
      "执行时间: 79.20863389968872 秒,57次未求解，当前强化学习值为-113257.0,利润为170643.0\n",
      "第64次训练\n",
      "执行时间: 80.21722507476807 秒,58次未求解，当前强化学习值为-134936.0,利润为150664.0\n",
      "第65次训练\n",
      "执行时间: 81.21145439147949 秒,58次未求解，当前强化学习值为-133954.0,利润为149646.0\n",
      "第66次训练\n",
      "执行时间: 82.88465523719788 秒,35次未求解，当前强化学习值为388026.0,利润为581726.0\n",
      "第67次训练\n",
      "执行时间: 83.94495129585266 秒,55次未求解，当前强化学习值为-82790.0,利润为194710.0\n",
      "第68次训练\n",
      "执行时间: 84.91909193992615 秒,60次未求解，当前强化学习值为-189397.0,利润为108603.0\n",
      "第69次训练\n",
      "执行时间: 86.38045382499695 秒,41次未求解，当前强化学习值为235517.0,利润为455017.0\n",
      "第70次训练\n",
      "执行时间: 87.39861249923706 秒,60次未求解，当前强化学习值为-183013.0,利润为112087.0\n",
      "第71次训练\n",
      "执行时间: 88.33405995368958 秒,59次未求解，当前强化学习值为-181862.0,利润为112638.0\n",
      "第72次训练\n",
      "执行时间: 89.58133697509766 秒,51次未求解，当前强化学习值为19955.0,利润为288055.0\n",
      "第73次训练\n",
      "执行时间: 91.56137347221375 秒,23次未求解，当前强化学习值为667000.0,利润为816100.0\n",
      "第74次训练\n",
      "执行时间: 92.61986470222473 秒,58次未求解，当前强化学习值为-135307.0,利润为152993.0\n",
      "第75次训练\n",
      "执行时间: 93.57529735565186 秒,60次未求解，当前强化学习值为-189684.0,利润为105716.0\n",
      "第76次训练\n",
      "执行时间: 94.529616355896 秒,60次未求解，当前强化学习值为-188425.0,利润为108375.0\n",
      "第77次训练\n",
      "执行时间: 95.49549078941345 秒,60次未求解，当前强化学习值为-187397.0,利润为108603.0\n",
      "第78次训练\n",
      "执行时间: 96.7206883430481 秒,47次未求解，当前强化学习值为87333.0,利润为338433.0\n",
      "第79次训练\n",
      "执行时间: 97.80291867256165 秒,55次未求解，当前强化学习值为-72993.0,利润为206507.0\n",
      "第80次训练\n",
      "执行时间: 98.76064229011536 秒,60次未求解，当前强化学习值为-185213.0,利润为112087.0\n",
      "第81次训练\n",
      "执行时间: 99.71186685562134 秒,60次未求解，当前强化学习值为-188866.0,利润为107834.0\n",
      "第82次训练\n",
      "执行时间: 101.1367130279541 秒,44次未求解，当前强化学习值为168940.0,利润为403940.0\n",
      "第83次训练\n",
      "执行时间: 102.72883558273315 秒,37次未求解，当前强化学习值为302257.0,利润为513357.0\n",
      "第84次训练\n",
      "执行时间: 103.68984532356262 秒,60次未求解，当前强化学习值为-185397.0,利润为108603.0\n",
      "第85次训练\n",
      "执行时间: 104.7415132522583 秒,60次未求解，当前强化学习值为-187524.0,利润为108376.0\n",
      "第86次训练\n",
      "执行时间: 105.96233081817627 秒,51次未求解，当前强化学习值为35023.0,利润为293323.0\n",
      "第87次训练\n",
      "执行时间: 106.89941883087158 秒,60次未求解，当前强化学习值为-185897.0,利润为108603.0\n",
      "第88次训练\n",
      "执行时间: 107.8716242313385 秒,60次未求解，当前强化学习值为-186790.0,利润为112310.0\n",
      "第89次训练\n",
      "执行时间: 108.86753177642822 秒,60次未求解，当前强化学习值为-189598.0,利润为108602.0\n",
      "第90次训练\n",
      "执行时间: 110.29196691513062 秒,43次未求解，当前强化学习值为176044.0,利润为412544.0\n",
      "第91次训练\n",
      "执行时间: 111.2357029914856 秒,60次未求解，当前强化学习值为-187525.0,利润为108375.0\n",
      "第92次训练\n",
      "执行时间: 112.51746869087219 秒,50次未求解，当前强化学习值为55062.0,利润为308862.0\n",
      "第93次训练\n",
      "执行时间: 113.96824717521667 秒,43次未求解，当前强化学习值为179440.0,利润为413340.0\n",
      "第94次训练\n",
      "执行时间: 115.20708656311035 秒,54次未求解，当前强化学习值为-46443.0,利润为228857.0\n",
      "第95次训练\n",
      "执行时间: 116.19106864929199 秒,60次未求解，当前强化学习值为-191197.0,利润为108603.0\n",
      "第96次训练\n",
      "执行时间: 117.2844717502594 秒,54次未求解，当前强化学习值为-49093.0,利润为226707.0\n",
      "第97次训练\n",
      "执行时间: 118.26419067382812 秒,60次未求解，当前强化学习值为-188684.0,利润为105716.0\n",
      "第98次训练\n",
      "执行时间: 119.45181608200073 秒,51次未求解，当前强化学习值为25755.0,利润为288455.0\n",
      "第99次训练\n",
      "执行时间: 120.3560893535614 秒,60次未求解，当前强化学习值为-184166.0,利润为107834.0\n",
      "第100次训练\n",
      "最优模型为35\n",
      "执行时间: 121.3204836845398 秒,60次未求解，当前强化学习值为-196920.0,利润为104480.0\n",
      "第101次训练\n",
      "最优模型为35\n",
      "执行时间: 122.23788475990295 秒,60次未求解，当前强化学习值为-192184.0,利润为105716.0\n",
      "第102次训练\n",
      "最优模型为35\n",
      "执行时间: 123.19744944572449 秒,60次未求解，当前强化学习值为-185214.0,利润为112086.0\n",
      "第103次训练\n",
      "最优模型为35\n",
      "执行时间: 124.13381385803223 秒,60次未求解，当前强化学习值为-191384.0,利润为105716.0\n",
      "第104次训练\n",
      "最优模型为35\n",
      "执行时间: 125.08623027801514 秒,60次未求解，当前强化学习值为-186925.0,利润为108375.0\n",
      "第105次训练\n",
      "最优模型为35\n",
      "执行时间: 126.30799269676208 秒,48次未求解，当前强化学习值为79958.0,利润为334558.0\n",
      "第106次训练\n",
      "最优模型为35\n",
      "执行时间: 128.32657599449158 秒,23次未求解，当前强化学习值为639598.0,利润为791798.0\n",
      "第107次训练\n",
      "最优模型为35\n",
      "执行时间: 129.66857504844666 秒,47次未求解，当前强化学习值为98970.99993335613,利润为357870.9999333561\n",
      "第108次训练\n",
      "最优模型为35\n",
      "执行时间: 130.82310009002686 秒,58次未求解，当前强化学习值为-143817.0,利润为146083.0\n",
      "第109次训练\n",
      "最优模型为35\n",
      "执行时间: 131.8901650905609 秒,58次未求解，当前强化学习值为-136629.0,利润为152071.0\n",
      "第110次训练\n",
      "最优模型为35\n",
      "执行时间: 134.40908408164978 秒,7次未求解，当前强化学习值为983056.0,利润为1087856.0\n",
      "第111次训练\n",
      "最优模型为35\n",
      "执行时间: 135.56054759025574 秒,58次未求解，当前强化学习值为-137837.0,利润为150663.0\n",
      "第112次训练\n",
      "最优模型为35\n",
      "执行时间: 136.64506363868713 秒,60次未求解，当前强化学习值为-186825.0,利润为108375.0\n",
      "第113次训练\n",
      "最优模型为35\n",
      "执行时间: 137.77993845939636 秒,54次未求解，当前强化学习值为-60230.0,利润为219370.0\n",
      "第114次训练\n",
      "最优模型为35\n",
      "执行时间: 139.00611233711243 秒,59次未求解，当前强化学习值为-165156.0,利润为127244.0\n",
      "第115次训练\n",
      "最优模型为35\n",
      "执行时间: 140.6410892009735 秒,48次未求解，当前强化学习值为86041.0,利润为339341.0\n",
      "第116次训练\n",
      "最优模型为35\n",
      "执行时间: 142.40051746368408 秒,35次未求解，当前强化学习值为376823.0,利润为571523.0\n",
      "第117次训练\n",
      "最优模型为35\n",
      "执行时间: 143.57704281806946 秒,53次未求解，当前强化学习值为-48617.0,利润为224883.0\n",
      "第118次训练\n",
      "最优模型为35\n",
      "执行时间: 145.75464940071106 秒,21次未求解，当前强化学习值为670918.0,利润为821018.0\n",
      "第119次训练\n",
      "最优模型为35\n",
      "执行时间: 147.02079129219055 秒,50次未求解，当前强化学习值为50265.0,利润为305165.0\n",
      "第120次训练\n",
      "最优模型为35\n",
      "执行时间: 148.03165912628174 秒,60次未求解，当前强化学习值为-191624.0,利润为108376.0\n",
      "第121次训练\n",
      "最优模型为35\n",
      "执行时间: 149.89130783081055 秒,27次未求解，当前强化学习值为552953.0,利润为719653.0\n",
      "第122次训练\n",
      "最优模型为35\n",
      "执行时间: 151.05049443244934 秒,45次未求解，当前强化学习值为160090.0,利润为396590.0\n",
      "第123次训练\n",
      "最优模型为35\n",
      "执行时间: 151.98367047309875 秒,60次未求解，当前强化学习值为-191197.0,利润为108603.0\n",
      "第124次训练\n",
      "最优模型为35\n",
      "执行时间: 152.91549134254456 秒,58次未求解，当前强化学习值为-134437.0,利润为150663.0\n",
      "第125次训练\n",
      "最优模型为35\n",
      "执行时间: 154.26281690597534 秒,38次未求解，当前强化学习值为302843.0,利润为512143.0\n",
      "第126次训练\n",
      "最优模型为35\n",
      "执行时间: 155.22281908988953 秒,58次未求解，当前强化学习值为-136437.0,利润为150663.0\n",
      "第127次训练\n",
      "最优模型为35\n",
      "执行时间: 156.25933933258057 秒,52次未求解，当前强化学习值为-13601.0,利润为256899.0\n",
      "第128次训练\n",
      "最优模型为35\n",
      "执行时间: 157.1459345817566 秒,58次未求解，当前强化学习值为-143435.0,利润为143665.0\n",
      "第129次训练\n",
      "最优模型为35\n",
      "执行时间: 158.03922820091248 秒,60次未求解，当前强化学习值为-187925.0,利润为108375.0\n",
      "第130次训练\n",
      "最优模型为35\n",
      "执行时间: 158.92097401618958 秒,60次未求解，当前强化学习值为-193284.0,利润为105716.0\n",
      "第131次训练\n",
      "最优模型为35\n",
      "执行时间: 159.84232568740845 秒,60次未求解，当前强化学习值为-185489.0,利润为112311.0\n",
      "第132次训练\n",
      "最优模型为35\n",
      "执行时间: 160.9287087917328 秒,54次未求解，当前强化学习值为-55947.0,利润为224053.0\n",
      "第133次训练\n",
      "最优模型为35\n",
      "执行时间: 162.09905314445496 秒,51次未求解，当前强化学习值为27708.0,利润为290308.0\n",
      "第134次训练\n",
      "最优模型为35\n",
      "执行时间: 163.93159818649292 秒,18次未求解，当前强化学习值为766780.0,利润为900680.0\n",
      "第135次训练\n",
      "最优模型为35\n",
      "执行时间: 164.9489631652832 秒,60次未求解，当前强化学习值为-187066.0,利润为107834.0\n",
      "第136次训练\n",
      "最优模型为35\n",
      "执行时间: 166.57515454292297 秒,37次未求解，当前强化学习值为310584.0,利润为519484.0\n",
      "第137次训练\n",
      "最优模型为35\n",
      "执行时间: 167.5441632270813 秒,60次未求解，当前强化学习值为-183913.0,利润为112087.0\n",
      "第138次训练\n",
      "最优模型为35\n",
      "执行时间: 168.517085313797 秒,60次未求解，当前强化学习值为-181515.0,利润为112085.0\n",
      "第139次训练\n",
      "最优模型为35\n",
      "执行时间: 169.47752785682678 秒,60次未求解，当前强化学习值为-189025.0,利润为108375.0\n",
      "第140次训练\n",
      "最优模型为35\n",
      "执行时间: 170.4704098701477 秒,58次未求解，当前强化学习值为-145145.0,利润为143955.0\n",
      "第141次训练\n",
      "最优模型为35\n",
      "执行时间: 171.61494302749634 秒,57次未求解，当前强化学习值为-123261.0,利润为163339.0\n",
      "第142次训练\n",
      "最优模型为35\n",
      "执行时间: 172.6714940071106 秒,56次未求解，当前强化学习值为-82394.0,利润为196206.0\n",
      "第143次训练\n",
      "最优模型为35\n",
      "执行时间: 174.24093580245972 秒,40次未求解，当前强化学习值为265885.0,利润为482185.0\n",
      "第144次训练\n",
      "最优模型为35\n",
      "执行时间: 175.2280457019806 秒,59次未求解，当前强化学习值为-164656.0,利润为127244.0\n",
      "第145次训练\n",
      "最优模型为35\n",
      "执行时间: 177.02452993392944 秒,30次未求解，当前强化学习值为494887.0,利润为671987.0\n",
      "第146次训练\n",
      "最优模型为35\n",
      "执行时间: 178.81011962890625 秒,32次未求解，当前强化学习值为454655.0,利润为637055.0\n",
      "第147次训练\n",
      "最优模型为35\n",
      "执行时间: 179.76635479927063 秒,60次未求解，当前强化学习值为-184214.0,利润为112086.0\n",
      "第148次训练\n",
      "最优模型为35\n",
      "执行时间: 180.75130462646484 秒,60次未求解，当前强化学习值为-184889.0,利润为112311.0\n",
      "第149次训练\n",
      "最优模型为35\n",
      "执行时间: 181.89295649528503 秒,51次未求解，当前强化学习值为17677.0,利润为282877.0\n",
      "第150次训练\n",
      "最优模型为35\n",
      "执行时间: 182.9985318183899 秒,54次未求解，当前强化学习值为-56781.0,利润为219019.0\n",
      "第151次训练\n",
      "最优模型为35\n",
      "执行时间: 184.2939157485962 秒,48次未求解，当前强化学习值为107515.0,利润为354815.0\n",
      "第152次训练\n",
      "最优模型为35\n",
      "执行时间: 185.66520404815674 秒,43次未求解，当前强化学习值为196426.0,利润为428626.0\n",
      "第153次训练\n",
      "最优模型为35\n",
      "执行时间: 186.6812460422516 秒,58次未求解，当前强化学习值为-137489.0,利润为147511.0\n",
      "第154次训练\n",
      "最优模型为35\n",
      "执行时间: 187.61258220672607 秒,60次未求解，当前强化学习值为-192720.0,利润为104480.0\n",
      "第155次训练\n",
      "最优模型为35\n",
      "执行时间: 188.56989240646362 秒,60次未求解，当前强化学习值为-195420.0,利润为104480.0\n",
      "第156次训练\n",
      "最优模型为35\n",
      "执行时间: 189.5723340511322 秒,58次未求解，当前强化学习值为-135482.0,利润为147518.0\n",
      "第157次训练\n",
      "最优模型为35\n",
      "执行时间: 190.57703518867493 秒,59次未求解，当前强化学习值为-162056.0,利润为127244.0\n",
      "第158次训练\n",
      "最优模型为35\n",
      "执行时间: 191.57029938697815 秒,58次未求解，当前强化学习值为-131497.0,利润为151303.0\n",
      "第159次训练\n",
      "最优模型为35\n",
      "执行时间: 193.18104481697083 秒,37次未求解，当前强化学习值为314272.0,利润为528572.0\n",
      "第160次训练\n",
      "最优模型为35\n",
      "执行时间: 194.1245415210724 秒,60次未求解，当前强化学习值为-192543.0,利润为104257.0\n",
      "第161次训练\n",
      "最优模型为35\n",
      "执行时间: 196.5143163204193 秒,9次未求解，当前强化学习值为990179.0,利润为1085879.0\n",
      "第162次训练\n",
      "最优模型为35\n",
      "执行时间: 197.64509797096252 秒,58次未求解，当前强化学习值为-129827.0,利润为155773.0\n",
      "第163次训练\n",
      "最优模型为35\n",
      "执行时间: 198.55867409706116 秒,60次未求解，当前强化学习值为-183925.0,利润为108375.0\n",
      "第164次训练\n",
      "最优模型为35\n",
      "执行时间: 199.6782088279724 秒,58次未求解，当前强化学习值为-146245.0,利润为143955.0\n",
      "第165次训练\n",
      "最优模型为35\n",
      "执行时间: 200.66531991958618 秒,59次未求解，当前强化学习值为-166653.0,利润为127247.0\n",
      "第166次训练\n",
      "最优模型为35\n",
      "执行时间: 201.62729954719543 秒,60次未求解，当前强化学习值为-186589.0,利润为112311.0\n",
      "第167次训练\n",
      "最优模型为35\n",
      "执行时间: 202.84313583374023 秒,51次未求解，当前强化学习值为36776.0,利润为297976.0\n",
      "第168次训练\n",
      "最优模型为35\n",
      "执行时间: 203.75165796279907 秒,60次未求解，当前强化学习值为-185115.0,利润为112085.0\n",
      "第169次训练\n",
      "最优模型为35\n",
      "执行时间: 205.69684290885925 秒,6次未求解，当前强化学习值为1047551.0,利润为1136751.0\n",
      "第170次训练\n",
      "最优模型为35\n",
      "执行时间: 206.5575942993164 秒,60次未求解，当前强化学习值为-188198.0,利润为108602.0\n",
      "第171次训练\n",
      "最优模型为35\n",
      "执行时间: 207.49178290367126 秒,58次未求解，当前强化学习值为-148817.0,利润为146083.0\n",
      "第172次训练\n",
      "最优模型为35\n",
      "执行时间: 208.49127411842346 秒,60次未求解，当前强化学习值为-189425.0,利润为108375.0\n",
      "第173次训练\n",
      "最优模型为35\n",
      "执行时间: 209.3841781616211 秒,58次未求解，当前强化学习值为-139403.0,利润为152997.0\n",
      "第174次训练\n",
      "最优模型为35\n",
      "执行时间: 210.64575910568237 秒,43次未求解，当前强化学习值为169834.0,利润为408134.0\n",
      "第175次训练\n",
      "最优模型为35\n",
      "执行时间: 211.51969575881958 秒,60次未求解，当前强化学习值为-193766.0,利润为107234.0\n",
      "第176次训练\n",
      "最优模型为35\n",
      "执行时间: 212.39733839035034 秒,58次未求解，当前强化学习值为-138903.0,利润为145397.0\n",
      "第177次训练\n",
      "最优模型为35\n",
      "执行时间: 213.3943166732788 秒,54次未求解，当前强化学习值为-56505.0,利润为221195.0\n",
      "第178次训练\n",
      "最优模型为35\n",
      "执行时间: 214.3564896583557 秒,54次未求解，当前强化学习值为-58473.0,利润为222027.0\n",
      "第179次训练\n",
      "最优模型为35\n",
      "执行时间: 215.46778202056885 秒,45次未求解，当前强化学习值为152563.0,利润为390763.0\n",
      "第180次训练\n",
      "最优模型为35\n",
      "执行时间: 216.35304832458496 秒,60次未求解，当前强化学习值为-190566.0,利润为107834.0\n",
      "第181次训练\n",
      "最优模型为35\n",
      "执行时间: 217.1901683807373 秒,60次未求解，当前强化学习值为-189566.0,利润为107834.0\n",
      "第182次训练\n",
      "最优模型为35\n",
      "执行时间: 218.28790140151978 秒,47次未求解，当前强化学习值为102841.0,利润为351441.0\n",
      "第183次训练\n",
      "最优模型为35\n",
      "执行时间: 219.1589617729187 秒,60次未求解，当前强化学习值为-186188.0,利润为112312.0\n",
      "第184次训练\n",
      "最优模型为35\n",
      "执行时间: 220.04464960098267 秒,58次未求解，当前强化学习值为-137583.0,利润为149517.0\n",
      "第185次训练\n",
      "最优模型为35\n",
      "执行时间: 220.87177443504333 秒,60次未求解，当前强化学习值为-185797.0,利润为108603.0\n",
      "第186次训练\n",
      "最优模型为35\n",
      "执行时间: 221.76338267326355 秒,58次未求解，当前强化学习值为-136153.0,利润为149647.0\n",
      "第187次训练\n",
      "最优模型为35\n",
      "执行时间: 222.57552814483643 秒,60次未求解，当前强化学习值为-195184.0,利润为105716.0\n",
      "第188次训练\n",
      "最优模型为35\n",
      "执行时间: 223.44258451461792 秒,59次未求解，当前强化学习值为-161479.0,利润为129321.0\n",
      "第189次训练\n",
      "最优模型为35\n",
      "执行时间: 224.59621119499207 秒,43次未求解，当前强化学习值为178706.0,利润为416006.0\n",
      "第190次训练\n",
      "最优模型为35\n",
      "执行时间: 225.45598483085632 秒,60次未求解，当前强化学习值为-192425.0,利润为108375.0\n",
      "第191次训练\n",
      "最优模型为35\n",
      "执行时间: 226.28961968421936 秒,60次未求解，当前强化学习值为-189966.0,利润为107834.0\n",
      "第192次训练\n",
      "最优模型为35\n",
      "执行时间: 228.23953676223755 秒,31次未求解，当前强化学习值为482222.0,利润为660722.0\n",
      "第193次训练\n",
      "最优模型为35\n",
      "执行时间: 229.31199431419373 秒,59次未求解，当前强化学习值为-165180.0,利润为129320.0\n",
      "第194次训练\n",
      "最优模型为35\n",
      "执行时间: 230.2102234363556 秒,60次未求解，当前强化学习值为-182688.0,利润为112312.0\n",
      "第195次训练\n",
      "最优模型为35\n",
      "执行时间: 231.1345226764679 秒,60次未求解，当前强化学习值为-181087.0,利润为112313.0\n",
      "第196次训练\n",
      "最优模型为35\n",
      "执行时间: 232.0563771724701 秒,60次未求解，当前强化学习值为-192484.0,利润为105716.0\n",
      "第197次训练\n",
      "最优模型为35\n",
      "执行时间: 233.13523602485657 秒,51次未求解，当前强化学习值为38378.0,利润为297678.0\n",
      "第198次训练\n",
      "最优模型为35\n",
      "执行时间: 233.98434162139893 秒,60次未求解，当前强化学习值为-191384.0,利润为105716.0\n",
      "第199次训练\n",
      "最优模型为35\n",
      "执行时间: 235.4276783466339 秒,34次未求解，当前强化学习值为398089.0,利润为596089.0\n",
      "第200次训练\n",
      "最优模型为35\n",
      "执行时间: 236.49652671813965 秒,51次未求解，当前强化学习值为42467.0,利润为296067.0\n",
      "第201次训练\n",
      "最优模型为35\n",
      "执行时间: 237.3912591934204 秒,58次未求解，当前强化学习值为-137200.0,利润为151300.0\n",
      "第202次训练\n",
      "最优模型为35\n",
      "执行时间: 238.45400261878967 秒,52次未求解，当前强化学习值为-18412.0,利润为255788.0\n",
      "第203次训练\n",
      "最优模型为35\n",
      "执行时间: 239.55296993255615 秒,47次未求解，当前强化学习值为117494.0,利润为364394.0\n",
      "第204次训练\n",
      "最优模型为35\n",
      "执行时间: 240.5828664302826 秒,51次未求解，当前强化学习值为9457.0,利润为275757.0\n",
      "第205次训练\n",
      "最优模型为35\n",
      "执行时间: 241.49330592155457 秒,60次未求解，当前强化学习值为-190498.0,利润为108602.0\n",
      "第206次训练\n",
      "最优模型为35\n",
      "执行时间: 242.33976364135742 秒,60次未求解，当前强化学习值为-183399.0,利润为108601.0\n",
      "第207次训练\n",
      "最优模型为35\n",
      "执行时间: 243.2067174911499 秒,60次未求解，当前强化学习值为-191443.0,利润为104257.0\n",
      "第208次训练\n",
      "最优模型为35\n",
      "执行时间: 244.13494753837585 秒,59次未求解，当前强化学习值为-161679.0,利润为129321.0\n",
      "第209次训练\n",
      "最优模型为35\n",
      "执行时间: 245.13988399505615 秒,51次未求解，当前强化学习值为23954.0,利润为286754.0\n",
      "第210次训练\n",
      "最优模型为35\n",
      "执行时间: 246.03105187416077 秒,60次未求解，当前强化学习值为-183013.0,利润为112087.0\n",
      "第211次训练\n",
      "最优模型为35\n",
      "执行时间: 247.0749294757843 秒,48次未求解，当前强化学习值为76701.0,利润为330601.0\n",
      "第212次训练\n",
      "最优模型为35\n",
      "执行时间: 248.1468517780304 秒,49次未求解，当前强化学习值为59010.0,利润为318010.0\n",
      "第213次训练\n",
      "最优模型为35\n",
      "执行时间: 249.4684054851532 秒,41次未求解，当前强化学习值为202808.0,利润为437508.0\n",
      "第214次训练\n",
      "最优模型为35\n",
      "执行时间: 250.35275077819824 秒,60次未求解，当前强化学习值为-189198.0,利润为108602.0\n",
      "第215次训练\n",
      "最优模型为35\n",
      "执行时间: 252.0311381816864 秒,24次未求解，当前强化学习值为648412.0,利润为803012.0\n",
      "第216次训练\n",
      "最优模型为35\n",
      "执行时间: 253.1603446006775 秒,51次未求解，当前强化学习值为48759.0,利润为307659.0\n",
      "第217次训练\n",
      "最优模型为35\n",
      "执行时间: 254.5544149875641 秒,40次未求解，当前强化学习值为270620.0,利润为488320.0\n",
      "第218次训练\n",
      "最优模型为35\n",
      "执行时间: 255.55295395851135 秒,59次未求解，当前强化学习值为-163656.0,利润为127244.0\n",
      "第219次训练\n",
      "最优模型为35\n",
      "执行时间: 256.5672302246094 秒,58次未求解，当前强化学习值为-140753.0,利润为149647.0\n",
      "第220次训练\n",
      "最优模型为35\n",
      "执行时间: 257.70841789245605 秒,48次未求解，当前强化学习值为73917.0,利润为328117.0\n",
      "第221次训练\n",
      "最优模型为35\n",
      "执行时间: 258.69633173942566 秒,59次未求解，当前强化学习值为-163358.0,利润为127242.0\n",
      "第222次训练\n",
      "最优模型为35\n",
      "执行时间: 260.21940994262695 秒,43次未求解，当前强化学习值为198826.0,利润为425426.0\n",
      "第223次训练\n",
      "最优模型为35\n",
      "执行时间: 261.903112411499 秒,33次未求解，当前强化学习值为405308.0,利润为600808.0\n",
      "第224次训练\n",
      "最优模型为35\n",
      "执行时间: 262.8095097541809 秒,60次未求解，当前强化学习值为-185625.0,利润为108375.0\n",
      "第225次训练\n",
      "最优模型为35\n",
      "执行时间: 263.83556151390076 秒,57次未求解，当前强化学习值为-123961.0,利润为163339.0\n",
      "第226次训练\n",
      "最优模型为35\n",
      "执行时间: 265.05682849884033 秒,51次未求解，当前强化学习值为40659.0,利润为299259.0\n",
      "第227次训练\n",
      "最优模型为35\n",
      "执行时间: 265.98778200149536 秒,60次未求解，当前强化学习值为-182715.0,利润为112085.0\n",
      "第228次训练\n",
      "最优模型为35\n",
      "执行时间: 267.057181596756 秒,55次未求解，当前强化学习值为-48599.0,利润为218201.0\n",
      "第229次训练\n",
      "最优模型为35\n",
      "执行时间: 267.9928729534149 秒,60次未求解，当前强化学习值为-194184.0,利润为105716.0\n",
      "第230次训练\n",
      "最优模型为35\n",
      "执行时间: 268.964234828949 秒,60次未求解，当前强化学习值为-187866.0,利润为107834.0\n",
      "第231次训练\n",
      "最优模型为35\n",
      "执行时间: 269.9218215942383 秒,60次未求解，当前强化学习值为-193943.0,利润为104257.0\n",
      "第232次训练\n",
      "最优模型为35\n",
      "执行时间: 270.89600133895874 秒,60次未求解，当前强化学习值为-191198.0,利润为108602.0\n",
      "第233次训练\n",
      "最优模型为35\n",
      "执行时间: 272.1765356063843 秒,48次未求解，当前强化学习值为79180.0,利润为333880.0\n",
      "第234次训练\n",
      "最优模型为35\n",
      "执行时间: 273.50864362716675 秒,47次未求解，当前强化学习值为109717.0,利润为353317.0\n",
      "第235次训练\n",
      "最优模型为35\n",
      "执行时间: 274.6288814544678 秒,54次未求解，当前强化学习值为-55096.0,利润为221304.0\n",
      "第236次训练\n",
      "最优模型为35\n",
      "执行时间: 275.55756282806396 秒,60次未求解，当前强化学习值为-181387.0,利润为112313.0\n",
      "第237次训练\n",
      "最优模型为35\n",
      "执行时间: 276.5299873352051 秒,60次未求解，当前强化学习值为-185989.0,利润为112311.0\n",
      "第238次训练\n",
      "最优模型为35\n",
      "执行时间: 277.5316972732544 秒,60次未求解，当前强化学习值为-186115.0,利润为112085.0\n",
      "第239次训练\n",
      "最优模型为35\n",
      "执行时间: 278.9533004760742 秒,42次未求解，当前强化学习值为211392.0,利润为440392.0\n",
      "第240次训练\n",
      "最优模型为35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 151\u001b[0m\n\u001b[0;32m    149\u001b[0m     next_order_states \u001b[38;5;241m=\u001b[39m vectorization_order(orders_unmatched)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# 这里防止梯度爆炸缩小了reward\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgrid_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_vehicle_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_order_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     env\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m=\u001b[39m time\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n",
      "File \u001b[1;32md:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:167\u001b[0m, in \u001b[0;36mMultiAgentAC.update\u001b[1;34m(self, vehicle_states, order_states, actions, rewards, next_vehicle_states, next_order_states, dones)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 167\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dirty_test\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\dirty_test\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import SETTING\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "\n",
    "# 初始化\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 设置s_0\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G ,speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME/2)\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        # agent.load_model(load_path)\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            greedy_epsilon = 0.6\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = 0.001\n",
    "            explore = False\n",
    "            if episode == burn_in:\n",
    "                best_model = train_rewards.index(max(train_rewards))\n",
    "                agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                print(f\"最优模型为{best_model}\")\n",
    "                agent.eval()\n",
    "            if episode > burn_in:\n",
    "                if train_rewards[best_model] > train_rewards[-1] or first_invalid >= invalid_time:\n",
    "                    agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "                else:\n",
    "                    best_model = train_rewards.index(train_rewards[-1]) \n",
    "                    agent = torch.load(f'model_checkpoint_{best_model+1}.pth',map_location=\"cpu\")\n",
    "                    print(f\"最优模型为{best_model}\")\n",
    "                    agent.eval()\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "            \"\"\"\n",
    "            if order.matched is False:\n",
    "                order.virtual_departure = order.departure \n",
    "            \"\"\"\n",
    "        if time != 0 and episode != 0:\n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            agent.update(vehicle_states, order_states, action,\n",
    "                         grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            \n",
    "            greedy = random.randint(0, 1)\n",
    "            if greedy > greedy_epsilon:\n",
    "                greedy = True\n",
    "            action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "            reward = env.test_step(orders_unmatched,action)\n",
    "            # 一个循环代码让我达到最优\n",
    "            # 屁股后面的代码是为了让我达到最优\n",
    "            # 这里是为了让我达到最优\n",
    "            # 现在放弃了重采样\n",
    "            \"\"\"\n",
    "            COUNT = 1000\n",
    "            max_reward = -999999\n",
    "            max_action = action\n",
    "            \n",
    "           \n",
    "            while reward != 1000 and COUNT > 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                action = agent.take_action(vehicle_states, order_states, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "                COUNT -= 1\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_action = action\n",
    "            if COUNT == 0:\n",
    "                reward = env.test_step(orders_unmatched, max_action)\n",
    "            \"\"\"\n",
    "            \n",
    "           \n",
    "            \n",
    "            ACTIONS.append(action) \n",
    "            \"\"\"\n",
    "            while reward != 0 :\n",
    "                action = agent.take_action(vehicle_states, order_states)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self_update(Vehicles, G)\n",
    "            # print(f\"{episode}轮，{time}次，{len(group[1])}辆车不在城市\")\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            # 利润（如果有）减去新增的取消订单\n",
    "            \n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            # 防止梯度爆炸\n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            # print(grid_reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            # base_vehicle_class.append(copy.deepcopy(Vehicles))\n",
    "            # base_order_class.append(copy.deepcopy(Total_order))\n",
    "            \n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "            \"\"\"\n",
    "            if episode == 1:\n",
    "                first_revnue.append(objval)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # if base_revenue[time] != objval:\n",
    "            #    print(\"base_revenue\",time, objval,base_revenue[time])\n",
    "            # if first_revnue[time] != objval:\n",
    "            #    print(\"first_revenue\",time, first_revnue[time], objval)\n",
    "            \"\"\" \n",
    "            if base_vehicle[time] != group[0]:\n",
    "                print(\"vehicle is different\", len(base_vehicle[time]), len(group[0]))\n",
    "            if base_city_node[time] != env.cities:\n",
    "                print(time, base_city_node[time],\"\\n\", env.cities)\n",
    "            \"\"\"\n",
    "            \n",
    "        # print(f\"{len(orders_unmatched)}订单未被匹配,{order_canceled}订单超时,总利润为{objval},强化学习利润为{reward}\")\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        # torch.save(agent.state_dict(), 'model_checkpoint.pth')\n",
    "        torch.save(agent, f\"model_checkpoint_{episode}.pth\")\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")\n",
    "    # grid_rewards.append(0)\n",
    "    # save_path = f\"actor_critic_model{episode}.pth\"\n",
    "    # load_path = f\"actor_critic_model{episode}.pth\"\n",
    "    \n",
    "plt.plot(grid_rewards, label='Grid Reward Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Grid Reward')\n",
    "plt.title('Reward Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# print(find_duplicates_with_positions(ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:alns.ALNS:Finished iterating in 0.15s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best routes: [[0, 1, 0], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0]]\n",
      "Best cost: 858.1198159028768\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from alns import ALNS, State\n",
    "from alns.accept import SimulatedAnnealing\n",
    "from alns.stop import MaxIterations\n",
    "from alns.select import RouletteWheel\n",
    "import random\n",
    "\n",
    "# Problem Data\n",
    "np.random.seed(42)\n",
    "num_customers = 10\n",
    "num_vehicles = 3\n",
    "capacity = 100\n",
    "\n",
    "# 随机生成坐标和需求\n",
    "depot = np.array([50, 50])\n",
    "nodes = np.random.randint(0, 100, (num_customers, 2))\n",
    "demands = np.random.randint(5, 20, num_customers)\n",
    "\n",
    "# 计算欧几里得距离\n",
    "def distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "distance_matrix = np.zeros((num_customers + 1, num_customers + 1))\n",
    "nodes_full = np.vstack([depot, nodes])  # 加入仓库\n",
    "for i in range(len(nodes_full)):\n",
    "    for j in range(len(nodes_full)):\n",
    "        distance_matrix[i, j] = distance(nodes_full[i], nodes_full[j])\n",
    "\n",
    "# 初始状态\n",
    "class VRPState(State):\n",
    "    def __init__(self, routes):\n",
    "        self.routes = routes\n",
    "    \n",
    "    def objective(self):\n",
    "        total_cost = sum(\n",
    "            distance_matrix[route[i], route[i+1]]\n",
    "            for route in self.routes for i in range(len(route)-1)\n",
    "        )\n",
    "        return total_cost\n",
    "    \n",
    "    def copy(self):\n",
    "        return VRPState([route[:] for route in self.routes])\n",
    "\n",
    "# 破坏算子\n",
    "def random_removal(state, rng, num_remove=2):\n",
    "    if isinstance(num_remove, np.random.Generator):  \n",
    "        num_remove = rng.integers(1, 4)  \n",
    "\n",
    "    new_state = state.copy()\n",
    "    for _ in range(num_remove):\n",
    "        if any(new_state.routes):\n",
    "            route = random.choice(new_state.routes)\n",
    "            if len(route) > 2:  # 只有在长度 > 2 时才移除\n",
    "                idx = rng.integers(1, max(2, len(route) - 1))  # 确保 idx 合法\n",
    "                route.pop(idx)\n",
    "    return new_state\n",
    "\n",
    "\n",
    "\n",
    "# 修复算子\n",
    "def greedy_insert(state, rng):\n",
    "    new_state = state.copy()\n",
    "    unassigned = [i for i in range(1, num_customers + 1) if not any(i in r for r in new_state.routes)]\n",
    "    for i in unassigned:\n",
    "        best_cost = float('inf')\n",
    "        best_route = None\n",
    "        best_position = None\n",
    "        for route in new_state.routes:\n",
    "            for pos in range(1, len(route)):\n",
    "                temp_route = route[:pos] + [i] + route[pos:]\n",
    "                cost = sum(distance_matrix[temp_route[j], temp_route[j+1]] for j in range(len(temp_route)-1))\n",
    "                if cost < best_cost:\n",
    "                    best_cost, best_route, best_position = cost, route, pos\n",
    "        if best_route is not None:\n",
    "            best_route.insert(best_position, i)\n",
    "    return new_state\n",
    "\n",
    "# ALNS 运行\n",
    "initial_routes = [[0, i, 0] for i in range(1, num_customers + 1)]  # 每个客户单独一辆车\n",
    "initial_state = VRPState(initial_routes)\n",
    "alns = ALNS()\n",
    "alns.add_destroy_operator(random_removal)\n",
    "alns.add_repair_operator(greedy_insert)\n",
    "\n",
    "# 设定接受准则（模拟退火）\n",
    "accept = SimulatedAnnealing(1000, 1, 500, method=\"linear\")\n",
    "select = RouletteWheel([1] * 4, 0.8, 1, 1)\n",
    "stop = MaxIterations(1000)\n",
    "\n",
    "result = alns.iterate(initial_state, select, accept, stop)\n",
    "\n",
    "# 输出最优解\n",
    "best_state = result.best_state\n",
    "print(\"Best routes:\", best_state.routes)\n",
    "print(\"Best cost:\", best_state.objective())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据固定的逐步调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# 假设有如下初始化函数\n",
    "\n",
    "\"\"\"这里是非强化学习部分\"\"\"\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "Vehicles = {}\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "Vehicles = vehicle_generator(num_vehicle, num_city)\n",
    "orders_unmatched = {}\n",
    "G = CityGraph(num_city, 0.3, (10, 30))\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "Total_order = {}\n",
    "\n",
    "# 生成固定样本\n",
    "for time in range(TIME):\n",
    "    Orders = order_generator(num_order, time, num_city-1, CAPACITY, G, speed)\n",
    "    for order in Orders.values():\n",
    "        Total_order[order.id] = order\n",
    "\n",
    "# 保存样本到本地\n",
    "with open('sample_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'Vehicles': Vehicles,\n",
    "        'Total_order': Total_order,\n",
    "        'G':G\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'MARL_BASE.MultiAgentAC'>\n",
      "第0次训练\n",
      "未加强化学习利润为569776.0,101次未求解\n",
      "第1次训练\n",
      "执行时间: 1.8214232921600342 秒,131次未求解，当前强化学习值为-14450.0,利润为-14450.0\n",
      "第2次训练\n",
      "执行时间: 2.5819530487060547 秒,124次未求解，当前强化学习值为126067.0,利润为126067.0\n",
      "第3次训练\n",
      "执行时间: 3.209756374359131 秒,127次未求解，当前强化学习值为65143.0,利润为65143.0\n",
      "第4次训练\n",
      "执行时间: 3.8005588054656982 秒,129次未求解，当前强化学习值为31061.0,利润为31061.0\n",
      "第5次训练\n",
      "执行时间: 5.171870470046997 秒,106次未求解，当前强化学习值为479609.0,利润为479609.0\n",
      "第6次训练\n",
      "执行时间: 6.1604015827178955 秒,115次未求解，当前强化学习值为300821.0,利润为300821.0\n",
      "第7次训练\n",
      "执行时间: 8.136986494064331 秒,90次未求解，当前强化学习值为816723.0,利润为816723.0\n",
      "第8次训练\n",
      "执行时间: 9.025310754776001 秒,116次未求解，当前强化学习值为277792.0,利润为277792.0\n",
      "第9次训练\n",
      "执行时间: 9.848188877105713 秒,117次未求解，当前强化学习值为269611.0,利润为269611.0\n",
      "第10次训练\n",
      "执行时间: 10.697981357574463 秒,116次未求解，当前强化学习值为278482.0,利润为278482.0\n",
      "第11次训练\n",
      "执行时间: 11.367264032363892 秒,124次未求解，当前强化学习值为124872.0,利润为124872.0\n",
      "第12次训练\n",
      "执行时间: 12.09115719795227 秒,120次未求解，当前强化学习值为201886.0,利润为201886.0\n",
      "第13次训练\n",
      "执行时间: 13.527049779891968 秒,102次未求解，当前强化学习值为560118.0,利润为560118.0\n",
      "第14次训练\n",
      "执行时间: 16.618025302886963 秒,84次未求解，当前强化学习值为926673.0,利润为926673.0\n",
      "第15次训练\n",
      "执行时间: 22.534926652908325 秒,38次未求解，当前强化学习值为1789543.0,利润为1789543.0\n",
      "第16次训练\n",
      "执行时间: 23.355343103408813 秒,131次未求解，当前强化学习值为-14443.0,利润为-14443.0\n",
      "第17次训练\n",
      "执行时间: 25.035274505615234 秒,113次未求解，当前强化学习值为315959.0,利润为315959.0\n",
      "第18次训练\n",
      "执行时间: 26.2196364402771 秒,125次未求解，当前强化学习值为91957.0,利润为91957.0\n",
      "第19次训练\n",
      "执行时间: 27.13193154335022 秒,131次未求解，当前强化学习值为-14443.0,利润为-14443.0\n",
      "第20次训练\n",
      "执行时间: 31.63504981994629 秒,64次未求解，当前强化学习值为1257148.0,利润为1257148.0\n",
      "第21次训练\n",
      "执行时间: 32.52990698814392 秒,129次未求解，当前强化学习值为31066.0,利润为31066.0\n",
      "第22次训练\n",
      "执行时间: 34.44188714027405 秒,110次未求解，当前强化学习值为389375.0,利润为389375.0\n",
      "第23次训练\n",
      "执行时间: 35.773600578308105 秒,120次未求解，当前强化学习值为211811.0,利润为211811.0\n",
      "第24次训练\n",
      "执行时间: 38.032304763793945 秒,103次未求解，当前强化学习值为513696.0,利润为513696.0\n",
      "第25次训练\n",
      "执行时间: 39.7885627746582 秒,113次未求解，当前强化学习值为325222.0,利润为325222.0\n",
      "第26次训练\n",
      "执行时间: 40.999921560287476 秒,123次未求解，当前强化学习值为156491.0,利润为156491.0\n",
      "第27次训练\n",
      "执行时间: 41.95594310760498 秒,129次未求解，当前强化学习值为31061.0,利润为31061.0\n",
      "第28次训练\n",
      "执行时间: 42.895962715148926 秒,129次未求解，当前强化学习值为31063.0,利润为31063.0\n",
      "第29次训练\n",
      "执行时间: 44.09080910682678 秒,123次未求解，当前强化学习值为153352.0,利润为153352.0\n",
      "第30次训练\n",
      "执行时间: 44.91484260559082 秒,131次未求解，当前强化学习值为-16975.0,利润为-16975.0\n",
      "第31次训练\n",
      "执行时间: 45.74456477165222 秒,131次未求解，当前强化学习值为-14445.0,利润为-14445.0\n",
      "第32次训练\n",
      "执行时间: 47.3925518989563 秒,115次未求解，当前强化学习值为297322.0,利润为297322.0\n",
      "第33次训练\n",
      "执行时间: 48.92673945426941 秒,116次未求解，当前强化学习值为287109.0,利润为287109.0\n",
      "第34次训练\n",
      "执行时间: 49.849727153778076 秒,129次未求解，当前强化学习值为31061.0,利润为31061.0\n",
      "第35次训练\n",
      "执行时间: 50.6612765789032 秒,131次未求解，当前强化学习值为-16397.0,利润为-16397.0\n",
      "第36次训练\n",
      "执行时间: 51.66808581352234 秒,131次未求解，当前强化学习值为-14443.0,利润为-14443.0\n",
      "第37次训练\n",
      "执行时间: 53.88011026382446 秒,105次未求解，当前强化学习值为497273.0,利润为497273.0\n",
      "第38次训练\n",
      "执行时间: 57.15330123901367 秒,91次未求解，当前强化学习值为786774.0,利润为786774.0\n",
      "第39次训练\n",
      "执行时间: 59.29867720603943 秒,105次未求解，当前强化学习值为494245.0,利润为494245.0\n",
      "第40次训练\n",
      "执行时间: 60.812331199645996 秒,119次未求解，当前强化学习值为240677.0,利润为240677.0\n",
      "第41次训练\n",
      "执行时间: 61.64568018913269 秒,131次未求解，当前强化学习值为-16975.0,利润为-16975.0\n",
      "第42次训练\n",
      "执行时间: 63.15235710144043 秒,118次未求解，当前强化学习值为255655.0,利润为255655.0\n",
      "第43次训练\n",
      "执行时间: 65.44764471054077 秒,104次未求解，当前强化学习值为507412.0,利润为507412.0\n",
      "第44次训练\n",
      "执行时间: 66.99716067314148 秒,116次未求解，当前强化学习值为283400.0,利润为283400.0\n",
      "第45次训练\n",
      "执行时间: 70.27388453483582 秒,84次未求解，当前强化学习值为912019.0,利润为912019.0\n",
      "第46次训练\n",
      "执行时间: 71.97512602806091 秒,114次未求解，当前强化学习值为319628.0,利润为319628.0\n",
      "第47次训练\n",
      "执行时间: 72.97674775123596 秒,129次未求解，当前强化学习值为31061.0,利润为31061.0\n",
      "第48次训练\n",
      "执行时间: 74.74558210372925 秒,115次未求解，当前强化学习值为298634.0,利润为298634.0\n",
      "第49次训练\n",
      "执行时间: 75.7597107887268 秒,127次未求解，当前强化学习值为61443.0,利润为61443.0\n",
      "第50次训练\n",
      "执行时间: 78.42941665649414 秒,99次未求解，当前强化学习值为630135.0,利润为630135.0\n",
      "第51次训练\n",
      "执行时间: 79.26717495918274 秒,131次未求解，当前强化学习值为-14443.0,利润为-14443.0\n",
      "第52次训练\n",
      "执行时间: 82.47699856758118 秒,91次未求解，当前强化学习值为781065.0,利润为781065.0\n",
      "第53次训练\n",
      "执行时间: 84.72926735877991 秒,107次未求解，当前强化学习值为452649.0,利润为452649.0\n",
      "第54次训练\n",
      "执行时间: 86.4223883152008 秒,116次未求解，当前强化学习值为287985.0,利润为287985.0\n",
      "第55次训练\n",
      "执行时间: 89.22439432144165 秒,97次未求解，当前强化学习值为628220.0,利润为628220.0\n",
      "第56次训练\n",
      "执行时间: 91.51757502555847 秒,106次未求解，当前强化学习值为483053.0,利润为483053.0\n",
      "第57次训练\n",
      "执行时间: 95.51028609275818 秒,77次未求解，当前强化学习值为1065824.0,利润为1065824.0\n",
      "第58次训练\n",
      "执行时间: 97.14673733711243 秒,116次未求解，当前强化学习值为287453.0,利润为287453.0\n",
      "第59次训练\n",
      "执行时间: 98.00488805770874 秒,131次未求解，当前强化学习值为-16974.0,利润为-16974.0\n",
      "第60次训练\n",
      "执行时间: 102.48795771598816 秒,66次未求解，当前强化学习值为1246911.0,利润为1246911.0\n",
      "第61次训练\n",
      "执行时间: 106.5568494796753 秒,75次未求解，当前强化学习值为1076759.0,利润为1076759.0\n",
      "第62次训练\n",
      "执行时间: 107.55579280853271 秒,127次未求解，当前强化学习值为67723.0,利润为67723.0\n",
      "第63次训练\n",
      "执行时间: 108.57955551147461 秒,129次未求解，当前强化学习值为31061.0,利润为31061.0\n",
      "第64次训练\n",
      "执行时间: 110.50117135047913 秒,113次未求解，当前强化学习值为335324.0,利润为335324.0\n",
      "第65次训练\n",
      "执行时间: 116.14572882652283 秒,58次未求解，当前强化学习值为1391986.0,利润为1391986.0\n",
      "第66次训练\n",
      "执行时间: 118.66831040382385 秒,103次未求解，当前强化学习值为517956.0,利润为517956.0\n",
      "第67次训练\n",
      "执行时间: 121.3284044265747 秒,103次未求解，当前强化学习值为529692.0,利润为529692.0\n",
      "第68次训练\n",
      "执行时间: 122.17198634147644 秒,131次未求解，当前强化学习值为-15893.0,利润为-15893.0\n",
      "第69次训练\n",
      "执行时间: 125.02498865127563 秒,98次未求解，当前强化学习值为622552.0,利润为622552.0\n",
      "第70次训练\n",
      "执行时间: 129.86267852783203 秒,65次未求解，当前强化学习值为1260287.0,利润为1260287.0\n",
      "第71次训练\n",
      "执行时间: 131.83547854423523 秒,113次未求解，当前强化学习值为320544.0,利润为320544.0\n",
      "第72次训练\n",
      "执行时间: 132.80047059059143 秒,129次未求解，当前强化学习值为31066.0,利润为31066.0\n",
      "第73次训练\n",
      "执行时间: 134.59910416603088 秒,115次未求解，当前强化学习值为300608.0,利润为300608.0\n",
      "第74次训练\n",
      "执行时间: 136.58974409103394 秒,116次未求解，当前强化学习值为288610.0,利润为288610.0\n",
      "第75次训练\n",
      "执行时间: 137.65809154510498 秒,127次未求解，当前强化学习值为67732.0,利润为67732.0\n",
      "第76次训练\n",
      "执行时间: 141.6179337501526 秒,84次未求解，当前强化学习值为911699.0,利润为911699.0\n",
      "第77次训练\n",
      "执行时间: 142.57835721969604 秒,131次未求解，当前强化学习值为-14445.0,利润为-14445.0\n",
      "第78次训练\n",
      "执行时间: 145.06165480613708 秒,108次未求解，当前强化学习值为437900.0,利润为437900.0\n",
      "第79次训练\n",
      "执行时间: 147.88490223884583 秒,103次未求解，当前强化学习值为537557.0,利润为537557.0\n",
      "第80次训练\n",
      "执行时间: 149.97578263282776 秒,114次未求解，当前强化学习值为298354.0,利润为298354.0\n",
      "第81次训练\n",
      "执行时间: 150.94512677192688 秒,131次未求解，当前强化学习值为-14449.0,利润为-14449.0\n",
      "第82次训练\n",
      "执行时间: 152.089426279068 秒,131次未求解，当前强化学习值为-14443.0,利润为-14443.0\n",
      "第83次训练\n",
      "执行时间: 154.77017641067505 秒,110次未求解，当前强化学习值为396857.0,利润为396857.0\n",
      "第84次训练\n",
      "执行时间: 155.80234503746033 秒,131次未求解，当前强化学习值为-14446.0,利润为-14446.0\n",
      "第85次训练\n",
      "执行时间: 158.48473143577576 秒,106次未求解，当前强化学习值为477695.0,利润为477695.0\n",
      "第86次训练\n",
      "执行时间: 160.61724615097046 秒,113次未求解，当前强化学习值为324385.0,利润为324385.0\n",
      "第87次训练\n",
      "执行时间: 161.7976689338684 秒,129次未求解，当前强化学习值为31063.0,利润为31063.0\n",
      "第88次训练\n",
      "执行时间: 162.8837330341339 秒,131次未求解，当前强化学习值为-14445.0,利润为-14445.0\n",
      "第89次训练\n",
      "执行时间: 165.05448698997498 秒,113次未求解，当前强化学习值为335402.0,利润为335402.0\n",
      "第90次训练\n",
      "执行时间: 166.22543740272522 秒,127次未求解，当前强化学习值为67730.0,利润为67730.0\n",
      "第91次训练\n",
      "执行时间: 168.19342803955078 秒,112次未求解，当前强化学习值为353868.0,利润为353868.0\n",
      "第92次训练\n",
      "执行时间: 169.4076042175293 秒,127次未求解，当前强化学习值为65139.0,利润为65139.0\n",
      "第93次训练\n",
      "执行时间: 172.2340168952942 秒,103次未求解，当前强化学习值为535283.0,利润为535283.0\n",
      "第94次训练\n",
      "执行时间: 174.34201407432556 秒,112次未求解，当前强化学习值为336939.0,利润为336939.0\n",
      "第95次训练\n",
      "执行时间: 175.35545945167542 秒,129次未求解，当前强化学习值为31061.0,利润为31061.0\n",
      "第96次训练\n",
      "执行时间: 176.66936039924622 秒,123次未求解，当前强化学习值为153355.0,利润为153355.0\n",
      "第97次训练\n",
      "执行时间: 178.40384078025818 秒,116次未求解，当前强化学习值为270619.0,利润为270619.0\n",
      "第98次训练\n",
      "执行时间: 180.2589933872223 秒,116次未求解，当前强化学习值为279791.0,利润为279791.0\n",
      "第99次训练\n",
      "执行时间: 181.12314319610596 秒,131次未求解，当前强化学习值为-14443.0,利润为-14443.0\n",
      "第100次训练\n",
      "执行时间: 183.21787548065186 秒,105次未求解，当前强化学习值为491543.0,利润为491543.0\n",
      "第101次训练\n",
      "执行时间: 184.73030376434326 秒,120次未求解，当前强化学习值为207664.0,利润为207664.0\n",
      "第102次训练\n",
      "执行时间: 185.57831001281738 秒,131次未求解，当前强化学习值为-14447.0,利润为-14447.0\n",
      "第103次训练\n",
      "执行时间: 186.42217111587524 秒,131次未求解，当前强化学习值为-14454.0,利润为-14454.0\n",
      "第104次训练\n",
      "执行时间: 187.32396459579468 秒,131次未求解，当前强化学习值为-22392.0,利润为-22392.0\n",
      "第105次训练\n",
      "执行时间: 188.57190418243408 秒,125次未求解，当前强化学习值为96802.0,利润为96802.0\n",
      "第106次训练\n",
      "执行时间: 190.3823025226593 秒,114次未求解，当前强化学习值为305191.0,利润为305191.0\n",
      "第107次训练\n",
      "执行时间: 192.03277611732483 秒,115次未求解，当前强化学习值为297562.0,利润为297562.0\n",
      "第108次训练\n",
      "执行时间: 192.84421014785767 秒,131次未求解，当前强化学习值为-14444.0,利润为-14444.0\n",
      "第109次训练\n",
      "执行时间: 193.75510048866272 秒,131次未求解，当前强化学习值为-14443.0,利润为-14443.0\n",
      "第110次训练\n",
      "执行时间: 194.93958139419556 秒,124次未求解，当前强化学习值为126086.0,利润为126086.0\n",
      "第111次训练\n",
      "执行时间: 195.8718626499176 秒,131次未求解，当前强化学习值为-14446.0,利润为-14446.0\n",
      "第112次训练\n",
      "执行时间: 196.7375934123993 秒,131次未求解，当前强化学习值为-14447.0,利润为-14447.0\n",
      "第113次训练\n",
      "执行时间: 197.9547562599182 秒,125次未求解，当前强化学习值为88691.0,利润为88691.0\n",
      "第114次训练\n",
      "执行时间: 199.67902731895447 秒,118次未求解，当前强化学习值为251785.0,利润为251785.0\n",
      "第115次训练\n",
      "执行时间: 204.08070373535156 秒,75次未求解，当前强化学习值为1082932.0,利润为1082932.0\n",
      "第116次训练\n",
      "执行时间: 206.29789113998413 秒,105次未求解，当前强化学习值为492760.0,利润为492760.0\n",
      "第117次训练\n",
      "执行时间: 207.11286401748657 秒,131次未求解，当前强化学习值为-16975.0,利润为-16975.0\n",
      "第118次训练\n",
      "执行时间: 208.54626655578613 秒,119次未求解，当前强化学习值为235912.0,利润为235912.0\n",
      "第119次训练\n",
      "执行时间: 211.83202719688416 秒,85次未求解，当前强化学习值为898089.0,利润为898089.0\n",
      "第120次训练\n",
      "执行时间: 212.91113090515137 秒,124次未求解，当前强化学习值为126082.0,利润为126082.0\n",
      "第121次训练\n",
      "执行时间: 213.92434310913086 秒,131次未求解，当前强化学习值为-14454.0,利润为-14454.0\n",
      "第122次训练\n",
      "执行时间: 215.52410888671875 秒,115次未求解，当前强化学习值为300775.0,利润为300775.0\n",
      "第123次训练\n",
      "执行时间: 216.7352955341339 秒,123次未求解，当前强化学习值为142987.0,利润为142987.0\n",
      "第124次训练\n",
      "执行时间: 217.81157088279724 秒,124次未求解，当前强化学习值为123332.0,利润为123332.0\n",
      "第125次训练\n",
      "执行时间: 219.8474736213684 秒,108次未求解，当前强化学习值为433638.0,利润为433638.0\n",
      "第126次训练\n",
      "执行时间: 220.7762689590454 秒,131次未求解，当前强化学习值为-16397.0,利润为-16397.0\n",
      "第127次训练\n",
      "执行时间: 222.12179350852966 秒,119次未求解，当前强化学习值为233900.0,利润为233900.0\n",
      "第128次训练\n",
      "执行时间: 223.05001616477966 秒,131次未求解，当前强化学习值为-14446.0,利润为-14446.0\n",
      "第129次训练\n",
      "执行时间: 224.04048991203308 秒,131次未求解，当前强化学习值为-16400.0,利润为-16400.0\n",
      "第130次训练\n",
      "执行时间: 224.9626705646515 秒,131次未求解，当前强化学习值为-20484.0,利润为-20484.0\n",
      "第131次训练\n",
      "执行时间: 225.84599494934082 秒,131次未求解，当前强化学习值为-14449.0,利润为-14449.0\n",
      "第132次训练\n",
      "执行时间: 226.76835465431213 秒,131次未求解，当前强化学习值为-20484.0,利润为-20484.0\n",
      "第133次训练\n",
      "执行时间: 228.92255353927612 秒,106次未求解，当前强化学习值为492668.0,利润为492668.0\n",
      "第134次训练\n",
      "执行时间: 230.83630967140198 秒,116次未求解，当前强化学习值为284789.0,利润为284789.0\n",
      "第135次训练\n",
      "执行时间: 233.98107290267944 秒,91次未求解，当前强化学习值为779414.0,利润为779414.0\n",
      "第136次训练\n",
      "执行时间: 235.45134472846985 秒,120次未求解，当前强化学习值为210498.0,利润为210498.0\n",
      "第137次训练\n",
      "执行时间: 236.42625284194946 秒,131次未求解，当前强化学习值为-14449.0,利润为-14449.0\n",
      "第138次训练\n",
      "执行时间: 238.11157727241516 秒,116次未求解，当前强化学习值为281182.0,利润为281182.0\n",
      "第139次训练\n",
      "执行时间: 240.25167798995972 秒,107次未求解，当前强化学习值为463760.0,利润为463760.0\n",
      "第140次训练\n",
      "执行时间: 241.27816891670227 秒,131次未求解，当前强化学习值为-14452.0,利润为-14452.0\n",
      "第141次训练\n",
      "执行时间: 243.5403242111206 秒,106次未求解，当前强化学习值为480098.0,利润为480098.0\n",
      "第142次训练\n",
      "执行时间: 245.48853373527527 秒,112次未求解，当前强化学习值为341823.0,利润为341823.0\n",
      "第143次训练\n",
      "执行时间: 246.45514273643494 秒,131次未求解，当前强化学习值为-16982.0,利润为-16982.0\n",
      "第144次训练\n",
      "执行时间: 249.76111817359924 秒,89次未求解，当前强化学习值为807407.0,利润为807407.0\n",
      "第145次训练\n",
      "执行时间: 250.60175395011902 秒,131次未求解，当前强化学习值为-18930.0,利润为-18930.0\n",
      "第146次训练\n",
      "执行时间: 251.4382565021515 秒,131次未求解，当前强化学习值为-14449.0,利润为-14449.0\n",
      "第147次训练\n",
      "执行时间: 254.17027592658997 秒,98次未求解，当前强化学习值为625963.0,利润为625963.0\n",
      "第148次训练\n",
      "执行时间: 255.6959900856018 秒,120次未求解，当前强化学习值为206020.0,利润为206020.0\n",
      "第149次训练\n",
      "执行时间: 256.6906623840332 秒,131次未求解，当前强化学习值为-16975.0,利润为-16975.0\n",
      "第150次训练\n",
      "执行时间: 258.57758021354675 秒,112次未求解，当前强化学习值为351751.0,利润为351751.0\n",
      "第151次训练\n",
      "执行时间: 259.4673056602478 秒,131次未求解，当前强化学习值为-16974.0,利润为-16974.0\n",
      "第152次训练\n",
      "执行时间: 261.93441128730774 秒,101次未求解，当前强化学习值为579294.0,利润为579294.0\n",
      "第153次训练\n",
      "执行时间: 263.2770984172821 秒,124次未求解，当前强化学习值为132196.0,利润为132196.0\n",
      "第154次训练\n",
      "执行时间: 264.1945617198944 秒,131次未求解，当前强化学习值为-16984.0,利润为-16984.0\n",
      "第155次训练\n",
      "执行时间: 265.18861389160156 秒,131次未求解，当前强化学习值为-20484.0,利润为-20484.0\n",
      "第156次训练\n",
      "执行时间: 266.2058255672455 秒,131次未求解，当前强化学习值为-14451.0,利润为-14451.0\n",
      "第157次训练\n",
      "执行时间: 268.5834722518921 秒,103次未求解，当前强化学习值为525913.0,利润为525913.0\n",
      "第158次训练\n",
      "执行时间: 270.5861163139343 秒,109次未求解，当前强化学习值为426547.0,利润为426547.0\n",
      "第159次训练\n",
      "执行时间: 271.99001812934875 秒,110次未求解，当前强化学习值为394628.0,利润为394628.0\n",
      "第160次训练\n",
      "执行时间: 272.62744092941284 秒,131次未求解，当前强化学习值为-18845.0,利润为-18845.0\n",
      "第161次训练\n",
      "执行时间: 273.3124694824219 秒,127次未求解，当前强化学习值为69892.0,利润为69892.0\n",
      "第162次训练\n",
      "执行时间: 274.3592128753662 秒,114次未求解，当前强化学习值为307357.0,利润为307357.0\n",
      "第163次训练\n",
      "执行时间: 275.2888216972351 秒,117次未求解，当前强化学习值为269781.0,利润为269781.0\n",
      "第164次训练\n",
      "执行时间: 276.5026304721832 秒,109次未求解，当前强化学习值为417226.0,利润为417226.0\n",
      "第165次训练\n",
      "执行时间: 277.08573722839355 秒,131次未求解，当前强化学习值为-14454.0,利润为-14454.0\n",
      "第166次训练\n",
      "执行时间: 277.6321473121643 秒,131次未求解，当前强化学习值为-16399.0,利润为-16399.0\n",
      "第167次训练\n",
      "执行时间: 278.97875785827637 秒,103次未求解，当前强化学习值为532440.0,利润为532440.0\n",
      "第168次训练\n",
      "执行时间: 279.5555672645569 秒,131次未求解，当前强化学习值为-16401.0,利润为-16401.0\n",
      "第169次训练\n",
      "执行时间: 280.45557022094727 秒,120次未求解，当前强化学习值为209832.0,利润为209832.0\n",
      "第170次训练\n",
      "执行时间: 281.2005956172943 秒,123次未求解，当前强化学习值为140313.0,利润为140313.0\n",
      "第171次训练\n",
      "执行时间: 282.0207402706146 秒,120次未求解，当前强化学习值为215174.0,利润为215174.0\n",
      "第172次训练\n",
      "执行时间: 282.8884017467499 秒,120次未求解，当前强化学习值为216756.0,利润为216756.0\n",
      "第173次训练\n",
      "执行时间: 283.4390335083008 秒,131次未求解，当前强化学习值为-14452.0,利润为-14452.0\n",
      "第174次训练\n",
      "执行时间: 283.9845793247223 秒,131次未求解，当前强化学习值为-16399.0,利润为-16399.0\n",
      "第175次训练\n",
      "执行时间: 284.7341067790985 秒,123次未求解，当前强化学习值为150398.0,利润为150398.0\n",
      "第176次训练\n",
      "执行时间: 285.2445955276489 秒,131次未求解，当前强化学习值为-14444.0,利润为-14444.0\n",
      "第177次训练\n",
      "执行时间: 286.066974401474 秒,119次未求解，当前强化学习值为222080.0,利润为222080.0\n",
      "第178次训练\n",
      "执行时间: 286.5631322860718 秒,131次未求解，当前强化学习值为-14444.0,利润为-14444.0\n",
      "第179次训练\n",
      "执行时间: 287.7713122367859 秒,112次未求解，当前强化学习值为368386.0,利润为368386.0\n",
      "第180次训练\n",
      "执行时间: 288.38169503211975 秒,127次未求解，当前强化学习值为70055.0,利润为70055.0\n",
      "第181次训练\n",
      "执行时间: 288.94581270217896 秒,131次未求解，当前强化学习值为-18845.0,利润为-18845.0\n",
      "第182次训练\n",
      "执行时间: 289.6493887901306 秒,123次未求解，当前强化学习值为149435.0,利润为149435.0\n",
      "第183次训练\n",
      "执行时间: 290.53173065185547 秒,116次未求解，当前强化学习值为290256.0,利润为290256.0\n",
      "第184次训练\n",
      "执行时间: 292.39545464515686 秒,86次未求解，当前强化学习值为875231.0,利润为875231.0\n",
      "第185次训练\n",
      "执行时间: 292.95083355903625 秒,131次未求解，当前强化学习值为-16401.0,利润为-16401.0\n",
      "第186次训练\n",
      "执行时间: 294.1444375514984 秒,124次未求解，当前强化学习值为126058.0,利润为126058.0\n",
      "第187次训练\n",
      "执行时间: 296.4616205692291 秒,102次未求解，当前强化学习值为552765.0,利润为552765.0\n",
      "第188次训练\n",
      "执行时间: 297.5014476776123 秒,125次未求解，当前强化学习值为94401.0,利润为94401.0\n",
      "第189次训练\n",
      "执行时间: 298.3249430656433 秒,131次未求解，当前强化学习值为-18932.0,利润为-18932.0\n",
      "第190次训练\n",
      "执行时间: 299.4127633571625 秒,124次未求解，当前强化学习值为128185.0,利润为128185.0\n",
      "第191次训练\n",
      "执行时间: 300.2283353805542 秒,131次未求解，当前强化学习值为-16401.0,利润为-16401.0\n",
      "第192次训练\n",
      "执行时间: 303.1000864505768 秒,90次未求解，当前强化学习值为762272.0,利润为762272.0\n",
      "第193次训练\n",
      "执行时间: 305.1813097000122 秒,107次未求解，当前强化学习值为454601.0,利润为454601.0\n",
      "第194次训练\n",
      "执行时间: 306.96586060523987 秒,112次未求解，当前强化学习值为353041.0,利润为353041.0\n",
      "第195次训练\n",
      "执行时间: 307.8539354801178 秒,131次未求解，当前强化学习值为-16401.0,利润为-16401.0\n",
      "第196次训练\n",
      "执行时间: 309.9001376628876 秒,109次未求解，当前强化学习值为425606.0,利润为425606.0\n",
      "第197次训练\n",
      "执行时间: 311.46909379959106 秒,116次未求解，当前强化学习值为279720.0,利润为279720.0\n",
      "第198次训练\n",
      "执行时间: 319.0092806816101 秒,16次未求解，当前强化学习值为2256476.0,利润为2256476.0\n",
      "第199次训练\n",
      "执行时间: 320.2818558216095 秒,120次未求解，当前强化学习值为211401.0,利润为211401.0\n",
      "第200次训练\n",
      "执行时间: 321.0819263458252 秒,131次未求解，当前强化学习值为-16975.0,利润为-16975.0\n",
      "第201次训练\n",
      "执行时间: 322.84348011016846 秒,114次未求解，当前强化学习值为294616.0,利润为294616.0\n",
      "第202次训练\n",
      "执行时间: 324.4181742668152 秒,114次未求解，当前强化学习值为320719.0,利润为320719.0\n",
      "第203次训练\n",
      "执行时间: 328.04008507728577 秒,77次未求解，当前强化学习值为1051097.0,利润为1051097.0\n",
      "第204次训练\n",
      "执行时间: 328.82961916923523 秒,131次未求解，当前强化学习值为-14452.0,利润为-14452.0\n",
      "第205次训练\n",
      "执行时间: 329.974809885025 秒,123次未求解，当前强化学习值为140576.0,利润为140576.0\n",
      "第206次训练\n",
      "执行时间: 330.7637097835541 秒,131次未求解，当前强化学习值为-14445.0,利润为-14445.0\n",
      "第207次训练\n",
      "执行时间: 331.59944796562195 秒,131次未求解，当前强化学习值为-14446.0,利润为-14446.0\n",
      "第208次训练\n",
      "执行时间: 332.44909715652466 秒,131次未求解，当前强化学习值为-14451.0,利润为-14451.0\n",
      "第209次训练\n",
      "执行时间: 335.081387758255 秒,97次未求解，当前强化学习值为663108.0,利润为663108.0\n",
      "第210次训练\n",
      "执行时间: 336.1696264743805 秒,124次未求解，当前强化学习值为129477.0,利润为129477.0\n",
      "第211次训练\n",
      "执行时间: 336.950053691864 秒,131次未求解，当前强化学习值为-18930.0,利润为-18930.0\n",
      "第212次训练\n",
      "执行时间: 338.7670907974243 秒,112次未求解，当前强化学习值为344855.0,利润为344855.0\n",
      "第213次训练\n",
      "执行时间: 339.5642457008362 秒,131次未求解，当前强化学习值为-14452.0,利润为-14452.0\n",
      "第214次训练\n",
      "执行时间: 340.5911281108856 秒,125次未求解，当前强化学习值为98730.0,利润为98730.0\n",
      "第215次训练\n",
      "执行时间: 341.42001938819885 秒,131次未求解，当前强化学习值为-14452.0,利润为-14452.0\n",
      "第216次训练\n",
      "执行时间: 342.540611743927 秒,124次未求解，当前强化学习值为128929.0,利润为128929.0\n",
      "第217次训练\n",
      "执行时间: 344.0923047065735 秒,115次未求解，当前强化学习值为302994.0,利润为302994.0\n",
      "第218次训练\n",
      "执行时间: 345.23276567459106 秒,124次未求解，当前强化学习值为134705.0,利润为134705.0\n",
      "第219次训练\n",
      "执行时间: 346.4606454372406 秒,123次未求解，当前强化学习值为140435.0,利润为140435.0\n",
      "第220次训练\n",
      "执行时间: 347.3146119117737 秒,131次未求解，当前强化学习值为-14452.0,利润为-14452.0\n",
      "第221次训练\n",
      "执行时间: 348.38785457611084 秒,124次未求解，当前强化学习值为116994.0,利润为116994.0\n",
      "第222次训练\n",
      "执行时间: 351.1658115386963 秒,92次未求解，当前强化学习值为741516.0,利润为741516.0\n",
      "第223次训练\n",
      "执行时间: 352.7915976047516 秒,117次未求解，当前强化学习值为259512.0,利润为259512.0\n",
      "第224次训练\n",
      "执行时间: 353.6409068107605 秒,131次未求解，当前强化学习值为-16397.0,利润为-16397.0\n",
      "第225次训练\n",
      "执行时间: 354.78355383872986 秒,124次未求解，当前强化学习值为121408.0,利润为121408.0\n",
      "第226次训练\n",
      "执行时间: 357.021609544754 秒,104次未求解，当前强化学习值为516893.0,利润为516893.0\n",
      "第227次训练\n",
      "执行时间: 358.47946333885193 秒,117次未求解，当前强化学习值为266636.0,利润为266636.0\n",
      "第228次训练\n",
      "执行时间: 359.33396339416504 秒,131次未求解，当前强化学习值为-22392.0,利润为-22392.0\n",
      "第229次训练\n",
      "执行时间: 360.157190322876 秒,131次未求解，当前强化学习值为-14443.0,利润为-14443.0\n",
      "第230次训练\n",
      "执行时间: 361.73282766342163 秒,115次未求解，当前强化学习值为305914.0,利润为305914.0\n",
      "第231次训练\n",
      "执行时间: 366.3977599143982 秒,64次未求解，当前强化学习值为1259737.0,利润为1259737.0\n",
      "第232次训练\n",
      "执行时间: 367.8122124671936 秒,119次未求解，当前强化学习值为228072.0,利润为228072.0\n",
      "第233次训练\n",
      "执行时间: 368.6424059867859 秒,131次未求解，当前强化学习值为-18931.0,利润为-18931.0\n",
      "第234次训练\n",
      "执行时间: 370.3302354812622 秒,115次未求解，当前强化学习值为278419.0,利润为278419.0\n",
      "第235次训练\n",
      "执行时间: 371.1660888195038 秒,131次未求解，当前强化学习值为-22392.0,利润为-22392.0\n",
      "第236次训练\n",
      "执行时间: 372.94395637512207 秒,112次未求解，当前强化学习值为338018.0,利润为338018.0\n",
      "第237次训练\n",
      "执行时间: 373.8019540309906 秒,131次未求解，当前强化学习值为-14454.0,利润为-14454.0\n",
      "第238次训练\n",
      "执行时间: 374.65035605430603 秒,131次未求解，当前强化学习值为-22571.0,利润为-22571.0\n",
      "第239次训练\n",
      "执行时间: 375.49175333976746 秒,131次未求解，当前强化学习值为-16399.0,利润为-16399.0\n",
      "第240次训练\n",
      "执行时间: 377.080961227417 秒,116次未求解，当前强化学习值为283290.0,利润为283290.0\n",
      "第241次训练\n",
      "执行时间: 378.4183142185211 秒,119次未求解，当前强化学习值为225538.00139696855,利润为225538.00139696855\n",
      "第242次训练\n",
      "执行时间: 381.81216955184937 秒,83次未求解，当前强化学习值为921194.0,利润为921194.0\n",
      "第243次训练\n",
      "执行时间: 384.59694027900696 秒,94次未求解，当前强化学习值为721426.0,利润为721426.0\n",
      "第244次训练\n",
      "执行时间: 385.7156801223755 秒,124次未求解，当前强化学习值为126083.0,利润为126083.0\n",
      "第245次训练\n",
      "执行时间: 386.55771827697754 秒,131次未求解，当前强化学习值为-18931.0,利润为-18931.0\n",
      "第246次训练\n",
      "执行时间: 387.47040700912476 秒,131次未求解，当前强化学习值为-14454.0,利润为-14454.0\n",
      "第247次训练\n",
      "执行时间: 388.5808050632477 秒,124次未求解，当前强化学习值为121408.0,利润为121408.0\n",
      "第248次训练\n",
      "执行时间: 391.6422393321991 秒,88次未求解，当前强化学习值为826873.0,利润为826873.0\n",
      "第249次训练\n",
      "执行时间: 392.49984407424927 秒,131次未求解，当前强化学习值为-16399.0,利润为-16399.0\n",
      "第250次训练\n",
      "执行时间: 393.32978415489197 秒,131次未求解，当前强化学习值为-14445.0,利润为-14445.0\n",
      "第251次训练\n",
      "执行时间: 394.1869843006134 秒,131次未求解，当前强化学习值为-14447.0,利润为-14447.0\n",
      "第252次训练\n",
      "执行时间: 396.21809363365173 秒,109次未求解，当前强化学习值为403724.0,利润为403724.0\n",
      "第253次训练\n",
      "执行时间: 397.36172747612 秒,123次未求解，当前强化学习值为142390.0,利润为142390.0\n",
      "第254次训练\n",
      "执行时间: 398.7395181655884 秒,120次未求解，当前强化学习值为215202.0,利润为215202.0\n",
      "第255次训练\n",
      "执行时间: 399.6252715587616 秒,131次未求解，当前强化学习值为-18931.0,利润为-18931.0\n",
      "第256次训练\n",
      "执行时间: 400.7414231300354 秒,125次未求解，当前强化学习值为103301.0,利润为103301.0\n",
      "第257次训练\n",
      "执行时间: 401.5563328266144 秒,131次未求解，当前强化学习值为-14449.0,利润为-14449.0\n",
      "第258次训练\n",
      "执行时间: 404.6853446960449 秒,91次未求解，当前强化学习值为758109.0,利润为758109.0\n",
      "第259次训练\n",
      "执行时间: 405.5002794265747 秒,131次未求解，当前强化学习值为-14444.0,利润为-14444.0\n",
      "第260次训练\n",
      "执行时间: 406.38867807388306 秒,131次未求解，当前强化学习值为-16405.0,利润为-16405.0\n",
      "第261次训练\n",
      "执行时间: 407.8405478000641 秒,118次未求解，当前强化学习值为251213.0,利润为251213.0\n",
      "第262次训练\n",
      "执行时间: 408.6661698818207 秒,131次未求解，当前强化学习值为-16982.0,利润为-16982.0\n",
      "第263次训练\n",
      "执行时间: 409.5201680660248 秒,131次未求解，当前强化学习值为-16400.0,利润为-16400.0\n",
      "第264次训练\n",
      "执行时间: 410.71553897857666 秒,123次未求解，当前强化学习值为143240.0,利润为143240.0\n",
      "第265次训练\n",
      "执行时间: 411.88081312179565 秒,125次未求解，当前强化学习值为105847.0,利润为105847.0\n",
      "第266次训练\n",
      "执行时间: 412.9284977912903 秒,127次未求解，当前强化学习值为70045.0,利润为70045.0\n",
      "第267次训练\n",
      "执行时间: 414.2932572364807 秒,123次未求解，当前强化学习值为143579.0,利润为143579.0\n",
      "第268次训练\n",
      "执行时间: 415.28262591362 秒,127次未求解，当前强化学习值为70043.0,利润为70043.0\n",
      "第269次训练\n",
      "执行时间: 417.0106508731842 秒,112次未求解，当前强化学习值为333804.0,利润为333804.0\n",
      "第270次训练\n",
      "执行时间: 417.8382136821747 秒,131次未求解，当前强化学习值为-14445.0,利润为-14445.0\n",
      "第271次训练\n",
      "执行时间: 418.89333844184875 秒,125次未求解，当前强化学习值为91939.0,利润为91939.0\n",
      "第272次训练\n",
      "执行时间: 419.7521548271179 秒,131次未求解，当前强化学习值为-18931.0,利润为-18931.0\n",
      "第273次训练\n",
      "执行时间: 421.48658871650696 秒,115次未求解，当前强化学习值为281749.0,利润为281749.0\n",
      "第274次训练\n",
      "执行时间: 423.16132712364197 秒,115次未求解，当前强化学习值为288168.0,利润为288168.0\n",
      "第275次训练\n",
      "执行时间: 423.96540093421936 秒,131次未求解，当前强化学习值为-14445.0,利润为-14445.0\n",
      "第276次训练\n",
      "执行时间: 425.5028774738312 秒,119次未求解，当前强化学习值为238179.0,利润为238179.0\n",
      "第277次训练\n",
      "执行时间: 428.9045262336731 秒,83次未求解，当前强化学习值为915412.0,利润为915412.0\n",
      "第278次训练\n",
      "执行时间: 429.73649764060974 秒,131次未求解，当前强化学习值为-22392.0,利润为-22392.0\n",
      "第279次训练\n",
      "执行时间: 431.5133831501007 秒,112次未求解，当前强化学习值为336562.0,利润为336562.0\n",
      "第280次训练\n",
      "执行时间: 433.31844997406006 秒,112次未求解，当前强化学习值为345446.0,利润为345446.0\n",
      "第281次训练\n",
      "执行时间: 434.802127122879 秒,117次未求解，当前强化学习值为267505.0,利润为267505.0\n",
      "第282次训练\n",
      "执行时间: 436.3354206085205 秒,116次未求解，当前强化学习值为286679.0,利润为286679.0\n",
      "第283次训练\n",
      "执行时间: 437.15650177001953 秒,131次未求解，当前强化学习值为-18931.0,利润为-18931.0\n",
      "第284次训练\n",
      "执行时间: 437.98800587654114 秒,131次未求解，当前强化学习值为-14444.0,利润为-14444.0\n",
      "第285次训练\n",
      "执行时间: 438.83183765411377 秒,131次未求解，当前强化学习值为-16401.0,利润为-16401.0\n",
      "第286次训练\n",
      "执行时间: 440.4087610244751 秒,115次未求解，当前强化学习值为293159.0,利润为293159.0\n",
      "第287次训练\n",
      "执行时间: 441.5441026687622 秒,123次未求解，当前强化学习值为142366.0,利润为142366.0\n",
      "第288次训练\n",
      "执行时间: 443.81729435920715 秒,104次未求解，当前强化学习值为504229.0,利润为504229.0\n",
      "第289次训练\n",
      "执行时间: 444.6574230194092 秒,131次未求解，当前强化学习值为-14444.0,利润为-14444.0\n",
      "第290次训练\n",
      "执行时间: 445.5072729587555 秒,131次未求解，当前强化学习值为-22392.0,利润为-22392.0\n",
      "第291次训练\n",
      "执行时间: 447.9133286476135 秒,103次未求解，当前强化学习值为524170.0,利润为524170.0\n",
      "第292次训练\n",
      "执行时间: 448.80811762809753 秒,131次未求解，当前强化学习值为-14447.0,利润为-14447.0\n",
      "第293次训练\n",
      "执行时间: 450.0933127403259 秒,121次未求解，当前强化学习值为190863.0,利润为190863.0\n",
      "第294次训练\n",
      "执行时间: 451.33338236808777 秒,124次未求解，当前强化学习值为120302.0,利润为120302.0\n",
      "第295次训练\n",
      "执行时间: 452.87981605529785 秒,120次未求解，当前强化学习值为213272.0,利润为213272.0\n",
      "第296次训练\n",
      "执行时间: 453.7427122592926 秒,131次未求解，当前强化学习值为-18931.0,利润为-18931.0\n",
      "第297次训练\n",
      "执行时间: 454.5995271205902 秒,131次未求解，当前强化学习值为-14450.0,利润为-14450.0\n",
      "第298次训练\n",
      "执行时间: 456.35800409317017 秒,115次未求解，当前强化学习值为296599.0,利润为296599.0\n",
      "第299次训练\n",
      "执行时间: 457.43274545669556 秒,125次未求解，当前强化学习值为105851.0,利润为105851.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import MARL_BASE as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        \n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            if last_vehicle:\n",
    "                \n",
    "                agent.update_third(vehicle_states, order_states, action, selected_log_probs, log_probs, probs,\n",
    "                            grid_reward, next_vehicle_states, next_order_states , if_end)\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            if len(group[0]) != 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                mask = env.get_mask(orders_unmatched)\n",
    "                action, selected_log_probs, log_probs, probs = agent.take_action_third(vehicle_states, order_states, mask, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "                ACTIONS.append(action) \n",
    "\n",
    "        \n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else :\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            grid_reward =   objval/1000\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "           \n",
    "       \n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAAA5zMAAAAAAMMf+QAAAAADgzu9AAAAAAEBV3kAAAAAA5EUdQQAAAABUXBJBAAAAAKbsKEEAAAAAgPQQQQAAAACsdBBBAAAAAEj/EEEAAAAAgHz+QAAAAADwpAhBAAAAAOwXIUEAAAAAokcsQQAAAABnTjtBAAAAAIA1zMAAAAAA3EgTQQAAAABQc/ZAAAAAAIA1zMAAAAAAvC4zQQAAAACAVt5AAAAAAPzDF0EAAAAAGNsJQQAAAACAWh9BAAAAAJjZE0EAAAAAWBoDQQAAAABAVd5AAAAAAMBV3kAAAAAAQLgCQQAAAADAk9DAAAAAAIA2zMAAAAAAqCUSQQAAAAAUhhFBAAAAAEBV3kAAAAAAQAPQwAAAAACANczAAAAAAORZHkEAAAAArAIoQQAAAACUKh5BAAAAAChhDUEAAAAAwJPQwAAAAAA4NQ9BAAAAAFD4HkEAAAAAIEwRQQAAAAAm1StBAAAAADCCE0EAAAAAQFXeQAAAAAAoOhJBAAAAAGAA7kAAAAAA7jojQQAAAACANczAAAAAABLWJ0EAAAAApKAbQQAAAADEkxFBAAAAAPgrI0EAAAAAtHsdQQAAAABgQzBBAAAAAHSLEUEAAAAAgJPQwAAAAAC/BjNBAAAAABduMEEAAAAAsIjwQAAAAABAVd5AAAAAAHB3FEEAAAAAcj01QQAAAAAQnR9BAAAAADgqIEEAAAAAgArPwAAAAACw/yJBAAAAAP86M0EAAAAAgJATQQAAAACAVt5AAAAAAABZEkEAAAAAiJ0RQQAAAABAifBAAAAAAKbSK0EAAAAAgDbMwAAAAAAwuhpBAAAAAKpnIEEAAAAAyDUSQQAAAACAOMzAAAAAAIA1zMAAAAAA5DgYQQAAAAAAN8zAAAAAAPwnHUEAAAAAhMwTQQAAAADAVd5AAAAAAIA2zMAAAAAAqHgUQQAAAAAgifBAAAAAADCZFUEAAAAAYM7vQAAAAADmVSBBAAAAAKyQFEEAAAAAQFXeQAAAAABYuAJBAAAAAGyEEEEAAAAAvBMRQQAAAACANczAAAAAAFwAHkEAAAAAgFkJQQAAAACAN8zAAAAAAAA7zMAAAAAAAN7VwAAAAAAgovdAAAAAAJygEkEAAAAAaCkSQQAAAAAANszAAAAAAIA1zMAAAAAAYMj+QAAAAAAAN8zAAAAAAIA3zMAAAAAAMKf1QAAAAABIvA5BAAAAADSGMEEAAAAAYBMeQQAAAADAk9DAAAAAAEDMDEEAAAAAUmgrQQAAAAAgyP5AAAAAAAA7zMAAAAAAnFsSQQAAAABYdAFBAAAAAEAc/kAAAAAAmHcaQQAAAABAA9DAAAAAAGCNDEEAAAAAADfMwAAAAAAABNDAAAAAAAAB1MAAAAAAgDjMwAAAAAAAAdTAAAAAAPARHkEAAAAA1GERQQAAAAAsySdBAAAAABCyCUEAAAAAgDjMwAAAAAB4KRFBAAAAAEBOHEEAAAAAADrMwAAAAACITR1BAAAAAPzcFEEAAAAAgJXQwAAAAADeoyhBAAAAAIB80sAAAAAAgDjMwAAAAABWGiNBAAAAACAmCUEAAAAAwJPQwAAAAAAceBVBAAAAAICT0MAAAAAAvK0hQQAAAAAgIwBBAAAAAACW0MAAAAAAAAHUwAAAAACAOczAAAAAALIMIEEAAAAAzAgaQQAAAAAQFhhBAAAAAEBn0sAAAAAAQBDxQAAAAAB0whJBAAAAAFR3EEEAAAAAKHcZQQAAAAAAO8zAAAAAAMAD0MAAAAAAsD8gQQAAAABABNDAAAAAAECdCUEAAAAAyCABQQAAAAAwRApBAAAAAKB1CkEAAAAAADrMwAAAAADAA9DAAAAAAPBbAkEAAAAAADbMwAAAAAAAHAtBAAAAAAA2zMAAAAAACHwWQQAAAABwGvFAAAAAAEBn0sAAAAAA2D0CQQAAAABAtxFBAAAAAL61KkEAAAAAQATQwAAAAACgxv5AAAAAAHreIEEAAAAAEAz3QAAAAAAAfdLAAAAAAJBL/0AAAAAAQATQwAAAAABAQydBAAAAACS/G0EAAAAARIwVQQAAAABABNDAAAAAABj6GUEAAAAAoBIRQQAAAAAuN0FBAAAAAEjOCUEAAAAAwJPQwAAAAABg+xFBAAAAADyTE0EAAAAA2QkwQQAAAAAAOszAAAAAAAApAUEAAAAAgDbMwAAAAAAAN8zAAAAAAIA5zMAAAAAAiDwkQQAAAABQnP9AAAAAAIB80sAAAAAAXAwVQQAAAAAAOszAAAAAAKAa+EAAAAAAADrMwAAAAAAQev9AAAAAAEh+EkEAAAAAiHEAQQAAAACYJAFBAAAAAAA6zMAAAAAAIJD8QAAAAAAYoSZBAAAAAMCtD0EAAAAAQAPQwAAAAAAApP1AAAAAAHSMH0EAAAAAMEYQQQAAAAAA3tXAAAAAAIA1zMAAAAAA6KsSQQAAAADZODNBAAAAAEDXC0EAAAAAwHzSwAAAAABM/hBBAAAAAADe1cAAAAAAiKEUQQAAAAAAO8zAAAAAAMAK1sAAAAAAwAPQwAAAAABoShFB8mncAhCIC0EAAAAA1BwsQQAAAAAkBCZBAAAAADDI/kAAAAAAwHzSwAAAAAAAO8zAAAAAAACk/UAAAAAA8jspQQAAAADAA9DAAAAAAIA2zMAAAAAAgDfMwAAAAAAwpBhBAAAAALBhAUEAAAAAEEUKQQAAAADAfNLAAAAAAFA4+UAAAAAAgDjMwAAAAAC6IidBAAAAAAA2zMAAAAAAQAXQwAAAAABoqg5BAAAAAICV0MAAAAAAAATQwAAAAABAfAFBAAAAAHDX+UAAAAAA0BnxQAAAAADYhgFBAAAAALAZ8UAAAAAAsF8UQQAAAACANszAAAAAADBy9kAAAAAAwHzSwAAAAABUMhFBAAAAAKCWEUEAAAAAgDbMwAAAAAAYEw1BAAAAAKjvK0EAAAAAAN7VwAAAAADIihRBAAAAAJgVFUEAAAAAxFMQQQAAAABcfxFBAAAAAMB80sAAAAAAADbMwAAAAABABNDAAAAAAJzkEUEAAAAA8GABQQAAAACUxh5BAAAAAAA2zMAAAAAAAN7VwAAAAAAo/h9BAAAAAIA3zMAAAAAAeEwHQQAAAADgXv1AAAAAAMAICkEAAAAAwHzSwAAAAAAAOczAAAAAAFwaEkEAAAAAsNf5QA==",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## buffer版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiagent.MultiAgentAC'>\n",
      "第0次训练\n",
      "Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gurobipy:Set parameter LicenseID to value 2584673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未加强化学习利润为569776.0,101次未求解\n",
      "第1次训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mycodelife\\workshop\\DRL_CO\\multiagent.py:211: UserWarning: Using a target size (torch.Size([1, 16])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.smooth_l1_loss(current_v, td_target.detach())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "执行时间: 2.3371808528900146 秒,112次未求解，当前强化学习值为344433.0,利润为344433.0\n",
      "第2次训练\n",
      "执行时间: 3.339600086212158 秒,124次未求解，当前强化学习值为132176.0,利润为132176.0\n",
      "第3次训练\n",
      "执行时间: 4.341895341873169 秒,122次未求解，当前强化学习值为165600.0,利润为165600.0\n",
      "第4次训练\n",
      "执行时间: 5.988557815551758 秒,106次未求解，当前强化学习值为483826.0,利润为483826.0\n",
      "第5次训练\n",
      "执行时间: 6.567944765090942 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第6次训练\n",
      "执行时间: 8.072954893112183 秒,112次未求解，当前强化学习值为339064.0,利润为339064.0\n",
      "第7次训练\n",
      "执行时间: 8.63939905166626 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第8次训练\n",
      "执行时间: 9.577927112579346 秒,125次未求解，当前强化学习值为109991.0,利润为109991.0\n",
      "第9次训练\n",
      "执行时间: 11.946207523345947 秒,94次未求解，当前强化学习值为692587.0,利润为692587.0\n",
      "第10次训练\n",
      "执行时间: 12.498178482055664 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第11次训练\n",
      "执行时间: 13.4780912399292 秒,124次未求解，当前强化学习值为132499.0,利润为132499.0\n",
      "第12次训练\n",
      "执行时间: 14.061967849731445 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第13次训练\n",
      "执行时间: 14.617982864379883 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第14次训练\n",
      "执行时间: 15.85045838356018 秒,116次未求解，当前强化学习值为294958.0,利润为294958.0\n",
      "第15次训练\n",
      "执行时间: 16.43802523612976 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第16次训练\n",
      "执行时间: 16.997923851013184 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第17次训练\n",
      "执行时间: 17.548630714416504 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第18次训练\n",
      "执行时间: 19.202767372131348 秒,106次未求解，当前强化学习值为468854.0,利润为468854.0\n",
      "第19次训练\n",
      "执行时间: 20.41223669052124 秒,117次未求解，当前强化学习值为262470.0,利润为262470.0\n",
      "第20次训练\n",
      "执行时间: 21.3829026222229 秒,123次未求解，当前强化学习值为136758.0,利润为136758.0\n",
      "第21次训练\n",
      "执行时间: 22.30305576324463 秒,126次未求解，当前强化学习值为73974.0,利润为73974.0\n",
      "第22次训练\n",
      "执行时间: 23.95950412750244 秒,107次未求解，当前强化学习值为464660.0,利润为464660.0\n",
      "第23次训练\n",
      "执行时间: 24.84302520751953 秒,126次未求解，当前强化学习值为67949.0,利润为67949.0\n",
      "第24次训练\n",
      "执行时间: 29.0027117729187 秒,57次未求解，当前强化学习值为1398749.0,利润为1398749.0\n",
      "第25次训练\n",
      "执行时间: 29.594388484954834 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第26次训练\n",
      "执行时间: 30.547958612442017 秒,125次未求解，当前强化学习值为97709.0,利润为97709.0\n",
      "第27次训练\n",
      "执行时间: 32.86777353286743 秒,94次未求解，当前强化学习值为689507.0,利润为689507.0\n",
      "第28次训练\n",
      "执行时间: 33.43313384056091 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第29次训练\n",
      "执行时间: 34.78779482841492 秒,114次未求解，当前强化学习值为304874.0,利润为304874.0\n",
      "第30次训练\n",
      "执行时间: 35.333056688308716 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第31次训练\n",
      "执行时间: 37.897838830947876 秒,88次未求解，当前强化学习值为838256.0,利润为838256.0\n",
      "第32次训练\n",
      "执行时间: 39.80291557312012 秒,107次未求解，当前强化学习值为457835.0,利润为457835.0\n",
      "第33次训练\n",
      "执行时间: 40.37932205200195 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第34次训练\n",
      "执行时间: 41.24758791923523 秒,126次未求解，当前强化学习值为81949.0,利润为81949.0\n",
      "第35次训练\n",
      "执行时间: 41.817726850509644 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第36次训练\n",
      "执行时间: 42.40641713142395 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第37次训练\n",
      "执行时间: 43.22764015197754 秒,127次未求解，当前强化学习值为72211.0,利润为72211.0\n",
      "第38次训练\n",
      "执行时间: 43.76789927482605 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第39次训练\n",
      "执行时间: 44.99150013923645 秒,116次未求解，当前强化学习值为287475.0,利润为287475.0\n",
      "第40次训练\n",
      "执行时间: 46.302549600601196 秒,113次未求解，当前强化学习值为330905.0,利润为330905.0\n",
      "第41次训练\n",
      "执行时间: 46.84775185585022 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第42次训练\n",
      "执行时间: 47.838069677352905 秒,123次未求解，当前强化学习值为155210.0,利润为155210.0\n",
      "第43次训练\n",
      "执行时间: 48.382896900177 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第44次训练\n",
      "执行时间: 50.267693519592285 秒,103次未求解，当前强化学习值为548454.0,利润为548454.0\n",
      "第45次训练\n",
      "执行时间: 52.87756967544556 秒,88次未求解，当前强化学习值为841674.0,利润为841674.0\n",
      "第46次训练\n",
      "执行时间: 57.89775371551514 秒,45次未求解，当前强化学习值为1628012.0,利润为1628012.0\n",
      "第47次训练\n",
      "执行时间: 59.54275631904602 秒,112次未求解，当前强化学习值为357893.0,利润为357893.0\n",
      "第48次训练\n",
      "执行时间: 60.47776770591736 秒,125次未求解，当前强化学习值为109938.0,利润为109938.0\n",
      "第49次训练\n",
      "执行时间: 61.687687158584595 秒,117次未求解，当前强化学习值为266356.0,利润为266356.0\n",
      "第50次训练\n",
      "执行时间: 65.61284375190735 秒,62次未求解，当前强化学习值为1311541.0,利润为1311541.0\n",
      "第51次训练\n",
      "执行时间: 66.9874336719513 秒,117次未求解，当前强化学习值为264891.0,利润为264891.0\n",
      "第52次训练\n",
      "执行时间: 68.43552589416504 秒,114次未求解，当前强化学习值为310083.0,利润为310083.0\n",
      "第53次训练\n",
      "执行时间: 69.90793776512146 秒,113次未求解，当前强化学习值为324002.0,利润为324002.0\n",
      "第54次训练\n",
      "执行时间: 70.83805274963379 秒,125次未求解，当前强化学习值为99246.0,利润为99246.0\n",
      "第55次训练\n",
      "执行时间: 71.83761548995972 秒,124次未求解，当前强化学习值为108373.0,利润为108373.0\n",
      "第56次训练\n",
      "执行时间: 72.5213692188263 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第57次训练\n",
      "执行时间: 73.93831753730774 秒,114次未求解，当前强化学习值为306942.0,利润为306942.0\n",
      "第58次训练\n",
      "执行时间: 79.91792750358582 秒,52次未求解，当前强化学习值为1479352.0,利润为1479352.0\n",
      "第59次训练\n",
      "执行时间: 86.03368592262268 秒,76次未求解，当前强化学习值为1043940.0,利润为1043940.0\n",
      "第60次训练\n",
      "执行时间: 88.60005068778992 秒,115次未求解，当前强化学习值为308997.0,利润为308997.0\n",
      "第61次训练\n",
      "执行时间: 90.41195821762085 秒,123次未求解，当前强化学习值为138534.0,利润为138534.0\n",
      "第62次训练\n",
      "执行时间: 91.25786089897156 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第63次训练\n",
      "执行时间: 92.79269742965698 秒,124次未求解，当前强化学习值为121874.0,利润为121874.0\n",
      "第64次训练\n",
      "执行时间: 94.19788312911987 秒,127次未求解，当前强化学习值为64018.0,利润为64018.0\n",
      "第65次训练\n",
      "执行时间: 100.57781291007996 秒,69次未求解，当前强化学习值为1175252.0,利润为1175252.0\n",
      "第66次训练\n",
      "执行时间: 103.83770251274109 秒,103次未求解，当前强化学习值为521159.0,利润为521159.0\n",
      "第67次训练\n",
      "执行时间: 104.70774412155151 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第68次训练\n",
      "执行时间: 105.84774565696716 秒,124次未求解，当前强化学习值为126045.0,利润为126045.0\n",
      "第69次训练\n",
      "执行时间: 108.03040099143982 秒,95次未求解，当前强化学习值为673496.0,利润为673496.0\n",
      "第70次训练\n",
      "执行时间: 109.580233335495 秒,110次未求解，当前强化学习值为398696.0,利润为398696.0\n",
      "第71次训练\n",
      "执行时间: 110.59771490097046 秒,122次未求解，当前强化学习值为170085.0,利润为170085.0\n",
      "第72次训练\n",
      "执行时间: 111.17898035049438 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第73次训练\n",
      "执行时间: 112.49568915367126 秒,115次未求解，当前强化学习值为287044.0,利润为287044.0\n",
      "第74次训练\n",
      "执行时间: 113.72279167175293 秒,116次未求解，当前强化学习值为279007.0,利润为279007.0\n",
      "第75次训练\n",
      "执行时间: 114.27727746963501 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第76次训练\n",
      "执行时间: 115.8109450340271 秒,110次未求解，当前强化学习值为381575.0,利润为381575.0\n",
      "第77次训练\n",
      "执行时间: 116.96759033203125 秒,120次未求解，当前强化学习值为228820.0,利润为228820.0\n",
      "第78次训练\n",
      "执行时间: 119.67886114120483 秒,86次未求解，当前强化学习值为878187.0,利润为878187.0\n",
      "第79次训练\n",
      "执行时间: 120.76757025718689 秒,122次未求解，当前强化学习值为154622.0,利润为154622.0\n",
      "第80次训练\n",
      "执行时间: 122.31753182411194 秒,112次未求解，当前强化学习值为339272.0,利润为339272.0\n",
      "第81次训练\n",
      "执行时间: 122.88732314109802 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第82次训练\n",
      "执行时间: 123.99229192733765 秒,120次未求解，当前强化学习值为216194.0,利润为216194.0\n",
      "第83次训练\n",
      "执行时间: 124.54246973991394 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第84次训练\n",
      "执行时间: 125.97232580184937 秒,114次未求解，当前强化学习值为312695.0,利润为312695.0\n",
      "第85次训练\n",
      "执行时间: 127.08502149581909 秒,124次未求解，当前强化学习值为134443.0,利润为134443.0\n",
      "第86次训练\n",
      "执行时间: 130.61905193328857 秒,77次未求解，当前强化学习值为1039529.0,利润为1039529.0\n",
      "第87次训练\n",
      "执行时间: 131.26399970054626 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第88次训练\n",
      "执行时间: 132.0703489780426 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第89次训练\n",
      "执行时间: 136.0475673675537 秒,96次未求解，当前强化学习值为651923.0,利润为651923.0\n",
      "第90次训练\n",
      "执行时间: 142.48445630073547 秒,69次未求解，当前强化学习值为1211756.0,利润为1211756.0\n",
      "第91次训练\n",
      "执行时间: 145.27748322486877 秒,112次未求解，当前强化学习值为353002.0,利润为353002.0\n",
      "第92次训练\n",
      "执行时间: 146.2624228000641 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第93次训练\n",
      "执行时间: 148.76248788833618 秒,115次未求解，当前强化学习值为295685.0,利润为295685.0\n",
      "第94次训练\n",
      "执行时间: 149.59467101097107 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第95次训练\n",
      "执行时间: 151.1514024734497 秒,111次未求解，当前强化学习值为377646.0,利润为377646.0\n",
      "第96次训练\n",
      "执行时间: 152.31246066093445 秒,123次未求解，当前强化学习值为151810.0,利润为151810.0\n",
      "第97次训练\n",
      "执行时间: 157.72004413604736 秒,38次未求解，当前强化学习值为1756656.0,利润为1756656.0\n",
      "第98次训练\n",
      "执行时间: 158.8199872970581 秒,124次未求解，当前强化学习值为126057.0,利润为126057.0\n",
      "第99次训练\n",
      "执行时间: 160.56221389770508 秒,109次未求解，当前强化学习值为417645.0,利润为417645.0\n",
      "第100次训练\n",
      "执行时间: 161.5206480026245 秒,124次未求解，当前强化学习值为123143.0,利润为123143.0\n",
      "第101次训练\n",
      "执行时间: 162.91526770591736 秒,113次未求解，当前强化学习值为318686.0,利润为318686.0\n",
      "第102次训练\n",
      "执行时间: 163.86711239814758 秒,124次未求解，当前强化学习值为125078.0,利润为125078.0\n",
      "第103次训练\n",
      "执行时间: 164.82575941085815 秒,124次未求解，当前强化学习值为129448.0,利润为129448.0\n",
      "第104次训练\n",
      "执行时间: 166.99868822097778 秒,97次未求解，当前强化学习值为626446.0,利润为626446.0\n",
      "第105次训练\n",
      "执行时间: 169.5198872089386 秒,94次未求解，当前强化学习值为727902.0,利润为727902.0\n",
      "第106次训练\n",
      "执行时间: 170.9420952796936 秒,117次未求解，当前强化学习值为254889.0,利润为254889.0\n",
      "第107次训练\n",
      "执行时间: 171.5436065196991 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第108次训练\n",
      "执行时间: 172.1211929321289 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第109次训练\n",
      "执行时间: 173.5530982017517 秒,113次未求解，当前强化学习值为316702.0,利润为316702.0\n",
      "第110次训练\n",
      "执行时间: 175.2373330593109 秒,107次未求解，当前强化学习值为464844.0,利润为464844.0\n",
      "第111次训练\n",
      "执行时间: 176.64241075515747 秒,114次未求解，当前强化学习值为305600.0,利润为305600.0\n",
      "第112次训练\n",
      "执行时间: 178.06217622756958 秒,112次未求解，当前强化学习值为356171.0,利润为356171.0\n",
      "第113次训练\n",
      "执行时间: 179.00998878479004 秒,124次未求解，当前强化学习值为132744.0,利润为132744.0\n",
      "第114次训练\n",
      "执行时间: 179.5870521068573 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第115次训练\n",
      "执行时间: 180.1635344028473 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第116次训练\n",
      "执行时间: 181.33733296394348 秒,118次未求解，当前强化学习值为260642.0,利润为260642.0\n",
      "第117次训练\n",
      "执行时间: 182.6270227432251 秒,117次未求解，当前强化学习值为267370.0,利润为267370.0\n",
      "第118次训练\n",
      "执行时间: 184.3174271583557 秒,106次未求解，当前强化学习值为484196.0,利润为484196.0\n",
      "第119次训练\n",
      "执行时间: 184.90182280540466 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第120次训练\n",
      "执行时间: 185.47210478782654 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第121次训练\n",
      "执行时间: 186.0485918521881 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第122次训练\n",
      "执行时间: 187.17737317085266 秒,120次未求解，当前强化学习值为203413.0,利润为203413.0\n",
      "第123次训练\n",
      "执行时间: 188.72725343704224 秒,112次未求解，当前强化学习值为354232.0,利润为354232.0\n",
      "第124次训练\n",
      "执行时间: 189.93008995056152 秒,120次未求解，当前强化学习值为221186.0,利润为221186.0\n",
      "第125次训练\n",
      "执行时间: 191.8372881412506 秒,105次未求解，当前强化学习值为484730.0,利润为484730.0\n",
      "第126次训练\n",
      "执行时间: 193.218759059906 秒,117次未求解，当前强化学习值为267634.0,利润为267634.0\n",
      "第127次训练\n",
      "执行时间: 196.86743831634521 秒,73次未求解，当前强化学习值为1124996.0,利润为1124996.0\n",
      "第128次训练\n",
      "执行时间: 200.28125 秒,77次未求解，当前强化学习值为1045098.0,利润为1045098.0\n",
      "第129次训练\n",
      "执行时间: 200.92610812187195 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第130次训练\n",
      "执行时间: 203.34251284599304 秒,115次未求解，当前强化学习值为297818.0,利润为297818.0\n",
      "第131次训练\n",
      "执行时间: 206.59513306617737 秒,105次未求解，当前强化学习值为515289.0,利润为515289.0\n",
      "第132次训练\n",
      "执行时间: 210.1770749092102 秒,100次未求解，当前强化学习值为574483.0,利润为574483.0\n",
      "第133次训练\n",
      "执行时间: 213.56567692756653 秒,101次未求解，当前强化学习值为578830.0,利润为578830.0\n",
      "第134次训练\n",
      "执行时间: 214.60624241828918 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第135次训练\n",
      "执行时间: 215.51699209213257 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第136次训练\n",
      "执行时间: 217.0973711013794 秒,124次未求解，当前强化学习值为124540.0,利润为124540.0\n",
      "第137次训练\n",
      "执行时间: 218.74723839759827 秒,124次未求解，当前强化学习值为128155.0,利润为128155.0\n",
      "第138次训练\n",
      "执行时间: 220.42935037612915 秒,125次未求解，当前强化学习值为109992.0,利润为109992.0\n",
      "第139次训练\n",
      "执行时间: 222.12294960021973 秒,123次未求解，当前强化学习值为132572.0,利润为132572.0\n",
      "第140次训练\n",
      "执行时间: 223.71935033798218 秒,126次未求解，当前强化学习值为73974.0,利润为73974.0\n",
      "第141次训练\n",
      "执行时间: 224.71727895736694 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第142次训练\n",
      "执行时间: 226.92619252204895 秒,115次未求解，当前强化学习值为293969.0,利润为293969.0\n",
      "第143次训练\n",
      "执行时间: 228.44214010238647 秒,124次未求解，当前强化学习值为113839.0,利润为113839.0\n",
      "第144次训练\n",
      "执行时间: 231.46720457077026 秒,106次未求解，当前强化学习值为470715.0,利润为470715.0\n",
      "第145次训练\n",
      "执行时间: 235.5473804473877 秒,94次未求解，当前强化学习值为722389.0,利润为722389.0\n",
      "第146次训练\n",
      "执行时间: 236.45922136306763 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第147次训练\n",
      "执行时间: 244.14692544937134 秒,54次未求解，当前强化学习值为1438268.0,利润为1438268.0\n",
      "第148次训练\n",
      "执行时间: 245.81430387496948 秒,125次未求解，当前强化学习值为102568.0,利润为102568.0\n",
      "第149次训练\n",
      "执行时间: 249.4796063899994 秒,100次未求解，当前强化学习值为596678.0,利润为596678.0\n",
      "第150次训练\n",
      "执行时间: 251.29711866378784 秒,121次未求解，当前强化学习值为179886.0,利润为179886.0\n",
      "第151次训练\n",
      "执行时间: 253.08922600746155 秒,125次未求解，当前强化学习值为105106.0,利润为105106.0\n",
      "第152次训练\n",
      "执行时间: 255.62734246253967 秒,113次未求解，当前强化学习值为333081.0,利润为333081.0\n",
      "第153次训练\n",
      "执行时间: 256.50727677345276 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第154次训练\n",
      "执行时间: 259.6270980834961 秒,108次未求解，当前强化学习值为441664.0,利润为441664.0\n",
      "第155次训练\n",
      "执行时间: 261.7773423194885 秒,116次未求解，当前强化学习值为278074.0,利润为278074.0\n",
      "第156次训练\n",
      "执行时间: 264.8569612503052 秒,107次未求解，当前强化学习值为471310.0,利润为471310.0\n",
      "第157次训练\n",
      "执行时间: 265.7271912097931 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第158次训练\n",
      "执行时间: 267.4059705734253 秒,126次未求解，当前强化学习值为72545.0,利润为72545.0\n",
      "第159次训练\n",
      "执行时间: 270.424334526062 秒,108次未求解，当前强化学习值为437299.0,利润为437299.0\n",
      "第160次训练\n",
      "执行时间: 273.3793566226959 秒,108次未求解，当前强化学习值为441796.0,利润为441796.0\n",
      "第161次训练\n",
      "执行时间: 275.7199306488037 秒,115次未求解，当前强化学习值为301570.0,利润为301570.0\n",
      "第162次训练\n",
      "执行时间: 277.8168852329254 秒,118次未求解，当前强化学习值为245547.0,利润为245547.0\n",
      "第163次训练\n",
      "执行时间: 280.82713890075684 秒,107次未求解，当前强化学习值为439692.0,利润为439692.0\n",
      "第164次训练\n",
      "执行时间: 283.06286239624023 秒,107次未求解，当前强化学习值为458863.0,利润为458863.0\n",
      "第165次训练\n",
      "执行时间: 288.6520998477936 秒,53次未求解，当前强化学习值为1496844.0,利润为1496844.0\n",
      "第166次训练\n",
      "执行时间: 290.89392256736755 秒,108次未求解，当前强化学习值为439047.0,利润为439047.0\n",
      "第167次训练\n",
      "执行时间: 291.5870313644409 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第168次训练\n",
      "执行时间: 292.7966413497925 秒,124次未求解，当前强化学习值为113683.0,利润为113683.0\n",
      "第169次训练\n",
      "执行时间: 297.0634617805481 秒,74次未求解，当前强化学习值为1104686.0,利润为1104686.0\n",
      "第170次训练\n",
      "执行时间: 299.2069926261902 秒,110次未求解，当前强化学习值为397759.0,利润为397759.0\n",
      "第171次训练\n",
      "执行时间: 299.916565656662 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第172次训练\n",
      "执行时间: 300.62153840065 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第173次训练\n",
      "执行时间: 301.90206503868103 秒,123次未求解，当前强化学习值为147128.0,利润为147128.0\n",
      "第174次训练\n",
      "执行时间: 303.6443293094635 秒,113次未求解，当前强化学习值为322590.0,利润为322590.0\n",
      "第175次训练\n",
      "执行时间: 304.31732511520386 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第176次训练\n",
      "执行时间: 305.78166913986206 秒,118次未求解，当前强化学习值为250248.0,利润为250248.0\n",
      "第177次训练\n",
      "执行时间: 306.9867293834686 秒,124次未求解，当前强化学习值为104511.0,利润为104511.0\n",
      "第178次训练\n",
      "执行时间: 309.3366084098816 秒,102次未求解，当前强化学习值为537590.0,利润为537590.0\n",
      "第179次训练\n",
      "执行时间: 312.96495819091797 秒,88次未求解，当前强化学习值为832659.0,利润为832659.0\n",
      "第180次训练\n",
      "执行时间: 315.6258819103241 秒,104次未求解，当前强化学习值为504832.0,利润为504832.0\n",
      "第181次训练\n",
      "执行时间: 317.3310739994049 秒,124次未求解，当前强化学习值为121850.0,利润为121850.0\n",
      "第182次训练\n",
      "执行时间: 320.4667098522186 秒,106次未求解，当前强化学习值为483031.0,利润为483031.0\n",
      "第183次训练\n",
      "执行时间: 321.2287223339081 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第184次训练\n",
      "执行时间: 323.9868047237396 秒,92次未求解，当前强化学习值为752991.0,利润为752991.0\n",
      "第185次训练\n",
      "执行时间: 324.9642860889435 秒,124次未求解，当前强化学习值为130664.0,利润为130664.0\n",
      "第186次训练\n",
      "执行时间: 326.39654183387756 秒,119次未求解，当前强化学习值为228008.0,利润为228008.0\n",
      "第187次训练\n",
      "执行时间: 328.0737500190735 秒,110次未求解，当前强化学习值为397038.0,利润为397038.0\n",
      "第188次训练\n",
      "执行时间: 329.1867661476135 秒,122次未求解，当前强化学习值为161806.0,利润为161806.0\n",
      "第189次训练\n",
      "执行时间: 330.16174578666687 秒,124次未求解，当前强化学习值为130033.0,利润为130033.0\n",
      "第190次训练\n",
      "执行时间: 331.8523516654968 秒,108次未求解，当前强化学习值为430175.0,利润为430175.0\n",
      "第191次训练\n",
      "执行时间: 333.11312770843506 秒,117次未求解，当前强化学习值为265899.0,利润为265899.0\n",
      "第192次训练\n",
      "执行时间: 333.7226619720459 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第193次训练\n",
      "执行时间: 337.05178785324097 秒,81次未求解，当前强化学习值为948909.0,利润为948909.0\n",
      "第194次训练\n",
      "执行时间: 341.95716166496277 秒,53次未求解，当前强化学习值为1489579.0,利润为1489579.0\n",
      "第195次训练\n",
      "执行时间: 343.8664200305939 秒,107次未求解，当前强化学习值为464129.0,利润为464129.0\n",
      "第196次训练\n",
      "执行时间: 345.70390582084656 秒,107次未求解，当前强化学习值为462153.0,利润为462153.0\n",
      "第197次训练\n",
      "执行时间: 346.30669140815735 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第198次训练\n",
      "执行时间: 348.9391837120056 秒,94次未求解，当前强化学习值为701743.0,利润为701743.0\n",
      "第199次训练\n",
      "执行时间: 350.7029812335968 秒,110次未求解，当前强化学习值为397367.0,利润为397367.0\n",
      "第200次训练\n",
      "执行时间: 353.46178817749023 秒,89次未求解，当前强化学习值为806692.0,利润为806692.0\n",
      "第201次训练\n",
      "执行时间: 356.3615686893463 秒,89次未求解，当前强化学习值为830735.0,利润为830735.0\n",
      "第202次训练\n",
      "执行时间: 357.5548462867737 秒,122次未求解，当前强化学习值为154949.0,利润为154949.0\n",
      "第203次训练\n",
      "执行时间: 358.79307198524475 秒,118次未求解，当前强化学习值为253543.0,利润为253543.0\n",
      "第204次训练\n",
      "执行时间: 360.0655915737152 秒,120次未求解，当前强化学习值为209710.0,利润为209710.0\n",
      "第205次训练\n",
      "执行时间: 361.6144309043884 秒,115次未求解，当前强化学习值为295362.0,利润为295362.0\n",
      "第206次训练\n",
      "执行时间: 364.95951199531555 秒,81次未求解，当前强化学习值为946977.0,利润为946977.0\n",
      "第207次训练\n",
      "执行时间: 366.4343886375427 秒,120次未求解，当前强化学习值为220280.0,利润为220280.0\n",
      "第208次训练\n",
      "执行时间: 367.63665556907654 秒,124次未求解，当前强化学习值为129183.0,利润为129183.0\n",
      "第209次训练\n",
      "执行时间: 368.67165780067444 秒,125次未求解，当前强化学习值为107647.0,利润为107647.0\n",
      "第210次训练\n",
      "执行时间: 370.79658913612366 秒,98次未求解，当前强化学习值为618791.0,利润为618791.0\n",
      "第211次训练\n",
      "执行时间: 373.6725585460663 秒,88次未求解，当前强化学习值为836753.0,利润为836753.0\n",
      "第212次训练\n",
      "执行时间: 374.29974269866943 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第213次训练\n",
      "执行时间: 374.92940616607666 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第214次训练\n",
      "执行时间: 375.9601604938507 秒,127次未求解，当前强化学习值为67702.0,利润为67702.0\n",
      "第215次训练\n",
      "执行时间: 377.08994340896606 秒,123次未求解，当前强化学习值为135042.0,利润为135042.0\n",
      "第216次训练\n",
      "执行时间: 378.2142150402069 秒,122次未求解，当前强化学习值为168385.0,利润为168385.0\n",
      "第217次训练\n",
      "执行时间: 378.78629660606384 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第218次训练\n",
      "执行时间: 379.7417747974396 秒,124次未求解，当前强化学习值为125860.0,利润为125860.0\n",
      "第219次训练\n",
      "执行时间: 381.5165915489197 秒,106次未求解，当前强化学习值为490369.0,利润为490369.0\n",
      "第220次训练\n",
      "执行时间: 382.1305003166199 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第221次训练\n",
      "执行时间: 383.236380815506 秒,124次未求解，当前强化学习值为124554.0,利润为124554.0\n",
      "第222次训练\n",
      "执行时间: 384.2808802127838 秒,122次未求解，当前强化学习值为168723.0,利润为168723.0\n",
      "第223次训练\n",
      "执行时间: 384.86500787734985 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第224次训练\n",
      "执行时间: 385.84801602363586 秒,124次未求解，当前强化学习值为126045.0,利润为126045.0\n",
      "第225次训练\n",
      "执行时间: 386.87139439582825 秒,124次未求解，当前强化学习值为102941.0,利润为102941.0\n",
      "第226次训练\n",
      "执行时间: 391.52120327949524 秒,53次未求解，当前强化学习值为1495955.0,利润为1495955.0\n",
      "第227次训练\n",
      "执行时间: 394.516535282135 秒,84次未求解，当前强化学习值为906360.0,利润为906360.0\n",
      "第228次训练\n",
      "执行时间: 395.4874749183655 秒,125次未求解，当前强化学习值为112054.0,利润为112054.0\n",
      "第229次训练\n",
      "执行时间: 396.5765063762665 秒,124次未求解，当前强化学习值为128086.0,利润为128086.0\n",
      "第230次训练\n",
      "执行时间: 398.13655972480774 秒,112次未求解，当前强化学习值为358829.0,利润为358829.0\n",
      "第231次训练\n",
      "执行时间: 400.13659739494324 秒,103次未求解，当前强化学习值为534760.0,利润为534760.0\n",
      "第232次训练\n",
      "执行时间: 402.3795962333679 秒,99次未求解，当前强化学习值为579968.0,利润为579968.0\n",
      "第233次训练\n",
      "执行时间: 404.8667182922363 秒,95次未求解，当前强化学习值为656420.0,利润为656420.0\n",
      "第234次训练\n",
      "执行时间: 407.14813590049744 秒,102次未求解，当前强化学习值为557623.0,利润为557623.0\n",
      "第235次训练\n",
      "执行时间: 408.1162521839142 秒,126次未求解，当前强化学习值为71610.0,利润为71610.0\n",
      "第236次训练\n",
      "执行时间: 408.7014248371124 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第237次训练\n",
      "执行时间: 409.2964804172516 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第238次训练\n",
      "执行时间: 411.6314148902893 秒,94次未求解，当前强化学习值为706663.0,利润为706663.0\n",
      "第239次训练\n",
      "执行时间: 415.6260721683502 秒,62次未求解，当前强化学习值为1303555.0,利润为1303555.0\n",
      "第240次训练\n",
      "执行时间: 416.2967185974121 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第241次训练\n",
      "执行时间: 418.58646512031555 秒,99次未求解，当前强化学习值为595816.0,利润为595816.0\n",
      "第242次训练\n",
      "执行时间: 419.1865351200104 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第243次训练\n",
      "执行时间: 419.7965159416199 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第244次训练\n",
      "执行时间: 420.9934220314026 秒,121次未求解，当前强化学习值为183677.0,利润为183677.0\n",
      "第245次训练\n",
      "执行时间: 421.6406903266907 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第246次训练\n",
      "执行时间: 424.04641938209534 秒,95次未求解，当前强化学习值为676656.0,利润为676656.0\n",
      "第247次训练\n",
      "执行时间: 425.951185464859 秒,108次未求解，当前强化学习值为445604.0,利润为445604.0\n",
      "第248次训练\n",
      "执行时间: 426.54637241363525 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第249次训练\n",
      "执行时间: 427.1582193374634 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第250次训练\n",
      "执行时间: 429.6263358592987 秒,92次未求解，当前强化学习值为721937.0,利润为721937.0\n",
      "第251次训练\n",
      "执行时间: 431.14615654945374 秒,112次未求解，当前强化学习值为348792.0,利润为348792.0\n",
      "第252次训练\n",
      "执行时间: 433.38853454589844 秒,97次未求解，当前强化学习值为632789.0,利润为632789.0\n",
      "第253次训练\n",
      "执行时间: 434.6073594093323 秒,119次未求解，当前强化学习值为234267.0,利润为234267.0\n",
      "第254次训练\n",
      "执行时间: 436.5613043308258 秒,105次未求解，当前强化学习值为497138.0,利润为497138.0\n",
      "第255次训练\n",
      "执行时间: 438.3920376300812 秒,109次未求解，当前强化学习值为417244.0,利润为417244.0\n",
      "第256次训练\n",
      "执行时间: 439.01136326789856 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第257次训练\n",
      "执行时间: 440.2923970222473 秒,118次未求解，当前强化学习值为257024.0,利润为257024.0\n",
      "第258次训练\n",
      "执行时间: 441.9386534690857 秒,112次未求解，当前强化学习值为352629.0,利润为352629.0\n",
      "第259次训练\n",
      "执行时间: 443.80635809898376 秒,106次未求解，当前强化学习值为477089.0,利润为477089.0\n",
      "第260次训练\n",
      "执行时间: 444.9110634326935 秒,123次未求解，当前强化学习值为154461.0,利润为154461.0\n",
      "第261次训练\n",
      "执行时间: 446.39137625694275 秒,115次未求解，当前强化学习值为294149.0,利润为294149.0\n",
      "第262次训练\n",
      "执行时间: 447.38596653938293 秒,124次未求解，当前强化学习值为132599.0,利润为132599.0\n",
      "第263次训练\n",
      "执行时间: 448.3115532398224 秒,125次未求解，当前强化学习值为91345.0,利润为91345.0\n",
      "第264次训练\n",
      "执行时间: 451.0060591697693 秒,89次未求解，当前强化学习值为814460.0,利润为814460.0\n",
      "第265次训练\n",
      "执行时间: 454.18131828308105 秒,76次未求解，当前强化学习值为1058843.0,利润为1058843.0\n",
      "第266次训练\n",
      "执行时间: 455.2534487247467 秒,122次未求解，当前强化学习值为166484.0,利润为166484.0\n",
      "第267次训练\n",
      "执行时间: 455.84613513946533 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第268次训练\n",
      "执行时间: 457.8196122646332 秒,106次未求解，当前强化学习值为484370.0,利润为484370.0\n",
      "第269次训练\n",
      "执行时间: 459.63398480415344 秒,107次未求解，当前强化学习值为471242.0,利润为471242.0\n",
      "第270次训练\n",
      "执行时间: 460.22604155540466 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第271次训练\n",
      "执行时间: 460.81721091270447 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第272次训练\n",
      "执行时间: 461.4005534648895 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第273次训练\n",
      "执行时间: 464.4613950252533 秒,81次未求解，当前强化学习值为950828.0,利润为950828.0\n",
      "第274次训练\n",
      "执行时间: 465.0612666606903 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第275次训练\n",
      "执行时间: 469.1324632167816 秒,83次未求解，当前强化学习值为934053.0,利润为934053.0\n",
      "第276次训练\n",
      "执行时间: 469.74145436286926 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第277次训练\n",
      "执行时间: 470.3361105918884 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第278次训练\n",
      "执行时间: 472.4985272884369 秒,97次未求解，当前强化学习值为643438.0,利润为643438.0\n",
      "第279次训练\n",
      "执行时间: 473.86605525016785 秒,118次未求解，当前强化学习值为249455.0,利润为249455.0\n",
      "第280次训练\n",
      "执行时间: 477.5499954223633 秒,72次未求解，当前强化学习值为1151094.0,利润为1151094.0\n",
      "第281次训练\n",
      "执行时间: 478.18582940101624 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第282次训练\n",
      "执行时间: 480.36584281921387 秒,100次未求解，当前强化学习值为581908.0,利润为581908.0\n",
      "第283次训练\n",
      "执行时间: 481.3362798690796 秒,126次未求解，当前强化学习值为77255.0,利润为77255.0\n",
      "第284次训练\n",
      "执行时间: 481.93598771095276 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第285次训练\n",
      "执行时间: 483.76337456703186 秒,107次未求解，当前强化学习值为464559.0,利润为464559.0\n",
      "第286次训练\n",
      "执行时间: 484.6412470340729 秒,127次未求解，当前强化学习值为70027.0,利润为70027.0\n",
      "第287次训练\n",
      "执行时间: 488.27089762687683 秒,69次未求解，当前强化学习值为1168821.0,利润为1168821.0\n",
      "第288次训练\n",
      "执行时间: 490.2708954811096 秒,107次未求解，当前强化学习值为464330.0,利润为464330.0\n",
      "第289次训练\n",
      "执行时间: 491.5009937286377 秒,119次未求解，当前强化学习值为235479.0,利润为235479.0\n",
      "第290次训练\n",
      "执行时间: 493.5663022994995 秒,101次未求解，当前强化学习值为575126.0,利润为575126.0\n",
      "第291次训练\n",
      "执行时间: 494.18595266342163 秒,135次未求解，当前强化学习值为-96525.0,利润为-96525.0\n",
      "第292次训练\n",
      "执行时间: 494.7798824310303 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第293次训练\n",
      "执行时间: 495.3662612438202 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n",
      "第294次训练\n",
      "执行时间: 496.8961908817291 秒,112次未求解，当前强化学习值为343023.0,利润为343023.0\n",
      "第295次训练\n",
      "执行时间: 500.8962986469269 秒,64次未求解，当前强化学习值为1291430.0,利润为1291430.0\n",
      "第296次训练\n",
      "执行时间: 502.2059462070465 秒,119次未求解，当前强化学习值为235077.0,利润为235077.0\n",
      "第297次训练\n",
      "执行时间: 503.20977234840393 秒,123次未求解，当前强化学习值为145378.0,利润为145378.0\n",
      "第298次训练\n",
      "执行时间: 506.9960353374481 秒,66次未求解，当前强化学习值为1232288.0,利润为1232288.0\n",
      "第299次训练\n",
      "执行时间: 507.6157672405243 秒,135次未求解，当前强化学习值为-96516.0,利润为-96516.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 64         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 300     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = magent.MultiAgentAC(\n",
    "    device = DEVICE,\n",
    "    VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "    ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "    NUM_CITIES = num_city, \n",
    "    HIDDEN_DIM = HIDDEN_DIM, \n",
    "    STATE_DIM = STATE_DIM\n",
    ")\n",
    "print(type(agent))\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        \n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 改了，不再是total_order\n",
    "            next_order_states = vectorization_order(orders_unmatched)\n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            if last_vehicle:\n",
    "                grid_reward = np.clip(grid_reward, 0, 2)\n",
    "                agent.store_experience(vehicle_states, order_states, selected_log_probs, \n",
    "                       log_probs, probs, grid_reward, next_vehicle_states, next_order_states)\n",
    "                agent.update_third_buffer_rnn()\n",
    "            env.time = time\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            \n",
    "            vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            # 这里也改了\n",
    "            order_states = vectorization_order(orders_unmatched)\n",
    "            if len(group[0]) != 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                if greedy > greedy_epsilon:\n",
    "                    greedy = True\n",
    "                mask = env.get_mask(orders_unmatched)\n",
    "                action, selected_log_probs, log_probs, probs = agent.take_action_third(vehicle_states, order_states, mask, explore, greedy)\n",
    "                reward = env.test_step(orders_unmatched,action)\n",
    "            \n",
    "                ACTIONS.append(action) \n",
    "\n",
    "        \n",
    "\n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else :\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            grid_reward =   objval/1000\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "        if episode == 0:\n",
    "            base_revenue.append(objval)\n",
    "            base_vehicle.append(copy.deepcopy(group[0]))\n",
    "            base_city_node.append(copy.deepcopy(city_node))\n",
    "           \n",
    "       \n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time\n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验记录\n",
    "\n",
    "1. 基模型：不收敛\n",
    "2. greedy_epsilon二次收敛，模型永远更新参数\n",
    "3. 改变了参数的传递，使得take_action_third()返回当前掩码下的概率分布，同时将这些概率传入update_third()，因此在对应使用中不再是之前的重新计算。\n",
    "4. 优化模型不求解则强化学习亦不更新参数\n",
    "5. update采用RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Iteration=%{x}<br>Reward=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "AAABAAIAAwAEAAUABgAHAAgACQAKAAsADAANAA4ADwAQABEAEgATABQAFQAWABcAGAAZABoAGwAcAB0AHgAfACAAIQAiACMAJAAlACYAJwAoACkAKgArACwALQAuAC8AMAAxADIAMwA0ADUANgA3ADgAOQA6ADsAPAA9AD4APwBAAEEAQgBDAEQARQBGAEcASABJAEoASwBMAE0ATgBPAFAAUQBSAFMAVABVAFYAVwBYAFkAWgBbAFwAXQBeAF8AYABhAGIAYwBkAGUAZgBnAGgAaQBqAGsAbABtAG4AbwBwAHEAcgBzAHQAdQB2AHcAeAB5AHoAewB8AH0AfgB/AIAAgQCCAIMAhACFAIYAhwCIAIkAigCLAIwAjQCOAI8AkACRAJIAkwCUAJUAlgCXAJgAmQCaAJsAnACdAJ4AnwCgAKEAogCjAKQApQCmAKcAqACpAKoAqwCsAK0ArgCvALAAsQCyALMAtAC1ALYAtwC4ALkAugC7ALwAvQC+AL8AwADBAMIAwwDEAMUAxgDHAMgAyQDKAMsAzADNAM4AzwDQANEA0gDTANQA1QDWANcA2ADZANoA2wDcAN0A3gDfAOAA4QDiAOMA5ADlAOYA5wDoAOkA6gDrAOwA7QDuAO8A8ADxAPIA8wD0APUA9gD3APgA+QD6APsA/AD9AP4A/wAAAQEBAgEDAQQBBQEGAQcBCAEJAQoBCwEMAQ0BDgEPARABEQESARMBFAEVARYBFwEYARkBGgEbARwBHQEeAR8BIAEhASIBIwEkASUBJgEnASgBKQEqAQ==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Reward Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Iteration"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "px.line(pd.DataFrame({'Iteration': range(len(train_rewards)), 'Reward': train_rewards}), \n",
    "        x='Iteration', y='Reward', title='Total Reward Curve').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新智能体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**新智能体的设计逻辑**\n",
    "1. 将订单按照最终可行的城市集分为多类\n",
    "2. 为每一个类设计一个智能体\n",
    "3. 智能体只在有对应订单时才进行强化学习\n",
    "4. 智能体的奖励来自于订单执行后的利润"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次训练\n",
      "未加强化学习利润为1208657.0,0次未求解\n",
      "第1次训练\n",
      "02 6 3315\n",
      "045 10 3756\n",
      "1236 15 2962\n",
      "045 17 9640\n",
      "0 18 4862\n",
      "16 20 1633\n",
      "0 22 3469\n",
      "014 25 1274\n",
      "1256 27 2165\n",
      "2356 31 1930\n",
      "06 33 4542\n",
      "06 34 4069\n",
      "05 35 2814\n",
      "0 43 4822\n",
      "0 47 2571\n",
      "0 48 2542\n",
      "2356 57 1944\n",
      "014 57 2795\n",
      "014 58 2743\n",
      "26 59 2949\n",
      "046 59 2457\n",
      "16 62 1574\n",
      "034 63 2287\n",
      "0 64 1106\n",
      "045 65 4839\n",
      "014 66 2675\n",
      "06 67 4487\n",
      "046 71 2259\n",
      "126 75 4464\n",
      "0 82 4952\n",
      "26 83 5512\n",
      "246 85 1654\n",
      "26 87 1317\n",
      "06 89 3544\n",
      "0 95 1111\n",
      "045 97 1843\n",
      "046 99 2499\n",
      "014 99 4356\n",
      "014 100 4525\n",
      "014 101 1168\n",
      "2356 103 2808\n",
      "1256 104 2112\n",
      "06 106 3594\n",
      "126 107 4617\n",
      "014 110 4491\n",
      "014 111 2512\n",
      "06 114 4541\n",
      "06 115 3306\n",
      "246 115 1655\n",
      "014 117 2612\n",
      "0 118 4923\n",
      "046 119 3177\n",
      "06 121 4410\n",
      "0 121 2585\n",
      "06 122 4607\n",
      "014 126 2482\n",
      "045 130 4954\n",
      "06 132 8444\n",
      "26 134 3000\n",
      "26 135 2861\n",
      "1356 135 1703\n",
      "0 136 4935\n",
      "0 137 2663\n",
      "0 138 2447\n",
      "执行时间: 4.62177300453186 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第2次训练\n",
      "02 6 3349\n",
      "045 10 3785\n",
      "1236 15 3001\n",
      "045 17 9649\n",
      "0 18 4877\n",
      "16 20 1603\n",
      "0 22 3459\n",
      "014 25 1349\n",
      "1256 27 2128\n",
      "2356 31 1932\n",
      "06 33 4486\n",
      "06 34 4050\n",
      "05 35 2753\n",
      "0 43 4799\n",
      "0 47 2527\n",
      "0 48 2524\n",
      "2356 57 1851\n",
      "014 57 2786\n",
      "014 58 2675\n",
      "26 59 2852\n",
      "046 59 2506\n",
      "16 62 1555\n",
      "034 63 2257\n",
      "0 64 1118\n",
      "045 65 4815\n",
      "014 66 2673\n",
      "06 67 4498\n",
      "046 71 2245\n",
      "126 75 4494\n",
      "0 82 4980\n",
      "26 83 5489\n",
      "246 85 1620\n",
      "26 87 1393\n",
      "06 89 3547\n",
      "0 95 1117\n",
      "045 97 1766\n",
      "046 99 2541\n",
      "014 99 4328\n",
      "014 100 4459\n",
      "014 101 1163\n",
      "2356 103 2827\n",
      "1256 104 2106\n",
      "06 106 3558\n",
      "126 107 4565\n",
      "014 110 4510\n",
      "014 111 2545\n",
      "06 114 4518\n",
      "06 115 3303\n",
      "246 115 1683\n",
      "014 117 2645\n",
      "0 118 4860\n",
      "046 119 3190\n",
      "06 121 4403\n",
      "0 121 2635\n",
      "06 122 4597\n",
      "014 126 2541\n",
      "045 130 4933\n",
      "06 132 8328\n",
      "26 134 2980\n",
      "26 135 2877\n",
      "1356 135 1713\n",
      "0 136 4941\n",
      "0 137 2656\n",
      "0 138 2457\n",
      "执行时间: 7.621607780456543 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第3次训练\n",
      "02 6 3387\n",
      "045 10 3747\n",
      "1236 15 3022\n",
      "045 17 9651\n",
      "0 18 4917\n",
      "16 20 1556\n",
      "0 22 3511\n",
      "014 25 1311\n",
      "1256 27 2156\n",
      "2356 31 1891\n",
      "06 33 4477\n",
      "06 34 4080\n",
      "05 35 2848\n",
      "0 43 4769\n",
      "0 47 2551\n",
      "0 48 2542\n",
      "2356 57 1898\n",
      "014 57 2840\n",
      "014 58 2696\n",
      "26 59 2902\n",
      "046 59 2472\n",
      "16 62 1600\n",
      "034 63 2279\n",
      "0 64 1120\n",
      "045 65 4824\n",
      "014 66 2686\n",
      "06 67 4459\n",
      "046 71 2290\n",
      "126 75 4448\n",
      "0 82 4951\n",
      "26 83 5449\n",
      "246 85 1669\n",
      "26 87 1345\n",
      "06 89 3524\n",
      "0 95 1110\n",
      "045 97 1770\n",
      "046 99 2487\n",
      "014 99 4389\n",
      "014 100 4465\n",
      "014 101 1171\n",
      "2356 103 2865\n",
      "1256 104 2147\n",
      "06 106 3591\n",
      "126 107 4614\n",
      "014 110 4534\n",
      "014 111 2510\n",
      "06 114 4589\n",
      "06 115 3345\n",
      "246 115 1672\n",
      "014 117 2656\n",
      "0 118 4851\n",
      "046 119 3199\n",
      "06 121 4481\n",
      "0 121 2602\n",
      "06 122 4611\n",
      "014 126 2489\n",
      "045 130 4908\n",
      "06 132 8383\n",
      "26 134 2957\n",
      "26 135 2883\n",
      "1356 135 1716\n",
      "0 136 4908\n",
      "0 137 2640\n",
      "0 138 2501\n",
      "执行时间: 10.599610090255737 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第4次训练\n",
      "02 6 3343\n",
      "045 10 3767\n",
      "1236 15 3022\n",
      "045 17 9714\n",
      "0 18 4934\n",
      "16 20 1580\n",
      "0 22 3524\n",
      "014 25 1253\n",
      "1256 27 2123\n",
      "2356 31 1899\n",
      "06 33 4505\n",
      "06 34 4090\n",
      "05 35 2819\n",
      "0 43 4782\n",
      "0 47 2600\n",
      "0 48 2528\n",
      "2356 57 1854\n",
      "014 57 2774\n",
      "014 58 2723\n",
      "26 59 2901\n",
      "046 59 2498\n",
      "16 62 1637\n",
      "034 63 2288\n",
      "0 64 1070\n",
      "045 65 4870\n",
      "014 66 2720\n",
      "06 67 4499\n",
      "046 71 2216\n",
      "126 75 4492\n",
      "0 82 4982\n",
      "26 83 5520\n",
      "246 85 1642\n",
      "26 87 1356\n",
      "06 89 3548\n",
      "0 95 1127\n",
      "045 97 1806\n",
      "046 99 2490\n",
      "014 99 4377\n",
      "014 100 4501\n",
      "014 101 1197\n",
      "2356 103 2867\n",
      "1256 104 2089\n",
      "06 106 3517\n",
      "126 107 4588\n",
      "014 110 4478\n",
      "014 111 2550\n",
      "06 114 4558\n",
      "06 115 3320\n",
      "246 115 1658\n",
      "014 117 2657\n",
      "0 118 4864\n",
      "046 119 3157\n",
      "06 121 4456\n",
      "0 121 2623\n",
      "06 122 4650\n",
      "014 126 2523\n",
      "045 130 4910\n",
      "06 132 8345\n",
      "26 134 2986\n",
      "26 135 2831\n",
      "1356 135 1753\n",
      "0 136 4898\n",
      "0 137 2606\n",
      "0 138 2443\n",
      "执行时间: 13.668502807617188 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第5次训练\n",
      "02 6 3380\n",
      "045 10 3797\n",
      "1236 15 3039\n",
      "045 17 9661\n",
      "0 18 4923\n",
      "16 20 1589\n",
      "0 22 3452\n",
      "014 25 1321\n",
      "1256 27 2199\n",
      "2356 31 1860\n",
      "06 33 4490\n",
      "06 34 4060\n",
      "05 35 2809\n",
      "0 43 4839\n",
      "0 47 2540\n",
      "0 48 2471\n",
      "2356 57 1893\n",
      "014 57 2819\n",
      "014 58 2683\n",
      "26 59 2892\n",
      "046 59 2521\n",
      "16 62 1635\n",
      "034 63 2218\n",
      "0 64 1102\n",
      "045 65 4850\n",
      "014 66 2699\n",
      "06 67 4460\n",
      "046 71 2277\n",
      "126 75 4405\n",
      "0 82 5033\n",
      "26 83 5512\n",
      "246 85 1697\n",
      "26 87 1389\n",
      "06 89 3477\n",
      "0 95 1110\n",
      "045 97 1836\n",
      "046 99 2457\n",
      "014 99 4318\n",
      "014 100 4514\n",
      "014 101 1166\n",
      "2356 103 2806\n",
      "1256 104 2102\n",
      "06 106 3517\n",
      "126 107 4602\n",
      "014 110 4540\n",
      "014 111 2557\n",
      "06 114 4560\n",
      "06 115 3271\n",
      "246 115 1664\n",
      "014 117 2657\n",
      "0 118 4914\n",
      "046 119 3187\n",
      "06 121 4449\n",
      "0 121 2555\n",
      "06 122 4636\n",
      "014 126 2508\n",
      "045 130 4995\n",
      "06 132 8382\n",
      "26 134 3042\n",
      "26 135 2840\n",
      "1356 135 1701\n",
      "0 136 4885\n",
      "0 137 2680\n",
      "0 138 2424\n",
      "执行时间: 16.672950744628906 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第6次训练\n",
      "02 6 3331\n",
      "045 10 3792\n",
      "1236 15 3020\n",
      "045 17 9692\n",
      "0 18 4945\n",
      "16 20 1561\n",
      "0 22 3456\n",
      "014 25 1328\n",
      "1256 27 2188\n",
      "2356 31 1942\n",
      "06 33 4527\n",
      "06 34 4016\n",
      "05 35 2792\n",
      "0 43 4787\n",
      "0 47 2512\n",
      "0 48 2457\n",
      "2356 57 1860\n",
      "014 57 2832\n",
      "014 58 2650\n",
      "26 59 2936\n",
      "046 59 2454\n",
      "16 62 1620\n",
      "034 63 2241\n",
      "0 64 1116\n",
      "045 65 4873\n",
      "014 66 2700\n",
      "06 67 4455\n",
      "046 71 2210\n",
      "126 75 4441\n",
      "0 82 4994\n",
      "26 83 5456\n",
      "246 85 1612\n",
      "26 87 1372\n",
      "06 89 3519\n",
      "0 95 1118\n",
      "045 97 1809\n",
      "046 99 2537\n",
      "014 99 4366\n",
      "014 100 4491\n",
      "014 101 1220\n",
      "2356 103 2833\n",
      "1256 104 2105\n",
      "06 106 3573\n",
      "126 107 4610\n",
      "014 110 4458\n",
      "014 111 2536\n",
      "06 114 4517\n",
      "06 115 3300\n",
      "246 115 1731\n",
      "014 117 2627\n",
      "0 118 4916\n",
      "046 119 3203\n",
      "06 121 4493\n",
      "0 121 2583\n",
      "06 122 4565\n",
      "014 126 2456\n",
      "045 130 4918\n",
      "06 132 8420\n",
      "26 134 2993\n",
      "26 135 2894\n",
      "1356 135 1718\n",
      "0 136 4886\n",
      "0 137 2673\n",
      "0 138 2523\n",
      "执行时间: 19.65946936607361 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第7次训练\n",
      "02 6 3306\n",
      "045 10 3748\n",
      "1236 15 2957\n",
      "045 17 9637\n",
      "0 18 4930\n",
      "16 20 1605\n",
      "0 22 3545\n",
      "014 25 1301\n",
      "1256 27 2114\n",
      "2356 31 1926\n",
      "06 33 4541\n",
      "06 34 4015\n",
      "05 35 2784\n",
      "0 43 4758\n",
      "0 47 2589\n",
      "0 48 2533\n",
      "2356 57 1911\n",
      "014 57 2830\n",
      "014 58 2743\n",
      "26 59 2880\n",
      "046 59 2507\n",
      "16 62 1630\n",
      "034 63 2263\n",
      "0 64 1120\n",
      "045 65 4858\n",
      "014 66 2705\n",
      "06 67 4475\n",
      "046 71 2277\n",
      "126 75 4446\n",
      "0 82 5033\n",
      "26 83 5516\n",
      "246 85 1628\n",
      "26 87 1312\n",
      "06 89 3525\n",
      "0 95 1128\n",
      "045 97 1817\n",
      "046 99 2533\n",
      "014 99 4320\n",
      "014 100 4490\n",
      "014 101 1244\n",
      "2356 103 2861\n",
      "1256 104 2051\n",
      "06 106 3574\n",
      "126 107 4621\n",
      "014 110 4452\n",
      "014 111 2579\n",
      "06 114 4587\n",
      "06 115 3306\n",
      "246 115 1731\n",
      "014 117 2680\n",
      "0 118 4896\n",
      "046 119 3238\n",
      "06 121 4447\n",
      "0 121 2642\n",
      "06 122 4610\n",
      "014 126 2476\n",
      "045 130 4955\n",
      "06 132 8411\n",
      "26 134 3012\n",
      "26 135 2874\n",
      "1356 135 1755\n",
      "0 136 4887\n",
      "0 137 2616\n",
      "0 138 2480\n",
      "执行时间: 22.675362586975098 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第8次训练\n",
      "02 6 3323\n",
      "045 10 3774\n",
      "1236 15 2965\n",
      "045 17 9677\n",
      "0 18 4908\n",
      "16 20 1622\n",
      "0 22 3514\n",
      "014 25 1251\n",
      "1256 27 2196\n",
      "2356 31 1910\n",
      "06 33 4531\n",
      "06 34 4037\n",
      "05 35 2786\n",
      "0 43 4771\n",
      "0 47 2541\n",
      "0 48 2491\n",
      "2356 57 1876\n",
      "014 57 2841\n",
      "014 58 2732\n",
      "26 59 2896\n",
      "046 59 2469\n",
      "16 62 1609\n",
      "034 63 2292\n",
      "0 64 1130\n",
      "045 65 4864\n",
      "014 66 2711\n",
      "06 67 4410\n",
      "046 71 2201\n",
      "126 75 4454\n",
      "0 82 5023\n",
      "26 83 5462\n",
      "246 85 1631\n",
      "26 87 1397\n",
      "06 89 3483\n",
      "0 95 1109\n",
      "045 97 1847\n",
      "046 99 2481\n",
      "014 99 4361\n",
      "014 100 4512\n",
      "014 101 1202\n",
      "2356 103 2855\n",
      "1256 104 2144\n",
      "06 106 3599\n",
      "126 107 4587\n",
      "014 110 4461\n",
      "014 111 2581\n",
      "06 114 4513\n",
      "06 115 3347\n",
      "246 115 1732\n",
      "014 117 2652\n",
      "0 118 4936\n",
      "046 119 3195\n",
      "06 121 4427\n",
      "0 121 2566\n",
      "06 122 4622\n",
      "014 126 2481\n",
      "045 130 4981\n",
      "06 132 8364\n",
      "26 134 2988\n",
      "26 135 2804\n",
      "1356 135 1779\n",
      "0 136 4885\n",
      "0 137 2640\n",
      "0 138 2467\n",
      "执行时间: 25.729580879211426 秒,0次未求解，当前强化学习值为1208657.0,利润为1208657.0\n",
      "第9次训练\n",
      "02 6 3398\n",
      "045 10 3715\n",
      "1236 15 2999\n",
      "045 17 9671\n",
      "0 18 4926\n",
      "16 20 1558\n",
      "0 22 3476\n",
      "014 25 1285\n",
      "1256 27 2185\n",
      "2356 31 1896\n",
      "06 33 4459\n",
      "06 34 4053\n",
      "05 35 2763\n",
      "0 43 4805\n",
      "0 47 2538\n",
      "0 48 2461\n",
      "2356 57 1858\n",
      "014 57 2772\n",
      "014 58 2681\n",
      "26 59 2938\n",
      "046 59 2504\n",
      "16 62 1582\n",
      "034 63 2243\n",
      "0 64 1082\n",
      "045 65 4895\n",
      "014 66 2694\n",
      "06 67 4457\n",
      "046 71 2286\n",
      "126 75 4440\n",
      "0 82 4990\n",
      "26 83 5512\n",
      "246 85 1601\n",
      "26 87 1347\n",
      "06 89 3458\n",
      "0 95 1179\n",
      "045 97 1801\n",
      "046 99 2483\n",
      "014 99 4304\n",
      "014 100 4459\n",
      "014 101 1198\n",
      "2356 103 2849\n",
      "1256 104 2150\n",
      "06 106 3559\n",
      "126 107 4643\n",
      "014 110 4498\n",
      "014 111 2566\n",
      "06 114 4515\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from gurobipy import *\n",
    "from CITY_GRAPH import *\n",
    "from CITY_NODE import *\n",
    "from ORDER import *\n",
    "from VEHICLE import *\n",
    "from tool_func import *\n",
    "from Lower_Layer import *\n",
    "import importlib\n",
    "import tool_func\n",
    "from update import *\n",
    "import os\n",
    "import time as tm\n",
    "import copy\n",
    "from my_env import *\n",
    "import torch\n",
    "import multiagent as magent\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "with open('sample_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    Vehicles = data['Vehicles']\n",
    "    Total_order = data['Total_order']\n",
    "    G = data['G']\n",
    "# 初始化\n",
    "num_vehicle = 20\n",
    "num_order = 6\n",
    "num_city = 8\n",
    "TIME = 144  # \n",
    "CAPACITY = 7\n",
    "row = [10, 1, 3, 10]\n",
    "speed = 20 # 之前是20\n",
    "cancel_penalty = 300\n",
    "battery_consume = 10\n",
    "battery_add = 300\n",
    "\n",
    "matrix = np.tile(row, (num_vehicle, 1))\n",
    "orders_unmatched = {}\n",
    "name = \"navie\"\n",
    "cancel_penalty = 300\n",
    "order_canceled = 0\n",
    "\n",
    "\n",
    "# 深复制最初的订单与车辆\n",
    "prim_order = copy.deepcopy(Total_order)\n",
    "prim_vehicle = copy.deepcopy(Vehicles)\n",
    "\"\"\"这里是强化学习部分\"\"\"\n",
    "# 超参数\n",
    "STATE_DIM_VEHICLE = 11   # 车辆状态的特征维度\n",
    "STATE_DIM_ORDER = 12     # 订单状态的特征维度\n",
    "HIDDEN_DIM = 128         # 隐藏层维度\n",
    "ACTION_DIM = num_city          # 动作空间维度\n",
    "ACTOR_LR = 1e-2          # Actor 学习率\n",
    "CRITIC_LR = 1e-2         # Critic 学习率\n",
    "GAMMA = 0.99             # 折扣因子\n",
    "NUM_EPISODES = 10     # 总训练轮数\n",
    "# 这里也改了\n",
    "STATE_DIM = 2 *HIDDEN_DIM       \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "action_types = order_same_action(Total_order, num_city, G)\n",
    "AGENT = {}\n",
    "for action_type, same_orders in action_types.items():\n",
    "    agent = magent.MultiAgentAC(\n",
    "        device = DEVICE,\n",
    "        VEHICLE_STATE_DIM = STATE_DIM_VEHICLE,\n",
    "        ORDER_STATE_DIM = STATE_DIM_ORDER, \n",
    "        NUM_CITIES = len(action_type), \n",
    "        HIDDEN_DIM = HIDDEN_DIM, \n",
    "        STATE_DIM = STATE_DIM,\n",
    "        \n",
    "    )\n",
    "    agent.action_key = action_type\n",
    "    AGENT[action_type] = agent\n",
    "\n",
    "grid_rewards = []\n",
    "# 开始计时\n",
    "start_time = tm.time()\n",
    "\n",
    "episode_reward = 0\n",
    "ACTIONS = []\n",
    "base_revenue = []\n",
    "first_revnue = []\n",
    "base_vehicle = []\n",
    "base_vehicle_class = []\n",
    "base_order_class = []\n",
    "base_city_node = []\n",
    "train_rewards = []\n",
    "burn_in = 100\n",
    "batch_time = int(TIME)\n",
    "greedy_epsilon = 0.7\n",
    "for episode in range(NUM_EPISODES):\n",
    "    print(f\"第{episode}次训练\")\n",
    "    Total_order = copy.deepcopy(prim_order)\n",
    "    Vehicles = copy.deepcopy(prim_vehicle)\n",
    "    objval = 0\n",
    "    total_objval = 0\n",
    "    reward = 0\n",
    "    episode_reward = 0\n",
    "    invalid_time =  0\n",
    "    orders_unmatched = {} # 忘记加这个了\n",
    "    orders_virtual = {}\n",
    "    \n",
    "    if episode > 0:\n",
    "        if episode > 1:\n",
    "            env.time = 0\n",
    "        \n",
    "        if episode < burn_in:\n",
    "            explore = True\n",
    "        else:\n",
    "            greedy_epsilon = greedy_epsilon * greedy_epsilon\n",
    "            explore = False\n",
    "    if_end = False\n",
    "    for time in range(batch_time):\n",
    "    \n",
    "        group = [[], []]\n",
    "        if time == batch_time -1:\n",
    "            if_end = True\n",
    "        # 按时间给出订单\n",
    "        for order in Total_order.values():\n",
    "            if order.start_time == time:\n",
    "                orders_unmatched[order.id] = order\n",
    "            # 加上这个代码后会导致性能降低\n",
    "        for vehicle in Vehicles.values():\n",
    "            if vehicle.whether_city:\n",
    "                group[0].append(vehicle.id)\n",
    "            else:\n",
    "                group[1].append(vehicle.id)\n",
    "        # 判断订单当前是否激活，并且赋值当前激活的订单给current_order\n",
    "        for agent in AGENT.values():\n",
    "            active_test(agent.action_key, agent, orders_unmatched)\n",
    "            get_multi_reward(agent)\n",
    "\n",
    "        if time != 0 and episode != 0:\n",
    "            \n",
    "            next_vehicle_states = vectorization_vehicle(Vehicles)\n",
    "            \n",
    "            # 这里防止梯度爆炸缩小了reward\n",
    "            if last_vehicle:\n",
    "                for agent in AGENT.values():\n",
    "                    if agent.active:\n",
    "                        next_order_states = vectorization_order(agent.current_order)\n",
    "                        if agent.last_order:\n",
    "                            order_states = vectorization_order(agent.last_order)\n",
    "                        else:\n",
    "                            continue\n",
    "                            # 过去time=0确保了order_states存在，现在需要第一次训练后才有\n",
    "                        # 改一下grid_reward\n",
    "                        # print(agent.action_key,time, agent.reward)\n",
    "                        agent.update(agent.v_states, order_states, agent.action, \n",
    "                                    agent.reward + objval, next_vehicle_states, next_order_states , if_end)\n",
    "                    \n",
    "            env.time = time\n",
    "\n",
    "        \n",
    "\n",
    "        if time == 0 :\n",
    "            orders_virtual = orders_unmatched\n",
    "           \n",
    "            city_node = city_node_generator(G, orders_virtual, Vehicles, orders_unmatched)\n",
    "            if episode == 1 :\n",
    "                env = DispatchEnv(\n",
    "                    G=G,\n",
    "                    vehicles=Vehicles,\n",
    "                    orders=Total_order,\n",
    "                    cities=city_node,\n",
    "                    capacity=CAPACITY\n",
    "                )\n",
    "            elif episode > 1:\n",
    "                env.cities = city_node\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            if episode == 0:\n",
    "                city_update_without_drl(city_node , Vehicles, orders_unmatched ,time)\n",
    "            else:\n",
    "                city_update_without_drl(env.cities , Vehicles, orders_unmatched, time)\n",
    "            \n",
    "        if episode != 0:\n",
    "            if len(group[0]) != 0:\n",
    "                greedy = random.randint(0, 1)\n",
    "                # if greedy > greedy_epsilon:\n",
    "                greedy = False\n",
    "                vehicle_states = vectorization_vehicle(Vehicles)\n",
    "                # 这里也改了\n",
    "                \n",
    "                for agent in AGENT.values():\n",
    "                    if agent.active:\n",
    "                        agent.v_states = vehicle_states\n",
    "                        order_states = vectorization_order(agent.current_order)\n",
    "                        agent.action= agent.take_action_skyrim(agent.v_states, order_states, explore, greedy)\n",
    "                        # ACTIONS.append(agent.action) \n",
    "        if len(group[0]) != 0:\n",
    "            last_vehicle = True\n",
    "            if episode == 0:\n",
    "                temp_Lower_Layer = Lower_Layer(G, city_node, Vehicles, orders_unmatched, name, group, time)\n",
    "            else:\n",
    "                temp_Lower_Layer = Lower_Layer(G, env.cities, Vehicles, orders_unmatched, name, group, time)\n",
    "            try:\n",
    "                temp_Lower_Layer.get_decision()\n",
    "                temp_Lower_Layer.constrain_1()\n",
    "                temp_Lower_Layer.constrain_2()\n",
    "                temp_Lower_Layer.constrain_3()\n",
    "                temp_Lower_Layer.constrain_4()\n",
    "                temp_Lower_Layer.constrain_5()\n",
    "                temp_Lower_Layer.model.setParam('OutputFlag', 0)\n",
    "                total_penalty = cancel_penalty * order_canceled\n",
    "                temp_Lower_Layer.set_objective(matrix)\n",
    "            \n",
    "                temp_Lower_Layer.model.optimize()\n",
    "\n",
    "                if temp_Lower_Layer.model.status == GRB.OPTIMAL:\n",
    "                    # save_results(temp_Lower_Layer,time)\n",
    "                    # print(\"Objective value:\", temp_Lower_Layer.model.objVal)\n",
    "                    objval = temp_Lower_Layer.model.objVal \n",
    "                else:\n",
    "                    temp_Lower_Layer.model.computeIIS()\n",
    "                    temp_Lower_Layer.model.write('iis.ilp')  # 保存不可行约束\n",
    "                    # print(f\"{time}次，No optimal solution found.\")\n",
    "                    self_update(Vehicles, G)\n",
    "                    objval = basic_cost(Vehicles, orders_unmatched)\n",
    "                    \n",
    "                \n",
    "                _, var_order = temp_Lower_Layer.get_decision()\n",
    "                update_var(temp_Lower_Layer, Vehicles, orders_unmatched)\n",
    "                vehicle_in_city = update_vehicle(Vehicles, battery_consume, battery_add, speed, G)\n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            except:\n",
    "                self_update(Vehicles, G)\n",
    "               \n",
    "                order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "                objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "                invalid_time += 1\n",
    "            \n",
    "        else:\n",
    "            last_vehicle = False\n",
    "            self_update(Vehicles, G)\n",
    "            order_canceled = order_canceled + update_order(orders_unmatched, time, speed)\n",
    "            objval = - basic_cost(Vehicles, orders_unmatched)\n",
    "            invalid_time += 1\n",
    "        objval = objval - update_order(orders_unmatched, time, speed) * cancel_penalty\n",
    "        if episode != 0: \n",
    "            \n",
    "            grid_reward =   objval\n",
    "            grid_rewards.append(reward)\n",
    "            episode_reward += reward + objval\n",
    "        total_objval += objval\n",
    "\n",
    "    end_time = tm.time()\n",
    "    execution_time = end_time - start_time\n",
    "    if episode != 0:\n",
    "        print(f\"执行时间: {execution_time} 秒,{invalid_time}次未求解，当前强化学习值为{episode_reward},利润为{total_objval}\")\n",
    "        save_path = f\"rl_para/model_checkpoint_{episode}.pth\"\n",
    "        torch.save(agent, save_path)\n",
    "        train_rewards.append(total_objval)\n",
    "        first_invalid = invalid_time        \n",
    "    else:\n",
    "        print(f\"未加强化学习利润为{total_objval},{invalid_time}次未求解\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dirty_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
